{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "dataset = np.loadtxt(\"./ml-1m/ratings.dat\",delimiter='::',dtype=int)[:,[0,1,3]]\n",
    "N_USERS = np.max(dataset[:,0])\n",
    "N_ITEMS = np.max(dataset[:,1])\n",
    "n_negatives = 4  ## 1正例对应n个负例 ##\n",
    "users_items = np.zeros((N_USERS+1, N_ITEMS+1), dtype = np.int8)  # 混淆矩阵\n",
    "user_input, item_input, labels = [],[],[]  # x1 x2 -> y\n",
    "for u in range(dataset.shape[0]):   # 评分数据集隐式化\n",
    "    users_items[dataset[u][0], dataset[u][1]] = 1\n",
    "uipositives = list() # 作为测试集的交互正例\n",
    "for i in range(N_USERS+1):\n",
    "    if i==0: \n",
    "        continue\n",
    "    uitems = dataset[dataset[:,0]==i]\n",
    "    onepos = uitems[uitems[:,-1]==np.max(uitems),:2][0]\n",
    "    uipositives.append(onepos)\n",
    "    users_items[onepos[0], onepos[1]]=0\n",
    "for uno, uitems in enumerate(users_items):\n",
    "    if uno == 0:\n",
    "        continue\n",
    "    positives = np.nonzero(uitems)[0]\n",
    "    n_sample = len(positives) * n_negatives\n",
    "    negative_items = list(set(range(N_ITEMS+1))^set(positives))\n",
    "    negatives = np.random.choice(negative_items, n_sample)  # 负采样 -- 不放回\n",
    "    for i in range(len(positives)): # 正实例\n",
    "        user_input.append(uno)\n",
    "        item_input.append(positives[i])\n",
    "        labels.append(1)\n",
    "    for j in range(n_sample): # 负实例\n",
    "        user_input.append(uno)\n",
    "        item_input.append(negatives[j])\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "utest = list()\n",
    "itest = list()\n",
    "for ui in uipositives:\n",
    "    u = ui[0]\n",
    "    i = ui[1]\n",
    "    positives = np.nonzero(users_items[u])[0]\n",
    "    negative_items = list(set(range(1,N_ITEMS+1))^set(positives))\n",
    "    negatives_sample = np.random.choice(negative_items, 99)  # 负采样 -- 不放回\n",
    "    negatives = [i]  # 正例\n",
    "    for n in negatives_sample:\n",
    "        negatives.append(n)  # 添加负例\n",
    "    utest.append([u for j in range(100)])\n",
    "    itest.append(negatives)\n",
    "ytest = np.zeros((N_USERS,100))\n",
    "ytest[:, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 150,\n",
       " 260,\n",
       " 527,\n",
       " 531,\n",
       " 588,\n",
       " 594,\n",
       " 595,\n",
       " 608,\n",
       " 661,\n",
       " 720,\n",
       " 745,\n",
       " 783,\n",
       " 914,\n",
       " 919,\n",
       " 938,\n",
       " 1022,\n",
       " 1028,\n",
       " 1029,\n",
       " 1035,\n",
       " 1097,\n",
       " 1193,\n",
       " 1197,\n",
       " 1207,\n",
       " 1246,\n",
       " 1270,\n",
       " 1287,\n",
       " 1545,\n",
       " 1566,\n",
       " 1721,\n",
       " 1836,\n",
       " 1907,\n",
       " 1961,\n",
       " 1962,\n",
       " 2018,\n",
       " 2028,\n",
       " 2294,\n",
       " 2321,\n",
       " 2340,\n",
       " 2355,\n",
       " 2398,\n",
       " 2687,\n",
       " 2692,\n",
       " 2762,\n",
       " 2791,\n",
       " 2797,\n",
       " 2804,\n",
       " 2918,\n",
       " 3105,\n",
       " 3114,\n",
       " 3186,\n",
       " 3408,\n",
       " 3941,\n",
       " 1784,\n",
       " 2992,\n",
       " 476,\n",
       " 3779,\n",
       " 1395,\n",
       " 314,\n",
       " 1884,\n",
       " 286,\n",
       " 2457,\n",
       " 985,\n",
       " 1996,\n",
       " 1065,\n",
       " 808,\n",
       " 3733,\n",
       " 1877,\n",
       " 840,\n",
       " 1925,\n",
       " 164,\n",
       " 502,\n",
       " 1362,\n",
       " 3185,\n",
       " 2538,\n",
       " 729,\n",
       " 2366,\n",
       " 2318,\n",
       " 536,\n",
       " 1244,\n",
       " 187,\n",
       " 3631,\n",
       " 1877,\n",
       " 946,\n",
       " 1113,\n",
       " 2432,\n",
       " 3414,\n",
       " 132,\n",
       " 1708,\n",
       " 1805,\n",
       " 3415,\n",
       " 219,\n",
       " 602,\n",
       " 328,\n",
       " 1902,\n",
       " 1667,\n",
       " 779,\n",
       " 1930,\n",
       " 1884,\n",
       " 2454,\n",
       " 455,\n",
       " 1379,\n",
       " 3205,\n",
       " 2323,\n",
       " 2605,\n",
       " 2978,\n",
       " 2501,\n",
       " 379,\n",
       " 814,\n",
       " 1930,\n",
       " 110,\n",
       " 187,\n",
       " 1381,\n",
       " 565,\n",
       " 3598,\n",
       " 453,\n",
       " 3731,\n",
       " 3194,\n",
       " 82,\n",
       " 2660,\n",
       " 394,\n",
       " 1539,\n",
       " 89,\n",
       " 2150,\n",
       " 140,\n",
       " 1295,\n",
       " 487,\n",
       " 3466,\n",
       " 3299,\n",
       " 2963,\n",
       " 1991,\n",
       " 3033,\n",
       " 3793,\n",
       " 3884,\n",
       " 845,\n",
       " 2616,\n",
       " 3120,\n",
       " 3617,\n",
       " 3779,\n",
       " 2629,\n",
       " 2528,\n",
       " 1431,\n",
       " 1909,\n",
       " 1405,\n",
       " 878,\n",
       " 644,\n",
       " 1832,\n",
       " 1929,\n",
       " 1477,\n",
       " 1597,\n",
       " 1294,\n",
       " 2324,\n",
       " 3654,\n",
       " 3643,\n",
       " 324,\n",
       " 1420,\n",
       " 3394,\n",
       " 2342,\n",
       " 1209,\n",
       " 807,\n",
       " 1778,\n",
       " 3217,\n",
       " 3787,\n",
       " 3712,\n",
       " 2289,\n",
       " 3119,\n",
       " 909,\n",
       " 2812,\n",
       " 1146,\n",
       " 1130,\n",
       " 2058,\n",
       " 464,\n",
       " 1008,\n",
       " 2552,\n",
       " 1366,\n",
       " 1703,\n",
       " 1408,\n",
       " 2738,\n",
       " 1295,\n",
       " 1690,\n",
       " 3760,\n",
       " 990,\n",
       " 2086,\n",
       " 620,\n",
       " 1375,\n",
       " 2172,\n",
       " 636,\n",
       " 3015,\n",
       " 2789,\n",
       " 1101,\n",
       " 79,\n",
       " 275,\n",
       " 3373,\n",
       " 2558,\n",
       " 3419,\n",
       " 388,\n",
       " 3720,\n",
       " 27,\n",
       " 3290,\n",
       " 3383,\n",
       " 605,\n",
       " 1834,\n",
       " 1610,\n",
       " 1164,\n",
       " 1809,\n",
       " 2227,\n",
       " 1891,\n",
       " 1612,\n",
       " 1805,\n",
       " 3797,\n",
       " 118,\n",
       " 2088,\n",
       " 1569,\n",
       " 2848,\n",
       " 548,\n",
       " 2004,\n",
       " 1660,\n",
       " 2144,\n",
       " 414,\n",
       " 1871,\n",
       " 292,\n",
       " 2457,\n",
       " 3568,\n",
       " 2418,\n",
       " 1741,\n",
       " 3250,\n",
       " 98,\n",
       " 157,\n",
       " 811,\n",
       " 2293,\n",
       " 3396,\n",
       " 1472,\n",
       " 809,\n",
       " 2875,\n",
       " 3140,\n",
       " 343,\n",
       " 2875,\n",
       " 3592,\n",
       " 3864,\n",
       " 1967,\n",
       " 2274,\n",
       " 2202,\n",
       " 269,\n",
       " 2535,\n",
       " 3342,\n",
       " 1081,\n",
       " 3897,\n",
       " 2467,\n",
       " 2895,\n",
       " 2386,\n",
       " 1569,\n",
       " 3278,\n",
       " 452,\n",
       " 3270,\n",
       " 1501,\n",
       " 3031,\n",
       " 881,\n",
       " 1968,\n",
       " 3549,\n",
       " 3591,\n",
       " 21,\n",
       " 95,\n",
       " 110,\n",
       " 163,\n",
       " 165,\n",
       " 235,\n",
       " 265,\n",
       " 292,\n",
       " 318,\n",
       " 349,\n",
       " 356,\n",
       " 368,\n",
       " 380,\n",
       " 434,\n",
       " 442,\n",
       " 457,\n",
       " 459,\n",
       " 480,\n",
       " 498,\n",
       " 515,\n",
       " 589,\n",
       " 590,\n",
       " 593,\n",
       " 647,\n",
       " 648,\n",
       " 736,\n",
       " 780,\n",
       " 902,\n",
       " 920,\n",
       " 982,\n",
       " 1084,\n",
       " 1090,\n",
       " 1096,\n",
       " 1103,\n",
       " 1124,\n",
       " 1188,\n",
       " 1193,\n",
       " 1196,\n",
       " 1198,\n",
       " 1207,\n",
       " 1210,\n",
       " 1213,\n",
       " 1217,\n",
       " 1225,\n",
       " 1244,\n",
       " 1245,\n",
       " 1246,\n",
       " 1247,\n",
       " 1253,\n",
       " 1259,\n",
       " 1265,\n",
       " 1293,\n",
       " 1357,\n",
       " 1370,\n",
       " 1372,\n",
       " 1385,\n",
       " 1408,\n",
       " 1442,\n",
       " 1527,\n",
       " 1537,\n",
       " 1544,\n",
       " 1552,\n",
       " 1597,\n",
       " 1610,\n",
       " 1690,\n",
       " 1784,\n",
       " 1792,\n",
       " 1801,\n",
       " 1834,\n",
       " 1873,\n",
       " 1917,\n",
       " 1945,\n",
       " 1953,\n",
       " 1954,\n",
       " 1955,\n",
       " 1957,\n",
       " 1962,\n",
       " 1968,\n",
       " 2002,\n",
       " 2006,\n",
       " 2028,\n",
       " 2067,\n",
       " 2126,\n",
       " 2194,\n",
       " 2236,\n",
       " 2268,\n",
       " 2278,\n",
       " 2312,\n",
       " 2321,\n",
       " 2353,\n",
       " 2359,\n",
       " 2396,\n",
       " 2427,\n",
       " 2490,\n",
       " 2501,\n",
       " 2571,\n",
       " 2628,\n",
       " 2717,\n",
       " 2728,\n",
       " 2852,\n",
       " 2858,\n",
       " 2881,\n",
       " 2916,\n",
       " 2943,\n",
       " 3030,\n",
       " 3035,\n",
       " 3068,\n",
       " 3071,\n",
       " 3095,\n",
       " 3105,\n",
       " 3107,\n",
       " 3108,\n",
       " 3147,\n",
       " 3255,\n",
       " 3256,\n",
       " 3257,\n",
       " 3334,\n",
       " 3418,\n",
       " 3451,\n",
       " 3468,\n",
       " 3471,\n",
       " 3578,\n",
       " 3654,\n",
       " 3678,\n",
       " 3699,\n",
       " 3735,\n",
       " 3809,\n",
       " 3893,\n",
       " 2256,\n",
       " 3655,\n",
       " 2418,\n",
       " 3634,\n",
       " 2147,\n",
       " 869,\n",
       " 1553,\n",
       " 2631,\n",
       " 724,\n",
       " 1663,\n",
       " 2986,\n",
       " 1270,\n",
       " 2184,\n",
       " 3176,\n",
       " 3417,\n",
       " 1991,\n",
       " 3448,\n",
       " 2599,\n",
       " 781,\n",
       " 2993,\n",
       " 338,\n",
       " 3633,\n",
       " 137,\n",
       " 3069,\n",
       " 425,\n",
       " 1021,\n",
       " 2165,\n",
       " 2759,\n",
       " 1497,\n",
       " 1926,\n",
       " 2032,\n",
       " 2434,\n",
       " 1340,\n",
       " 3688,\n",
       " 2631,\n",
       " 687,\n",
       " 59,\n",
       " 3012,\n",
       " 3936,\n",
       " 2505,\n",
       " 3543,\n",
       " 3655,\n",
       " 757,\n",
       " 309,\n",
       " 2245,\n",
       " 1566,\n",
       " 3155,\n",
       " 156,\n",
       " 1273,\n",
       " 2333,\n",
       " 753,\n",
       " 1816,\n",
       " 2418,\n",
       " 3670,\n",
       " 666,\n",
       " 229,\n",
       " 1413,\n",
       " 75,\n",
       " 915,\n",
       " 818,\n",
       " 504,\n",
       " 1347,\n",
       " 3409,\n",
       " 1823,\n",
       " 625,\n",
       " 3479,\n",
       " 1199,\n",
       " 2367,\n",
       " 1113,\n",
       " 3037,\n",
       " 992,\n",
       " 3480,\n",
       " 1839,\n",
       " 1893,\n",
       " 2015,\n",
       " 335,\n",
       " 1546,\n",
       " 2561,\n",
       " 3862,\n",
       " 2896,\n",
       " 3459,\n",
       " 170,\n",
       " 3530,\n",
       " 3102,\n",
       " 2784,\n",
       " 3034,\n",
       " 2383,\n",
       " 1789,\n",
       " 2809,\n",
       " 3051,\n",
       " 182,\n",
       " 3202,\n",
       " 3283,\n",
       " 1490,\n",
       " 3426,\n",
       " 1215,\n",
       " 1042,\n",
       " 567,\n",
       " 1200,\n",
       " 174,\n",
       " 3584,\n",
       " 1347,\n",
       " 3297,\n",
       " 2060,\n",
       " 3278,\n",
       " 3403,\n",
       " 2347,\n",
       " 435,\n",
       " 403,\n",
       " 3231,\n",
       " 2157,\n",
       " 125,\n",
       " 3919,\n",
       " 3221,\n",
       " 941,\n",
       " 3579,\n",
       " 1134,\n",
       " 2597,\n",
       " 1450,\n",
       " 2689,\n",
       " 1105,\n",
       " 396,\n",
       " 3146,\n",
       " 1222,\n",
       " 2542,\n",
       " 2495,\n",
       " 3493,\n",
       " 2151,\n",
       " 805,\n",
       " 624,\n",
       " 3512,\n",
       " 121,\n",
       " 812,\n",
       " 1902,\n",
       " 522,\n",
       " 1239,\n",
       " 1778,\n",
       " 3350,\n",
       " 3238,\n",
       " 443,\n",
       " 378,\n",
       " 263,\n",
       " 614,\n",
       " 2697,\n",
       " 1430,\n",
       " 974,\n",
       " 2564,\n",
       " 1573,\n",
       " 1067,\n",
       " 3883,\n",
       " 2244,\n",
       " 854,\n",
       " 3596,\n",
       " 3183,\n",
       " 3533,\n",
       " 223,\n",
       " 1831,\n",
       " 817,\n",
       " 1627,\n",
       " 3515,\n",
       " 965,\n",
       " 863,\n",
       " 3774,\n",
       " 732,\n",
       " 3365,\n",
       " 2388,\n",
       " 2892,\n",
       " 279,\n",
       " 2235,\n",
       " 267,\n",
       " 837,\n",
       " 3696,\n",
       " 3414,\n",
       " 898,\n",
       " 104,\n",
       " 1003,\n",
       " 283,\n",
       " 2578,\n",
       " 2973,\n",
       " 55,\n",
       " 3115,\n",
       " 3375,\n",
       " 2937,\n",
       " 226,\n",
       " 3834,\n",
       " 987,\n",
       " 438,\n",
       " 3194,\n",
       " 1323,\n",
       " 2847,\n",
       " 2738,\n",
       " 2518,\n",
       " 454,\n",
       " 2285,\n",
       " 554,\n",
       " 3862,\n",
       " 504,\n",
       " 3641,\n",
       " 3458,\n",
       " 2486,\n",
       " 260,\n",
       " 3278,\n",
       " 770,\n",
       " 2765,\n",
       " 1983,\n",
       " 1010,\n",
       " 2163,\n",
       " 1748,\n",
       " 3420,\n",
       " 712,\n",
       " 357,\n",
       " 1269,\n",
       " 1764,\n",
       " 2844,\n",
       " 729,\n",
       " 1295,\n",
       " 1166,\n",
       " 2190,\n",
       " 1011,\n",
       " 2918,\n",
       " 3746,\n",
       " 2789,\n",
       " 1630,\n",
       " 1627,\n",
       " 1566,\n",
       " 3480,\n",
       " 2528,\n",
       " 2497,\n",
       " 251,\n",
       " 52,\n",
       " 3948,\n",
       " 2352,\n",
       " 2014,\n",
       " 3647,\n",
       " 4,\n",
       " 794,\n",
       " 3236,\n",
       " 2879,\n",
       " 911,\n",
       " 1358,\n",
       " 2065,\n",
       " 2846,\n",
       " 620,\n",
       " 3053,\n",
       " 3500,\n",
       " 2917,\n",
       " 2770,\n",
       " 1197,\n",
       " 2810,\n",
       " 383,\n",
       " 3193,\n",
       " 2568,\n",
       " 477,\n",
       " 3369,\n",
       " 1168,\n",
       " 3372,\n",
       " 2213,\n",
       " 2473,\n",
       " 3166,\n",
       " 1110,\n",
       " 3209,\n",
       " 3057,\n",
       " 2531,\n",
       " 3820,\n",
       " 2986,\n",
       " 3922,\n",
       " 2299,\n",
       " 1686,\n",
       " 331,\n",
       " 2507,\n",
       " 1337,\n",
       " 3515,\n",
       " 2630,\n",
       " 948,\n",
       " 1692,\n",
       " 1226,\n",
       " 60,\n",
       " 3617,\n",
       " 136,\n",
       " 2475,\n",
       " 1546,\n",
       " 803,\n",
       " 490,\n",
       " 2854,\n",
       " 2397,\n",
       " 1872,\n",
       " 2291,\n",
       " 2991,\n",
       " 2584,\n",
       " 3070,\n",
       " 41,\n",
       " 3662,\n",
       " 3238,\n",
       " 2185,\n",
       " 2266,\n",
       " 1128,\n",
       " 3943,\n",
       " 131,\n",
       " 1038,\n",
       " 3143,\n",
       " 102,\n",
       " 1353,\n",
       " 3296,\n",
       " 1736,\n",
       " 2179,\n",
       " 2481,\n",
       " 520,\n",
       " 225,\n",
       " 1729,\n",
       " 1697,\n",
       " 1082,\n",
       " 88,\n",
       " 1120,\n",
       " 1072,\n",
       " 2275,\n",
       " 1107,\n",
       " 1313,\n",
       " 533,\n",
       " 28,\n",
       " 2578,\n",
       " 2775,\n",
       " 2657,\n",
       " 3843,\n",
       " 677,\n",
       " 2906,\n",
       " 1645,\n",
       " 2252,\n",
       " 2101,\n",
       " 2639,\n",
       " 1862,\n",
       " 1031,\n",
       " 2374,\n",
       " 2874,\n",
       " 2851,\n",
       " 136,\n",
       " 1274,\n",
       " 1024,\n",
       " 2712,\n",
       " 720,\n",
       " 2402,\n",
       " 436,\n",
       " 41,\n",
       " 2093,\n",
       " 1035,\n",
       " 599,\n",
       " 3498,\n",
       " 141,\n",
       " 928,\n",
       " 331,\n",
       " 2599,\n",
       " 2115,\n",
       " 916,\n",
       " 2101,\n",
       " 3511,\n",
       " 3705,\n",
       " 3894,\n",
       " 140,\n",
       " 3890,\n",
       " 1555,\n",
       " 3856,\n",
       " 3518,\n",
       " 866,\n",
       " 3831,\n",
       " 2420,\n",
       " 691,\n",
       " 511,\n",
       " 276,\n",
       " 1422,\n",
       " 913,\n",
       " 2243,\n",
       " 2464,\n",
       " 855,\n",
       " 3668,\n",
       " 1753,\n",
       " 3480,\n",
       " 3643,\n",
       " 2016,\n",
       " 3412,\n",
       " 2344,\n",
       " 3409,\n",
       " 1026,\n",
       " 2214,\n",
       " 3836,\n",
       " 3940,\n",
       " 2808,\n",
       " 2149,\n",
       " 2021,\n",
       " 2748,\n",
       " 2170,\n",
       " 3834,\n",
       " 2786,\n",
       " 1425,\n",
       " 2295,\n",
       " 3204,\n",
       " 3584,\n",
       " 373,\n",
       " 1827,\n",
       " 83,\n",
       " 2846,\n",
       " 2560,\n",
       " 271,\n",
       " 1309,\n",
       " 470,\n",
       " 3923,\n",
       " 511,\n",
       " 1480,\n",
       " 891,\n",
       " 1750,\n",
       " 1860,\n",
       " 3143,\n",
       " 3450,\n",
       " 1687,\n",
       " 1709,\n",
       " 1598,\n",
       " 1477,\n",
       " 3626,\n",
       " 791,\n",
       " 565,\n",
       " 1221,\n",
       " 193,\n",
       " 3405,\n",
       " 3097,\n",
       " 869,\n",
       " 3755,\n",
       " 305,\n",
       " 734,\n",
       " 1301,\n",
       " 1078,\n",
       " 3928,\n",
       " 3813,\n",
       " 3002,\n",
       " 208,\n",
       " 2277,\n",
       " 1935,\n",
       " 2138,\n",
       " 3573,\n",
       " 3216,\n",
       " 2097,\n",
       " 2580,\n",
       " 634,\n",
       " 249,\n",
       " 2603,\n",
       " 568,\n",
       " 3679,\n",
       " 901,\n",
       " 2128,\n",
       " 42,\n",
       " 280,\n",
       " 2104,\n",
       " 3877,\n",
       " 2176,\n",
       " 1484,\n",
       " 2632,\n",
       " 2983,\n",
       " 3357,\n",
       " 808,\n",
       " 3643,\n",
       " 2711,\n",
       " 3102,\n",
       " 2773,\n",
       " 3556,\n",
       " 1655,\n",
       " 2921,\n",
       " 3347,\n",
       " 786,\n",
       " 3646,\n",
       " 2159,\n",
       " 1662,\n",
       " 374,\n",
       " 3215,\n",
       " 1328,\n",
       " 876,\n",
       " 3517,\n",
       " 3317,\n",
       " 2834,\n",
       " 2934,\n",
       " 1379,\n",
       " 3917,\n",
       " 665,\n",
       " 3725,\n",
       " 2994,\n",
       " 1725,\n",
       " 2918,\n",
       " 1308,\n",
       " 2113,\n",
       " 3709,\n",
       " 2190,\n",
       " 730,\n",
       " 2412,\n",
       " 2688,\n",
       " 360,\n",
       " 1718,\n",
       " 2748,\n",
       " 997,\n",
       " 122,\n",
       " 560,\n",
       " 18,\n",
       " 1508,\n",
       " 2983,\n",
       " 1824,\n",
       " 2726,\n",
       " 361,\n",
       " 2808,\n",
       " 583,\n",
       " 872,\n",
       " 3852,\n",
       " 2932,\n",
       " 239,\n",
       " 2273,\n",
       " 908,\n",
       " 2867,\n",
       " 2293,\n",
       " 104,\n",
       " 260,\n",
       " 480,\n",
       " 552,\n",
       " 590,\n",
       " 593,\n",
       " 648,\n",
       " 653,\n",
       " 733,\n",
       " 1049,\n",
       " 1079,\n",
       " 1136,\n",
       " 1196,\n",
       " 1197,\n",
       " 1198,\n",
       " 1210,\n",
       " 1259,\n",
       " 1261,\n",
       " 1265,\n",
       " 1266,\n",
       " 1270,\n",
       " 1291,\n",
       " 1304,\n",
       " 1378,\n",
       " 1379,\n",
       " 1394,\n",
       " 1431,\n",
       " 1580,\n",
       " 1615,\n",
       " 1641,\n",
       " 1961,\n",
       " 1968,\n",
       " 2006,\n",
       " 2115,\n",
       " 2167,\n",
       " 2355,\n",
       " 2470,\n",
       " 2617,\n",
       " 2735,\n",
       " 2858,\n",
       " 2871,\n",
       " 2997,\n",
       " 3114,\n",
       " 3168,\n",
       " 3421,\n",
       " 3534,\n",
       " 3552,\n",
       " 3619,\n",
       " 3671,\n",
       " 3868,\n",
       " 878,\n",
       " 276,\n",
       " 2808,\n",
       " 1251,\n",
       " 3029,\n",
       " 2082,\n",
       " 1190,\n",
       " 2067,\n",
       " 1707,\n",
       " 3473,\n",
       " 2045,\n",
       " 3055,\n",
       " 2277,\n",
       " 465,\n",
       " 2454,\n",
       " 892,\n",
       " 6,\n",
       " 3401,\n",
       " 1368,\n",
       " 3286,\n",
       " 3056,\n",
       " 2497,\n",
       " 3728,\n",
       " 1415,\n",
       " 865,\n",
       " 2040,\n",
       " 2523,\n",
       " 2559,\n",
       " 414,\n",
       " 3772,\n",
       " 415,\n",
       " 736,\n",
       " 2914,\n",
       " 947,\n",
       " 3346,\n",
       " 2432,\n",
       " 2097,\n",
       " 619,\n",
       " 2939,\n",
       " 3351,\n",
       " 1414,\n",
       " 145,\n",
       " 1732,\n",
       " 296,\n",
       " 1091,\n",
       " 1701,\n",
       " 2305,\n",
       " 3774,\n",
       " 935,\n",
       " 1795,\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 12\n",
    "USER_VECTOR_SIZE = 1    # len(one-hot of user vecter) \n",
    "ITEM_VECTOR_SIZE = 1    # len(one-hot of item vecter) \n",
    "LAYERS = [64, 32, 16, 8]   # MLP  0层为输入层  0层/2为嵌入层  \n",
    "ACTIVATION = torch.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_x1 = torch.from_numpy(np.array(user_input, ndmin=2).T).type(torch.LongTensor)\n",
    "torch_x2 = torch.from_numpy(np.array(item_input, ndmin=2).T).type(torch.LongTensor)\n",
    "torch_y  = torch.from_numpy(np.array(labels, ndmin=2).T).type(torch.FloatTensor)\n",
    "\n",
    "#x1 = Variable(torch.from_numpy(np.array(user_input, ndmin=2, dtype=np.float32).T))\n",
    "#x2 = Variable(torch.from_numpy(np.array(item_input, ndmin=2, dtype=np.float32).T))\n",
    "#y = Variable(torch.from_numpy(np.array(labels, ndmin=2, dtype=np.float32).T))\n",
    "torch_dataset = data_utils.TensorDataset(torch_x1, torch_x2, torch_y)\n",
    "loader = data_utils.DataLoader(dataset = torch_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, user_vector_size, item_vector_size, layers,  \n",
    "                 n_users, n_items, activation = torch.relu, batch_normalization = False, n_output = 1):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.do_bn = batch_normalization\n",
    "        self.fcs = []\n",
    "        self.bns = []\n",
    "        self.n_layer  = len(layers)\n",
    "        parameter_LeCun = np.sqrt(layers[-1])\n",
    "\n",
    "        #self.bn_userInput = nn.BatchNorm1d(1)   # for input data\n",
    "        #self.bn_itemInput = nn.BatchNorm1d(1)   # for input data\n",
    "        \n",
    "        self.mlp_user_embedding_layer = nn.Embedding(n_users+1, int(layers[0]/2))\n",
    "        self._set_normalInit(self.mlp_user_embedding_layer, hasBias = False) \n",
    "        self.mlp_item_embedding_layer = nn.Embedding(n_items+1, int(layers[0]/2))\n",
    "        self._set_normalInit(self.mlp_item_embedding_layer, hasBias = False) \n",
    "        \n",
    "        #self.bn_user_elayer = nn.BatchNorm1d(mlp_embedding_size) \n",
    "        #self.bn_item_elayer = nn.BatchNorm1d(mlp_embedding_size)     \n",
    "        \n",
    "        for i in range(1, self.n_layer):               # build hidden layers and BN layers\n",
    "            #input_size = layers[0] if i == 0 else layers[mlp_n_layers-i]\n",
    "            fc = nn.Linear(layers[i-1], layers[i])\n",
    "            self._set_normalInit(fc)                  # parameters initialization\n",
    "            setattr(self, 'fc%i' % i, fc)       # IMPORTANT set layer to the Module\n",
    "            self.fcs.append(fc)\n",
    "            if self.do_bn:\n",
    "                bn = nn.BatchNorm1d(layers[i])\n",
    "                setattr(self, 'bn%i' % i, bn)   # IMPORTANT set layer to the Module\n",
    "                self.bns.append(bn)\n",
    "\n",
    "        self.predict = nn.Linear(layers[-1], n_output)         # output layer\n",
    "        self._set_uniformInit(self.predict, parameter = parameter_LeCun)            # parameters initialization\n",
    "        return\n",
    "\n",
    "    def _set_normalInit(self, layer, parameter = [0.0, 0.01], hasBias=True):\n",
    "        init.normal_(layer.weight, mean = parameter[0], std = parameter[1])\n",
    "        if hasBias:\n",
    "            init.normal_(layer.bias, mean = parameter[0], std = parameter[1])\n",
    "        return\n",
    "    \n",
    "    def _set_uniformInit(self, layer, parameter = 5, hasBias = True):\n",
    "        init.uniform_(layer.weight, a = - parameter, b = parameter)\n",
    "        if hasBias:\n",
    "            init.uniform_(layer.bias, a = - parameter, b = parameter)\n",
    "        return\n",
    "    \n",
    "    def _set_heNormalInit(self, layer, hasBias=True):\n",
    "        init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.kaiming_normal_(layer.bias, nonlinearity='relu')\n",
    "        return\n",
    "    \n",
    "    def _set_heUniformInit(self, layer, hasBias=True):\n",
    "        init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.kaiming_uniform_(layer.bias, nonlinearity='relu')\n",
    "        return\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        #if self.do_bn: \n",
    "            #x1 = self.bn_userInput(x1)     # input batch normalization\n",
    "            #x2 = self.bn_itemInput(x2)\n",
    "        x1 = self.mlp_user_embedding_layer(x1)\n",
    "        x2 = self.mlp_item_embedding_layer(x2)\n",
    "        x3 = torch.cat((x1, x2), dim=1)\n",
    "        x  = torch.flatten(x3, start_dim=1)\n",
    "        for i in range(1, self.n_layer):\n",
    "            x = self.fcs[i-1](x)\n",
    "            if self.do_bn: \n",
    "                x = self.bns[i-1](x)   # batch normalization\n",
    "            x = self.activation(x)\n",
    "        out = torch.sigmoid(self.predict(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (mlp_user_embedding_layer): Embedding(6041, 32)\n",
      "  (mlp_item_embedding_layer): Embedding(3953, 32)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc3): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=8, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net(user_vector_size = USER_VECTOR_SIZE, item_vector_size = ITEM_VECTOR_SIZE,\n",
    "          layers = LAYERS, n_users = N_USERS, n_items = N_ITEMS, activation = ACTIVATION, batch_normalization = False, n_output = 1)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "loss_func = torch.nn.BCELoss()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    #HR击中率，如果topk中有正例ID即认为正确\n",
    "    if gtItem in ranklist:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    #NDCG归一化折损累计增益\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return np.log(2) / np.log(i+2)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieEval(model, loss_func, utest, itest, ytest, topK = 10):\n",
    "    if len(utest)==len(itest)==len(ytest):\n",
    "        n_users = len(utest)\n",
    "    else:\n",
    "        print('the length of test sets are not equal.')\n",
    "        return\n",
    "    hit = 0\n",
    "    undcg = 0\n",
    "    test_loss = list()\n",
    "    for i in range(n_users):\n",
    "        map_item_score = dict()\n",
    "        x1test = Variable(torch.from_numpy(np.array(utest[i], ndmin=2).T).type(torch.LongTensor))\n",
    "        x2test = Variable(torch.from_numpy(np.array(itest[i], ndmin=2).T).type(torch.LongTensor))\n",
    "        y  = Variable(torch.from_numpy(np.array(ytest[i], ndmin=2).T).type(torch.FloatTensor))\n",
    "        prediction = model(x1test, x2test)\n",
    "        loss = loss_func(prediction, y)\n",
    "        test_loss.append(loss.item())\n",
    "        pred_vector = prediction.data.numpy().T[0]\n",
    "        positive_item = itest[i][0]  # 取正例\n",
    "        for j in range(len(itest[i])):\n",
    "            map_item_score[itest[i][j]] = pred_vector[j]\n",
    "        ranklist = heapq.nlargest(topK, map_item_score, key=map_item_score.get)\n",
    "        hit += getHitRatio(ranklist, positive_item)\n",
    "        undcg += getNDCG(ranklist, positive_item)\n",
    "    mean_test_loss = np.mean(test_loss)\n",
    "    hr = hit / n_users\n",
    "    ndcg = undcg / n_users\n",
    "    print('test_loss:', mean_test_loss)\n",
    "    print('HR@', topK, ' = %.4f' % hr)\n",
    "    print('NDCG@', topK, ' = %.4f' % ndcg)\n",
    "    return mean_test_loss, hr, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------第1个epoch------\n",
      "train_loss: 0.35027285884670206\n",
      "test_loss: 0.1786077437441278\n",
      "HR@ 10  = 0.4995\n",
      "NDCG@ 10  = 0.2776\n",
      "------第2个epoch------\n",
      "train_loss: 0.3094524699672389\n",
      "test_loss: 0.17772831390951052\n",
      "HR@ 10  = 0.5689\n",
      "NDCG@ 10  = 0.3214\n",
      "------第3个epoch------\n",
      "train_loss: 0.29028863589207315\n",
      "test_loss: 0.16597943011985858\n",
      "HR@ 10  = 0.6026\n",
      "NDCG@ 10  = 0.3387\n",
      "------第4个epoch------\n",
      "train_loss: 0.27962927136580573\n",
      "test_loss: 0.14710468863343684\n",
      "HR@ 10  = 0.6232\n",
      "NDCG@ 10  = 0.3495\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = list()\n",
    "test_loss_list  = list()\n",
    "hr_list = list()\n",
    "ndcg_list = list()\n",
    "for e in range(EPOCH):\n",
    "    train_loss = list()\n",
    "    for step, (batch_x1, batch_x2, batch_y) in enumerate(loader):\n",
    "        x1, x2, y = Variable(batch_x1), Variable(batch_x2), Variable(batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        prediction = net(x1, x2)\n",
    "        loss = loss_func(prediction, y)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('------第'+str(e+1)+'个epoch------')\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    print('train_loss:', mean_train_loss)\n",
    "    train_loss_list.append(mean_train_loss)    \n",
    "    test_loss, hr, ndcg = movieEval(net, loss_func, utest, itest, ytest)\n",
    "    test_loss_list.append(test_loss)\n",
    "    hr_list.append(hr)\n",
    "    ndcg_list.append(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
