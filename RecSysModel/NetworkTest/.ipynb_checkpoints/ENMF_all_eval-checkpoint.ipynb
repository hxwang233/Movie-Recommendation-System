{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "SEED = 2019\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('../ml-1m/ratings.dat', delimiter='::', usecols=[0,1,3], dtype=int)\n",
    "#dataset = np.loadtxt('../Yelp/yelp.rating', usecols=[0,1,3], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of items:  3952\n",
      "number of flows:  1000209\n",
      "avg of S(x):  253.0\n",
      "parameter phi:  0.00025\n",
      "parameter epsilon should less than or equal phi\n",
      "sketch belongs to half of the stream\n",
      "phi = 0.00025\n",
      "S = 1000209\n",
      "r = 4\n",
      "w = 1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def veiwData(dataset):\n",
    "    n_users  = np.max(dataset[:,0])\n",
    "    n_items  = np.max(dataset[:,1])\n",
    "    avgS     = round(len(dataset) / n_items, 0)\n",
    "    itemFreq = [0 for x in range(n_items)]\n",
    "    for record in dataset:\n",
    "        itemFreq[record[1]-1] += 1\n",
    "    realHH = set()\n",
    "    for i,n in enumerate(itemFreq):\n",
    "        if n >= avgS:\n",
    "            realHH.add(i+1)\n",
    "    itemFreq.sort(reverse=True)\n",
    "    plt.plot(range(len(itemFreq)), itemFreq)\n",
    "    print(\"number of items: \", n_items)\n",
    "    print(\"number of flows: \", len(dataset))\n",
    "    print(\"avg of S(x): \", avgS)\n",
    "    print(\"parameter phi: \", round(1 / n_items, 5))\n",
    "    print(\"parameter epsilon should less than or equal phi\")\n",
    "    print(\"sketch belongs to half of the stream\")\n",
    "    return realHH, round(1 / n_items, 5), dataset.shape[0]\n",
    "\n",
    "realHH, phi, S = veiwData(dataset)\n",
    "print(\"phi =\", phi)\n",
    "print(\"S =\", S)\n",
    "delta   = 0.05\n",
    "epsilon = 0.002\n",
    "#r = round(np.log2(1 / delta)).astype(np.int)\n",
    "r = round(np.log2(1 / delta))\n",
    "w = round(2 / epsilon)\n",
    "print(\"r =\", r)\n",
    "print(\"w =\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HHtracer():\n",
    "    def __init__(self, sketch_width, sketch_deep):\n",
    "        self.sketch_width = sketch_width\n",
    "        self.sketch_deep  = sketch_deep \n",
    "        self.sketch = [[(0,0,0) for x in range(sketch_width)] for y in range(sketch_deep)]\n",
    "        return\n",
    "    \n",
    "    def processStream_HH(self, dataset):\n",
    "        for record in dataset:\n",
    "            item = (record[1], 1)\n",
    "            self.update(item)\n",
    "        return\n",
    "    \n",
    "    def update(self, item):\n",
    "        x  = item[0]\n",
    "        vx = item[1]\n",
    "        for i in range(self.sketch_deep):\n",
    "            np.random.seed(i + x)\n",
    "            j = np.random.choice(self.sketch_width)\n",
    "            V = self.sketch[i][j][0] + vx\n",
    "            K = self.sketch[i][j][1]\n",
    "            C = self.sketch[i][j][2]\n",
    "            if K == x:\n",
    "                C += vx\n",
    "            else:\n",
    "                C -= vx\n",
    "                if C < 0:\n",
    "                    K = x\n",
    "                    C = -C\n",
    "            self.sketch[i][j] = (V, K, C)\n",
    "        return\n",
    "    \n",
    "    def queryU(self, x):\n",
    "        res_list = list()\n",
    "        for i in range(self.sketch_deep):\n",
    "            np.random.seed(i + x)\n",
    "            j = np.random.choice(self.sketch_width)\n",
    "            V = self.sketch[i][j][0]\n",
    "            K = self.sketch[i][j][1]\n",
    "            C = self.sketch[i][j][2] \n",
    "            if K == x:\n",
    "                S = (V + C) / 2\n",
    "            else:\n",
    "                S = (V - C) / 2\n",
    "            res_list.append(S)\n",
    "        return min(res_list)   \n",
    "    \n",
    "    def hitter(self, phi, S):\n",
    "        print(\"heavy hitter threshold: \", phi * S)\n",
    "        hh = dict()\n",
    "        for i in range(self.sketch_deep):\n",
    "            for j in range(self.sketch_width):\n",
    "                if self.sketch[i][j][0] >= phi * S:\n",
    "                    x = self.sketch[i][j][1]\n",
    "                    freq = self.queryU(x)\n",
    "                    if freq >= phi * S:\n",
    "                        if x not in hh.keys() or (x in hh.keys() and hh[x] < freq):\n",
    "                            hh[x] = freq\n",
    "        return hh\n",
    "    \n",
    "    def getHH(self, dataset, phi, S):\n",
    "        self.processStream_HH(dataset)\n",
    "        hh = self.hitter(phi, S)\n",
    "        return hh\n",
    "    \n",
    "    def evaluate(self, res, real):\n",
    "        tp = fp = fn = 0\n",
    "        for i in res:\n",
    "            if i in real:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        for j in real:\n",
    "            if j not in res:\n",
    "                fn += 1\n",
    "        print(\"TP =\",tp,\"   FP =\", fp,\"   FN =\", fn)\n",
    "        recall = tp / (tp + fn)\n",
    "        print('reacall:', recall)\n",
    "        precision = tp / (tp + fp)\n",
    "        print('precision:',precision)\n",
    "        f1 = (2 * recall * precision) / (precision + recall)\n",
    "        print('F1-score:',f1)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heavy hitter threshold:  250.05225000000002\n",
      "TP = 1115    FP = 45    FN = 91\n",
      "reacall: 0.9245439469320066\n",
      "precision: 0.9612068965517241\n",
      "F1-score: 0.9425190194420964\n"
     ]
    }
   ],
   "source": [
    "data = dataset[dataset[:,2].argsort()]  # (user,item,tiemstamp, ...)\n",
    "hh_tracer = HHtracer(w, r)\n",
    "hhDict = hh_tracer.getHH(data, phi, S)\n",
    "resHH = set(hhDict.keys())\n",
    "hh_tracer.evaluate(resHH, realHH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "EMB_SIZE = 64\n",
    "NEG_WEIGHT = 0.1\n",
    "DROP_RATIO = 0.3\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_from_local(path, n_user, n_item):\n",
    "    data = np.loadtxt(fname=path, delimiter=\"\\t\", skiprows=1, dtype=int)\n",
    "    train_matrix = np.zeros((n_user, n_item), dtype = np.int8)\n",
    "    for line in data:\n",
    "        train_matrix[line[0],line[1]] = 1\n",
    "    user_pos = dict()\n",
    "    max_item_id = train_matrix.shape[1]\n",
    "    max_item_num = 0\n",
    "    for u, i in enumerate(train_matrix):\n",
    "        pos_item = list(np.nonzero(i)[0])\n",
    "        pos_item_num = len(pos_item)\n",
    "        if  pos_item_num > max_item_num:\n",
    "            max_item_num = pos_item_num\n",
    "        user_pos[u] = pos_item\n",
    "    train_user = list()\n",
    "    train_item = list()\n",
    "    for k in user_pos.keys():\n",
    "        while len(user_pos[k]) < max_item_num:\n",
    "            user_pos[k].append(max_item_id)\n",
    "        train_user.append(k)\n",
    "        train_item.append(user_pos[k])\n",
    "    return np.array(train_user), np.array(train_item), train_matrix\n",
    "\n",
    "def generate_test_from_local(path):\n",
    "    data = np.loadtxt(fname=path, delimiter=\"\\t\", skiprows=1, dtype=int)\n",
    "    return data\n",
    "\n",
    "train_user, train_item, train_matrix = generate_train_from_local(path=\"../ml-1m/ml.train.txt\",n_user=N_USER,n_item=N_ITEM)\n",
    "test = generate_test_from_local(path=\"../ml-1m/ml.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    def rank(self, user):\\n        res = self.user_embs(user).unsqueeze(0)\\n        res = res * self.item_embs.weight\\n        res = res.matmul(self.h).squeeze(1)\\n        return res'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ENMF(nn.Module):\n",
    "    def __init__(self, emb_size, n_user, n_item, neg_weight, drop_out, count, c0=512, x=0.6):\n",
    "        super().__init__()\n",
    "        self.c0 = c0\n",
    "        self.x  = x\n",
    "        self.count = count\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.neg_weight = neg_weight\n",
    "        self.emb_size   = emb_size\n",
    "        self.user_embs = nn.Embedding(n_user, emb_size)\n",
    "        self.item_embs = nn.Embedding(n_item+1, emb_size)\n",
    "        self.h = nn.Parameter(torch.randn(emb_size, 1))\n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "        self.freq = self.calcu_freq()\n",
    "        self._reset_para()\n",
    "        return\n",
    "    \n",
    "    def _reset_para(self):\n",
    "        nn.init.xavier_normal_(self.user_embs.weight)\n",
    "        nn.init.xavier_normal_(self.item_embs.weight)\n",
    "        nn.init.constant_(self.h, 0.01)\n",
    "        return\n",
    "    \n",
    "    def calcu_freq(self):\n",
    "        freq_items = sorted(self.count.keys())\n",
    "        freq_count = [self.count[k] for k in freq_items]\n",
    "        freq = np.zeros(self.item_embs.weight.shape[0])\n",
    "        freq[freq_items] = freq_count       \n",
    "        #freq = freq/np.sum(freq)\n",
    "        freq = np.power(freq, self.x)\n",
    "        freq = self.c0 * freq/np.sum(freq)\n",
    "        freq = torch.from_numpy(freq).type(torch.float).cuda()\n",
    "        return freq\n",
    "    \n",
    "    def forward(self, uids, pos_iids):\n",
    "        '''\n",
    "        uids: B\n",
    "        u_iids: B * L\n",
    "        '''\n",
    "        u_emb = self.dropout(self.user_embs(uids))\n",
    "        pos_embs = self.item_embs(pos_iids)\n",
    "\n",
    "        # torch.einsum(\"ab,abc->abc\")\n",
    "        # B * L * D\n",
    "        mask = (~(pos_iids.eq(self.n_item))).float()\n",
    "        pos_embs = pos_embs * mask.unsqueeze(2)\n",
    "\n",
    "        # torch.einsum(\"ac,abc->abc\")\n",
    "        # B * L * D\n",
    "        pq = u_emb.unsqueeze(1) * pos_embs\n",
    "        # torch.einsum(\"ajk,kl->ajl\")\n",
    "        # B * L\n",
    "        hpq = pq.matmul(self.h).squeeze(2)\n",
    "\n",
    "        # loss\n",
    "        pos_data_loss = torch.sum((1 - self.neg_weight) * hpq.square() - 2.0 * hpq) # 添加真实值\n",
    "\n",
    "        # torch.einsum(\"ab,ac->abc\")\n",
    "        part_1 = self.item_embs.weight.unsqueeze(2).bmm(self.item_embs.weight.unsqueeze(1))\n",
    "        part_2 = u_emb.unsqueeze(2).bmm(u_emb.unsqueeze(1))\n",
    "\n",
    "        # D * D\n",
    "        part_1 = part_1.sum(0)\n",
    "        part_2 = part_2.sum(0)\n",
    "        part_3 = self.h.mm(self.h.t())\n",
    "        all_data_loss = torch.sum(part_1 * part_2 * part_3)\n",
    "\n",
    "        loss = self.neg_weight * all_data_loss + pos_data_loss\n",
    "        return loss\n",
    "    \n",
    "    def rank(self, uid):\n",
    "        '''\n",
    "        uid: Batch_size\n",
    "        '''\n",
    "        uid_embs = self.user_embs(uid)\n",
    "        user_all_items = uid_embs.unsqueeze(1) * self.item_embs.weight\n",
    "        items_score = user_all_items.matmul(self.h).squeeze(2)\n",
    "        return items_score\n",
    "    \n",
    "'''    def rank(self, user):\n",
    "        res = self.user_embs(user).unsqueeze(0)\n",
    "        res = res * self.item_embs.weight\n",
    "        res = res.matmul(self.h).squeeze(1)\n",
    "        return res'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    #HR击中率，如果topk中有正例ID即认为正确\n",
    "    if gtItem in ranklist:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    #NDCG归一化折损累计增益\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return np.log(2) / np.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieEval_1(model, test_loader, train_matrix, topK = 100):\n",
    "    n_users = train_matrix.shape[0]\n",
    "    hit_list = list()\n",
    "    undcg_list = list()\n",
    "    rank_all_users = list()\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for step, (batch_x, batch_y) in enumerate(test_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                batch_x = batch_x.cuda()  \n",
    "            prediction = model.rank(batch_x)\n",
    "            pred_vector = -1 * (prediction.cpu().data.numpy())\n",
    "            ranklist = np.argsort(pred_vector)\n",
    "            for j, r in enumerate(ranklist):\n",
    "                real_r = list()\n",
    "                u = batch_x[j].cpu().data.numpy()\n",
    "                i = 0\n",
    "                while len(real_r) < topK:\n",
    "                    if r[i]==train_matrix.shape[1]:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    if train_matrix[u][r[i]] == 0:\n",
    "                        real_r.append(r[i])\n",
    "                    i += 1     \n",
    "                rank_all_users.append(real_r)\n",
    "                pos_item = batch_y[j].cpu().data.numpy()\n",
    "                hit_list.append(getHitRatio(real_r, pos_item))\n",
    "                undcg_list.append(getNDCG(real_r, pos_item))\n",
    "    model.train()\n",
    "    hr = np.mean(hit_list)\n",
    "    ndcg = np.mean(undcg_list)\n",
    "    print('HR@', topK, ' = %.4f' %  hr)\n",
    "    print('NDCG@', topK, ' = %.4f' % ndcg)\n",
    "    return hr, ndcg, rank_all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLoader(train_user, train_item, test, batch_size):\n",
    "    torch_x1 = torch.from_numpy(train_user).type(torch.LongTensor)\n",
    "    torch_x2 = torch.from_numpy(train_item).type(torch.LongTensor)\n",
    "    torch_test = torch.from_numpy(test).type(torch.LongTensor)\n",
    "    torch_dataset = data_utils.TensorDataset(torch_x1, torch_x2)\n",
    "    train_loader = data_utils.DataLoader(dataset = torch_dataset, batch_size = batch_size, shuffle = True, num_workers = 0)\n",
    "    torch_testset = data_utils.TensorDataset(torch_test[:,0],torch_test[:,1])\n",
    "    test_loader = data_utils.DataLoader(dataset = torch_testset, batch_size = batch_size, num_workers = 0)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(emb_size, lr, n_user, n_item, neg_weight, drop_out, hhDict):\n",
    "    model = ENMF(emb_size=emb_size, n_user=n_user, n_item=n_item, neg_weight=neg_weight, drop_out = drop_out, count=hhDict)\n",
    "    if(torch.cuda.is_available()):\n",
    "        model = model.cuda()\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr = lr)\n",
    "    print(model)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENMF(\n",
      "  (user_embs): Embedding(6040, 64)\n",
      "  (item_embs): Embedding(3953, 64)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "------第1个epoch------\n",
      "train_loss: -0.8267037009742731\n",
      "HR@ 100  = 0.0821\n",
      "NDCG@ 100  = 0.0204\n",
      "------第2个epoch------\n",
      "train_loss: -7350.3158804575605\n",
      "HR@ 100  = 0.1877\n",
      "NDCG@ 100  = 0.0417\n",
      "------第3个epoch------\n",
      "train_loss: -10899.585271199545\n",
      "HR@ 100  = 0.2634\n",
      "NDCG@ 100  = 0.0602\n",
      "------第4个epoch------\n",
      "train_loss: -12763.057301839193\n",
      "HR@ 100  = 0.2879\n",
      "NDCG@ 100  = 0.0655\n",
      "------第5个epoch------\n",
      "train_loss: -13582.140833536783\n",
      "HR@ 100  = 0.3192\n",
      "NDCG@ 100  = 0.0747\n",
      "------第6个epoch------\n",
      "train_loss: -13926.143061319986\n",
      "HR@ 100  = 0.3369\n",
      "NDCG@ 100  = 0.0799\n",
      "------第7个epoch------\n",
      "train_loss: -14145.081939697266\n",
      "HR@ 100  = 0.3467\n",
      "NDCG@ 100  = 0.0824\n",
      "------第8个epoch------\n",
      "train_loss: -14283.450892130533\n",
      "HR@ 100  = 0.3588\n",
      "NDCG@ 100  = 0.0857\n",
      "------第9个epoch------\n",
      "train_loss: -14409.875183105469\n",
      "HR@ 100  = 0.3652\n",
      "NDCG@ 100  = 0.0870\n",
      "------第10个epoch------\n",
      "train_loss: -14512.50961303711\n",
      "HR@ 100  = 0.3732\n",
      "NDCG@ 100  = 0.0895\n",
      "------第11个epoch------\n",
      "train_loss: -14605.026323954264\n",
      "HR@ 100  = 0.3803\n",
      "NDCG@ 100  = 0.0904\n",
      "------第12个epoch------\n",
      "train_loss: -14678.951243082682\n",
      "HR@ 100  = 0.3887\n",
      "NDCG@ 100  = 0.0933\n",
      "------第13个epoch------\n",
      "train_loss: -14719.204783121744\n",
      "HR@ 100  = 0.3942\n",
      "NDCG@ 100  = 0.0946\n",
      "------第14个epoch------\n",
      "train_loss: -14778.079722086588\n",
      "HR@ 100  = 0.3945\n",
      "NDCG@ 100  = 0.0952\n",
      "------第15个epoch------\n",
      "train_loss: -14809.269810994467\n",
      "HR@ 100  = 0.3954\n",
      "NDCG@ 100  = 0.0963\n",
      "------第16个epoch------\n",
      "train_loss: -14850.031255086264\n",
      "HR@ 100  = 0.3992\n",
      "NDCG@ 100  = 0.0965\n",
      "------第17个epoch------\n",
      "train_loss: -14893.986852010092\n",
      "HR@ 100  = 0.4010\n",
      "NDCG@ 100  = 0.0973\n",
      "------第18个epoch------\n",
      "train_loss: -14919.800506591797\n",
      "HR@ 100  = 0.4061\n",
      "NDCG@ 100  = 0.0991\n",
      "------第19个epoch------\n",
      "train_loss: -14942.148610432943\n",
      "HR@ 100  = 0.4043\n",
      "NDCG@ 100  = 0.0994\n",
      "------第20个epoch------\n",
      "train_loss: -14967.224578857422\n",
      "HR@ 100  = 0.4073\n",
      "NDCG@ 100  = 0.1000\n",
      "------第21个epoch------\n",
      "train_loss: -14990.487609863281\n",
      "HR@ 100  = 0.4106\n",
      "NDCG@ 100  = 0.1018\n",
      "------第22个epoch------\n",
      "train_loss: -15022.654520670572\n",
      "HR@ 100  = 0.4099\n",
      "NDCG@ 100  = 0.1016\n",
      "------第23个epoch------\n",
      "train_loss: -15038.03482055664\n",
      "HR@ 100  = 0.4124\n",
      "NDCG@ 100  = 0.1023\n",
      "------第24个epoch------\n",
      "train_loss: -15061.29409790039\n",
      "HR@ 100  = 0.4139\n",
      "NDCG@ 100  = 0.1034\n",
      "------第25个epoch------\n",
      "train_loss: -15084.362991333008\n",
      "HR@ 100  = 0.4109\n",
      "NDCG@ 100  = 0.1030\n",
      "------第26个epoch------\n",
      "train_loss: -15104.999557495117\n",
      "HR@ 100  = 0.4157\n",
      "NDCG@ 100  = 0.1039\n",
      "------第27个epoch------\n",
      "train_loss: -15103.399892171225\n",
      "HR@ 100  = 0.4151\n",
      "NDCG@ 100  = 0.1039\n",
      "------第28个epoch------\n",
      "train_loss: -15128.785619099936\n",
      "HR@ 100  = 0.4182\n",
      "NDCG@ 100  = 0.1041\n",
      "------第29个epoch------\n",
      "train_loss: -15128.286305745443\n",
      "HR@ 100  = 0.4164\n",
      "NDCG@ 100  = 0.1044\n",
      "------第30个epoch------\n",
      "train_loss: -15158.71235148112\n",
      "HR@ 100  = 0.4162\n",
      "NDCG@ 100  = 0.1048\n",
      "------第31个epoch------\n",
      "train_loss: -15156.526224772135\n",
      "HR@ 100  = 0.4146\n",
      "NDCG@ 100  = 0.1044\n",
      "------第32个epoch------\n",
      "train_loss: -15185.238591512045\n",
      "HR@ 100  = 0.4171\n",
      "NDCG@ 100  = 0.1051\n",
      "------第33个epoch------\n",
      "train_loss: -15182.91030883789\n",
      "HR@ 100  = 0.4212\n",
      "NDCG@ 100  = 0.1065\n",
      "------第34个epoch------\n",
      "train_loss: -15195.755376180014\n",
      "HR@ 100  = 0.4202\n",
      "NDCG@ 100  = 0.1064\n",
      "------第35个epoch------\n",
      "train_loss: -15217.178568522135\n",
      "HR@ 100  = 0.4195\n",
      "NDCG@ 100  = 0.1060\n",
      "------第36个epoch------\n",
      "train_loss: -15218.688873291016\n",
      "HR@ 100  = 0.4169\n",
      "NDCG@ 100  = 0.1057\n",
      "------第37个epoch------\n",
      "train_loss: -15225.057678222656\n",
      "HR@ 100  = 0.4197\n",
      "NDCG@ 100  = 0.1060\n",
      "------第38个epoch------\n",
      "train_loss: -15239.465016682943\n",
      "HR@ 100  = 0.4217\n",
      "NDCG@ 100  = 0.1063\n",
      "------第39个epoch------\n",
      "train_loss: -15249.069152832031\n",
      "HR@ 100  = 0.4232\n",
      "NDCG@ 100  = 0.1066\n",
      "------第40个epoch------\n",
      "train_loss: -15247.912246704102\n",
      "HR@ 100  = 0.4263\n",
      "NDCG@ 100  = 0.1081\n",
      "------第41个epoch------\n",
      "train_loss: -15274.071940104166\n",
      "HR@ 100  = 0.4273\n",
      "NDCG@ 100  = 0.1081\n",
      "------第42个epoch------\n",
      "train_loss: -15274.72275797526\n",
      "HR@ 100  = 0.4260\n",
      "NDCG@ 100  = 0.1079\n",
      "------第43个epoch------\n",
      "train_loss: -15283.393030802408\n",
      "HR@ 100  = 0.4298\n",
      "NDCG@ 100  = 0.1090\n",
      "------第44个epoch------\n",
      "train_loss: -15275.436452229818\n",
      "HR@ 100  = 0.4280\n",
      "NDCG@ 100  = 0.1089\n",
      "------第45个epoch------\n",
      "train_loss: -15292.516672770182\n",
      "HR@ 100  = 0.4253\n",
      "NDCG@ 100  = 0.1080\n",
      "------第46个epoch------\n",
      "train_loss: -15315.079732259115\n",
      "HR@ 100  = 0.4270\n",
      "NDCG@ 100  = 0.1085\n",
      "------第47个epoch------\n",
      "train_loss: -15305.315999348959\n",
      "HR@ 100  = 0.4258\n",
      "NDCG@ 100  = 0.1083\n",
      "------第48个epoch------\n",
      "train_loss: -15334.162521362305\n",
      "HR@ 100  = 0.4248\n",
      "NDCG@ 100  = 0.1087\n",
      "------第49个epoch------\n",
      "train_loss: -15331.273946126303\n",
      "HR@ 100  = 0.4295\n",
      "NDCG@ 100  = 0.1094\n",
      "------第50个epoch------\n",
      "train_loss: -15333.006001790365\n",
      "HR@ 100  = 0.4268\n",
      "NDCG@ 100  = 0.1091\n",
      "------第51个epoch------\n",
      "train_loss: -15338.720947265625\n",
      "HR@ 100  = 0.4290\n",
      "NDCG@ 100  = 0.1094\n",
      "------第52个epoch------\n",
      "train_loss: -15344.117039998373\n",
      "HR@ 100  = 0.4300\n",
      "NDCG@ 100  = 0.1094\n",
      "------第53个epoch------\n",
      "train_loss: -15341.762349446615\n",
      "HR@ 100  = 0.4303\n",
      "NDCG@ 100  = 0.1089\n",
      "------第54个epoch------\n",
      "train_loss: -15339.687159220377\n",
      "HR@ 100  = 0.4301\n",
      "NDCG@ 100  = 0.1088\n",
      "------第55个epoch------\n",
      "train_loss: -15365.63436381022\n",
      "HR@ 100  = 0.4311\n",
      "NDCG@ 100  = 0.1094\n",
      "------第56个epoch------\n",
      "train_loss: -15360.718617757162\n",
      "HR@ 100  = 0.4313\n",
      "NDCG@ 100  = 0.1091\n",
      "------第57个epoch------\n",
      "train_loss: -15371.959437052408\n",
      "HR@ 100  = 0.4321\n",
      "NDCG@ 100  = 0.1095\n",
      "------第58个epoch------\n",
      "train_loss: -15373.023620605469\n",
      "HR@ 100  = 0.4313\n",
      "NDCG@ 100  = 0.1091\n",
      "------第59个epoch------\n",
      "train_loss: -15388.5358988444\n",
      "HR@ 100  = 0.4311\n",
      "NDCG@ 100  = 0.1093\n",
      "------第60个epoch------\n",
      "train_loss: -15390.502075195312\n",
      "HR@ 100  = 0.4315\n",
      "NDCG@ 100  = 0.1098\n",
      "------第61个epoch------\n",
      "train_loss: -15391.261688232422\n",
      "HR@ 100  = 0.4316\n",
      "NDCG@ 100  = 0.1103\n",
      "------第62个epoch------\n",
      "train_loss: -15381.56851196289\n",
      "HR@ 100  = 0.4313\n",
      "NDCG@ 100  = 0.1100\n",
      "------第63个epoch------\n",
      "train_loss: -15383.467244466146\n",
      "HR@ 100  = 0.4303\n",
      "NDCG@ 100  = 0.1097\n",
      "------第64个epoch------\n",
      "train_loss: -15413.393198649088\n",
      "HR@ 100  = 0.4298\n",
      "NDCG@ 100  = 0.1095\n",
      "------第65个epoch------\n",
      "train_loss: -15406.37266031901\n",
      "HR@ 100  = 0.4323\n",
      "NDCG@ 100  = 0.1103\n",
      "------第66个epoch------\n",
      "train_loss: -15410.63324991862\n",
      "HR@ 100  = 0.4320\n",
      "NDCG@ 100  = 0.1098\n",
      "------第67个epoch------\n",
      "train_loss: -15418.802042643229\n",
      "HR@ 100  = 0.4331\n",
      "NDCG@ 100  = 0.1098\n",
      "------第68个epoch------\n",
      "train_loss: -15427.811548868814\n",
      "HR@ 100  = 0.4336\n",
      "NDCG@ 100  = 0.1099\n",
      "------第69个epoch------\n",
      "train_loss: -15412.57039642334\n",
      "HR@ 100  = 0.4349\n",
      "NDCG@ 100  = 0.1102\n",
      "------第70个epoch------\n",
      "train_loss: -15430.72178141276\n",
      "HR@ 100  = 0.4356\n",
      "NDCG@ 100  = 0.1103\n",
      "------第71个epoch------\n",
      "train_loss: -15437.597295125326\n",
      "HR@ 100  = 0.4386\n",
      "NDCG@ 100  = 0.1109\n",
      "------第72个epoch------\n",
      "train_loss: -15438.20805867513\n",
      "HR@ 100  = 0.4392\n",
      "NDCG@ 100  = 0.1110\n",
      "------第73个epoch------\n",
      "train_loss: -15436.53315226237\n",
      "HR@ 100  = 0.4404\n",
      "NDCG@ 100  = 0.1113\n",
      "------第74个epoch------\n",
      "train_loss: -15441.213826497396\n",
      "HR@ 100  = 0.4391\n",
      "NDCG@ 100  = 0.1108\n",
      "------第75个epoch------\n",
      "train_loss: -15447.941314697266\n",
      "HR@ 100  = 0.4396\n",
      "NDCG@ 100  = 0.1109\n",
      "------第76个epoch------\n",
      "train_loss: -15443.283345540365\n",
      "HR@ 100  = 0.4359\n",
      "NDCG@ 100  = 0.1103\n",
      "------第77个epoch------\n",
      "train_loss: -15453.532719930014\n",
      "HR@ 100  = 0.4364\n",
      "NDCG@ 100  = 0.1101\n",
      "------第78个epoch------\n",
      "train_loss: -15440.758056640625\n",
      "HR@ 100  = 0.4368\n",
      "NDCG@ 100  = 0.1108\n",
      "------第79个epoch------\n",
      "train_loss: -15448.491241455078\n",
      "HR@ 100  = 0.4384\n",
      "NDCG@ 100  = 0.1110\n",
      "------第80个epoch------\n",
      "train_loss: -15456.236043294271\n",
      "HR@ 100  = 0.4381\n",
      "NDCG@ 100  = 0.1110\n",
      "------第81个epoch------\n",
      "train_loss: -15453.150380452475\n",
      "HR@ 100  = 0.4363\n",
      "NDCG@ 100  = 0.1108\n",
      "------第82个epoch------\n",
      "train_loss: -15461.741912841797\n",
      "HR@ 100  = 0.4374\n",
      "NDCG@ 100  = 0.1103\n",
      "------第83个epoch------\n",
      "train_loss: -15465.956385294596\n",
      "HR@ 100  = 0.4377\n",
      "NDCG@ 100  = 0.1105\n",
      "------第84个epoch------\n",
      "train_loss: -15467.761912027994\n",
      "HR@ 100  = 0.4389\n",
      "NDCG@ 100  = 0.1109\n",
      "------第85个epoch------\n",
      "train_loss: -15482.175582885742\n",
      "HR@ 100  = 0.4366\n",
      "NDCG@ 100  = 0.1104\n",
      "------第86个epoch------\n",
      "train_loss: -15480.22651163737\n",
      "HR@ 100  = 0.4381\n",
      "NDCG@ 100  = 0.1103\n",
      "------第87个epoch------\n",
      "train_loss: -15476.796615600586\n",
      "HR@ 100  = 0.4402\n",
      "NDCG@ 100  = 0.1111\n",
      "------第88个epoch------\n",
      "train_loss: -15473.251403808594\n",
      "HR@ 100  = 0.4419\n",
      "NDCG@ 100  = 0.1111\n",
      "------第89个epoch------\n",
      "train_loss: -15487.693659464518\n",
      "HR@ 100  = 0.4407\n",
      "NDCG@ 100  = 0.1112\n",
      "------第90个epoch------\n",
      "train_loss: -15477.004038492838\n",
      "HR@ 100  = 0.4424\n",
      "NDCG@ 100  = 0.1115\n",
      "------第91个epoch------\n",
      "train_loss: -15489.432840983072\n",
      "HR@ 100  = 0.4429\n",
      "NDCG@ 100  = 0.1114\n",
      "------第92个epoch------\n",
      "train_loss: -15484.259480794271\n",
      "HR@ 100  = 0.4397\n",
      "NDCG@ 100  = 0.1108\n"
     ]
    }
   ],
   "source": [
    "def train(train_user, train_item, test, train_matrix, epoch, batch_size, emb_size, lr, drop_out, neg_weight, hhDict, topK, n_user, n_item): \n",
    "    train_loader, test_loader = createLoader(train_user, train_item, test, batch_size)\n",
    "    model, optimizer = createModel(emb_size, lr, n_user, n_item, neg_weight, drop_out, hhDict)\n",
    "    train_loss_list = list()\n",
    "    hr_list = [.0]\n",
    "    ndcg_list = [.0]\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        train_loss = list()\n",
    "        for step, (batch_x1, batch_x2) in enumerate(train_loader):\n",
    "            x1, x2 = batch_x1, batch_x2\n",
    "            if (torch.cuda.is_available()):\n",
    "                x1, x2 = x1.cuda(), x2.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(x1, x2)\n",
    "            loss.backward()        \n",
    "            train_loss.append(loss.cpu().item())\n",
    "            optimizer.step()\n",
    "        print('------第'+str(e+1)+'个epoch------')\n",
    "        mean_train_loss = np.mean(train_loss)\n",
    "        print('train_loss:', mean_train_loss)\n",
    "        train_loss_list.append(mean_train_loss)\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model,test_loader,train_matrix,topK=topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "    #np.savetxt(\"./evalres/enmf/train_loss_list_\"+str(epoch)+\"epoch.txt\", train_loss_list)    \n",
    "    #np.savetxt(\"./evalres/enmf/hr_list_\"+str(epoch)+\"epoch.txt\", hr_list)\n",
    "    #np.savetxt(\"./evalres/enmf/ndcg_list_\"+str(epoch)+\"epoch.txt\", ndcg_list) \n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return model\n",
    "\n",
    "TOPK = 100\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "EMB_SIZE = 64\n",
    "NEG_WEIGHT = 0.1\n",
    "#NEG_WEIGHT = 0\n",
    "DROP_RATIO = 0.3\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 200\n",
    "model = train(train_user, train_item, test, train_matrix, epoch=EPOCH, batch_size=BATCH_SIZE, emb_size=EMB_SIZE, lr=LEARNING_RATE, \n",
    "      drop_out=DROP_RATIO, neg_weight=NEG_WEIGHT, hhDict=hhDict, topK=TOPK, n_user=N_USER, n_item=N_ITEM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./evalres/model/ENMF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_d(train_user, train_item, test, train_matrix, epoch, batch_size, emb_size, lr, drop_out, neg_weight, hhDict, topK, n_user, n_item): \n",
    "    train_loader, test_loader = createLoader(train_user, train_item, test, batch_size)\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    for d in emb_size:\n",
    "        model, optimizer = createModel(d, lr, n_user, n_item, neg_weight, drop_out, hhDict)\n",
    "        model.train()\n",
    "        for e in range(epoch):\n",
    "            for step, (batch_x1, batch_x2) in enumerate(train_loader):\n",
    "                x1, x2 = batch_x1, batch_x2\n",
    "                if (torch.cuda.is_available()):\n",
    "                    x1, x2 = x1.cuda(), x2.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                loss = model(x1, x2)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model,test_loader,train_matrix,topK=topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "        torch.cuda.empty_cache()  \n",
    "    np.savetxt(\"./evalres/enmf/hr_list_d.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/enmf/ndcg_list_d.txt\", ndcg_list)    \n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "TOPK = 100\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "EMB_SIZE = [8,16,32,64]\n",
    "NEG_WEIGHT = 0.1\n",
    "DROP_RATIO = 0.3\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 200\n",
    "train_eval_d(train_user, train_item, test, train_matrix, epoch=EPOCH, batch_size=BATCH_SIZE, emb_size=EMB_SIZE, lr=LEARNING_RATE, \n",
    "      drop_out=DROP_RATIO, neg_weight=NEG_WEIGHT, hhDict=hhDict, topK=TOPK, n_user=N_USER, n_item=N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_topK(train_user, train_item, test, train_matrix, epoch, batch_size, emb_size, lr, drop_out, neg_weight, hhDict, topK, n_user, n_item): \n",
    "    train_loader, test_loader = createLoader(train_user, train_item, test, batch_size)\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()    \n",
    "    model, optimizer = createModel(emb_size, lr, n_user, n_item, neg_weight, drop_out, hhDict)\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        train_loss = list()\n",
    "        for step, (batch_x1, batch_x2) in enumerate(train_loader):\n",
    "            x1, x2 = batch_x1, batch_x2\n",
    "            if (torch.cuda.is_available()):\n",
    "                x1, x2 = x1.cuda(), x2.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(x1, x2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    for k in topK:\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model,test_loader,train_matrix,topK=k)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "    torch.cuda.empty_cache()  \n",
    "    np.savetxt(\"./evalres/enmf/hr_list_topk.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/enmf/ndcg_list_topk.txt\", ndcg_list)    \n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "TOPK = [50,100,200]\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "EMB_SIZE = 64\n",
    "NEG_WEIGHT = 0.1\n",
    "DROP_RATIO = 0.3\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 200\n",
    "train_eval_topK(train_user, train_item, test, train_matrix, epoch=EPOCH, batch_size=BATCH_SIZE, emb_size=EMB_SIZE, lr=LEARNING_RATE, \n",
    "      drop_out=DROP_RATIO, neg_weight=NEG_WEIGHT, hhDict=hhDict, topK=TOPK, n_user=N_USER, n_item=N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_drop_out(train_user, train_item, test, train_matrix, epoch, batch_size, emb_size, lr, drop_out, neg_weight, hhDict, topK, n_user, n_item): \n",
    "    train_loader, test_loader = createLoader(train_user, train_item, test, batch_size)\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    for dout in drop_out:\n",
    "        model, optimizer = createModel(emb_size, lr, n_user, n_item, neg_weight, dout, hhDict)\n",
    "        model.train()\n",
    "        for e in range(epoch):\n",
    "            train_loss = list()\n",
    "            for step, (batch_x1, batch_x2) in enumerate(train_loader):\n",
    "                x1, x2 = batch_x1, batch_x2\n",
    "                if (torch.cuda.is_available()):\n",
    "                    x1, x2 = x1.cuda(), x2.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                loss = model(x1, x2)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model,test_loader,train_matrix,topK=topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "        torch.cuda.empty_cache()  \n",
    "    np.savetxt(\"./evalres/enmf/hr_list_drop_out.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/enmf/ndcg_list_drop_out.txt\", ndcg_list)    \n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "TOPK = 100\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "EMB_SIZE = 64\n",
    "NEG_WEIGHT = 0.1\n",
    "DROP_RATIO = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 200\n",
    "train_eval_drop_out(train_user, train_item, test, train_matrix, epoch=EPOCH, batch_size=BATCH_SIZE, emb_size=EMB_SIZE, lr=LEARNING_RATE, \n",
    "      drop_out=DROP_RATIO, neg_weight=NEG_WEIGHT, hhDict=hhDict, topK=TOPK, n_user=N_USER, n_item=N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENMF(\n",
      "  (user_embs): Embedding(6040, 64)\n",
      "  (item_embs): Embedding(3953, 64)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "HR@ 100  = 0.0361\n",
      "NDCG@ 100  = 0.0062\n",
      "ENMF(\n",
      "  (user_embs): Embedding(6040, 64)\n",
      "  (item_embs): Embedding(3953, 64)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "HR@ 100  = 0.4520\n",
      "NDCG@ 100  = 0.1136\n",
      "ENMF(\n",
      "  (user_embs): Embedding(6040, 64)\n",
      "  (item_embs): Embedding(3953, 64)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "HR@ 100  = 0.4578\n",
      "NDCG@ 100  = 0.1172\n",
      "ENMF(\n",
      "  (user_embs): Embedding(6040, 64)\n",
      "  (item_embs): Embedding(3953, 64)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "HR@ 100  = 0.4439\n",
      "NDCG@ 100  = 0.1164\n",
      "ENMF(\n",
      "  (user_embs): Embedding(6040, 64)\n",
      "  (item_embs): Embedding(3953, 64)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "HR@ 100  = 0.4512\n",
      "NDCG@ 100  = 0.1194\n",
      "ENMF(\n",
      "  (user_embs): Embedding(6040, 64)\n",
      "  (item_embs): Embedding(3953, 64)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "HR@ 100  = 0.4394\n",
      "NDCG@ 100  = 0.1171\n",
      "ENMF(\n",
      "  (user_embs): Embedding(6040, 64)\n",
      "  (item_embs): Embedding(3953, 64)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "HR@ 100  = 0.4402\n",
      "NDCG@ 100  = 0.1175\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "def train_eval_neg_weight(train_user, train_item, test, train_matrix, epoch, batch_size, emb_size, lr, drop_out, neg_weight, hhDict, topK, n_user, n_item): \n",
    "    train_loader, test_loader = createLoader(train_user, train_item, test, batch_size)\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    for nw in neg_weight:\n",
    "        model, optimizer = createModel(emb_size, lr, n_user, n_item, nw, drop_out, hhDict)\n",
    "        model.train()\n",
    "        for e in range(epoch):\n",
    "            train_loss = list()\n",
    "            for step, (batch_x1, batch_x2) in enumerate(train_loader):\n",
    "                x1, x2 = batch_x1, batch_x2\n",
    "                if (torch.cuda.is_available()):\n",
    "                    x1, x2 = x1.cuda(), x2.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                loss = model(x1, x2)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model,test_loader,train_matrix,topK=topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "        torch.cuda.empty_cache()  \n",
    "    np.savetxt(\"./evalres/enmf/hr_list_neg_weight.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/enmf/ndcg_list_neg_weight.txt\", ndcg_list)    \n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "TOPK = 100\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "EMB_SIZE = 64\n",
    "NEG_WEIGHT = [0, 0.1, 0.3, 0.5, 0.7, 0.9, 1]\n",
    "DROP_RATIO = 0.3\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 200\n",
    "train_eval_neg_weight(train_user, train_item, test, train_matrix, epoch=EPOCH, batch_size=BATCH_SIZE, emb_size=EMB_SIZE, lr=LEARNING_RATE, \n",
    "      drop_out=DROP_RATIO, neg_weight=NEG_WEIGHT, hhDict=hhDict, topK=TOPK, n_user=N_USER, n_item=N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENMF(\n",
      "  (user_embs): Embedding(6040, 64)\n",
      "  (item_embs): Embedding(3953, 64)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "time cost: 0.6019976139068604\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def train_eval_time(train_user, train_item, test, train_matrix, epoch, batch_size, emb_size, lr, drop_out, neg_weight, hhDict, topK, n_user, n_item): \n",
    "    train_loader, test_loader = createLoader(train_user, train_item, test, batch_size)\n",
    "    model, optimizer = createModel(emb_size, lr, n_user, n_item, neg_weight, drop_out, hhDict)\n",
    "    model.train()\n",
    "    for e in range(1):\n",
    "        time_start=time.time()\n",
    "        train_loss = list()\n",
    "        for step, (batch_x1, batch_x2) in enumerate(train_loader):\n",
    "            x1, x2 = batch_x1, batch_x2\n",
    "            if (torch.cuda.is_available()):\n",
    "                x1, x2 = x1.cuda(), x2.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(x1, x2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        time_end=time.time()\n",
    "    torch.cuda.empty_cache()  \n",
    "    print('time cost:', time_end-time_start)\n",
    "    np.savetxt(\"./evalres/enmf/single_time.txt\", [time_end-time_start])\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "TOPK = 100\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "EMB_SIZE = 64\n",
    "NEG_WEIGHT = 0.1\n",
    "DROP_RATIO = 0.3\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 200\n",
    "train_eval_time(train_user, train_item, test, train_matrix, epoch=EPOCH, batch_size=BATCH_SIZE, emb_size=EMB_SIZE, lr=LEARNING_RATE, \n",
    "      drop_out=DROP_RATIO, neg_weight=NEG_WEIGHT, hhDict=hhDict, topK=TOPK, n_user=N_USER, n_item=N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
