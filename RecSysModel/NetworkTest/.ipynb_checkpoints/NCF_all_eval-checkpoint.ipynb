{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "SEED = 2019\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"../ml-1m/ratings.dat\",delimiter='::',dtype=int)[:,[0,1,3]]\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_from_local(path, n_user, n_item, n_neg=7):\n",
    "    data = np.loadtxt(fname=path, delimiter=\"\\t\", skiprows=1, dtype=int)\n",
    "    train_matrix = np.zeros((n_user, n_item), dtype = np.int8)\n",
    "    for line in data:\n",
    "        train_matrix[line[0],line[1]] = 1\n",
    "    user_input, item_input, labels = [],[],[]  # x1 x2 -> y\n",
    "    for uno, uitems in enumerate(train_matrix):\n",
    "        positives = np.nonzero(uitems)[0]\n",
    "        n_sample = len(positives) * n_neg\n",
    "        negative_items = list(set(range(n_item))^set(positives))\n",
    "        negatives = np.random.choice(negative_items, n_sample)  # 负采样 -- 不放回\n",
    "        for i in range(len(positives)): # 正实例\n",
    "            user_input.append(uno)\n",
    "            item_input.append(positives[i])\n",
    "            labels.append(1)\n",
    "        for j in range(n_sample): # 负实例\n",
    "            user_input.append(uno)\n",
    "            item_input.append(negatives[j])\n",
    "            labels.append(0)\n",
    "    return np.array(user_input), np.array(item_input), np.array(labels), train_matrix\n",
    "\n",
    "def generate_test_from_local(path, n_user, n_item):\n",
    "    data = np.loadtxt(fname=path, delimiter=\"\\t\", skiprows=1, dtype=int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, gmf_n_factors, layers,  n_user, n_item, activation = torch.relu, batch_normalization = False, n_output = 1):\n",
    "        super(NCF, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.do_bn = batch_normalization\n",
    "        self.fcs = []\n",
    "        self.bns = []\n",
    "        self.n_layer  = len(layers)\n",
    "        parameter_LeCun = np.sqrt(gmf_n_factors + layers[-1])\n",
    "\n",
    "        #self.bn_userInput = nn.BatchNorm1d(1)   # for input data\n",
    "        #self.bn_itemInput = nn.BatchNorm1d(1)   # for input data\n",
    "        \n",
    "        self.mlp_user_embedding_layer = nn.Embedding(n_user, int(layers[0]/2))\n",
    "        self._set_normalInit(self.mlp_user_embedding_layer, hasBias = False) \n",
    "        self.mlp_item_embedding_layer = nn.Embedding(n_item, int(layers[0]/2))\n",
    "        self._set_normalInit(self.mlp_item_embedding_layer, hasBias = False) \n",
    "        \n",
    "        self.gmf_user_embedding_layer = nn.Embedding(n_user, gmf_n_factors)\n",
    "        self._set_normalInit(self.gmf_user_embedding_layer, hasBias = False) \n",
    "        self.gmf_item_embedding_layer = nn.Embedding(n_item, gmf_n_factors)\n",
    "        self._set_normalInit(self.gmf_item_embedding_layer, hasBias = False) \n",
    "        \n",
    "        for i in range(1, self.n_layer):               # build hidden layers and BN layers\n",
    "            fc = nn.Linear(layers[i-1], layers[i])\n",
    "            self._set_normalInit(fc)                  # parameters initialization\n",
    "            setattr(self, 'fc%i' % i, fc)       # IMPORTANT set layer to the Module\n",
    "            self.fcs.append(fc)\n",
    "            if self.do_bn:\n",
    "                bn = nn.BatchNorm1d(layers[i])\n",
    "                setattr(self, 'bn%i' % i, bn)   # IMPORTANT set layer to the Module\n",
    "                self.bns.append(bn)\n",
    "\n",
    "        self.predict = nn.Linear(gmf_n_factors + layers[-1], n_output)         # output layer\n",
    "        self._set_uniformInit(self.predict, parameter = parameter_LeCun)            # parameters initialization\n",
    "        return\n",
    "\n",
    "    def _set_normalInit(self, layer, parameter = [0.0, 0.01], hasBias=True):\n",
    "        init.normal_(layer.weight, mean = parameter[0], std = parameter[1])\n",
    "        if hasBias:\n",
    "            init.normal_(layer.bias, mean = parameter[0], std = parameter[1])\n",
    "        return\n",
    "    \n",
    "    def _set_uniformInit(self, layer, parameter = 5, hasBias = True):\n",
    "        init.uniform_(layer.weight, a = - parameter, b = parameter)\n",
    "        if hasBias:\n",
    "            init.uniform_(layer.bias, a = - parameter, b = parameter)\n",
    "        return\n",
    "    \n",
    "    def _set_heNormalInit(self, layer, hasBias=True):\n",
    "        init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.kaiming_normal_(layer.bias, nonlinearity='relu')\n",
    "        return\n",
    "    \n",
    "    def _set_heUniformInit(self, layer, hasBias=True):\n",
    "        init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.kaiming_uniform_(layer.bias, nonlinearity='relu')\n",
    "        return\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        #if self.do_bn: \n",
    "            #x1 = self.bn_userInput(x1)     # input batch normalization\n",
    "            #x2 = self.bn_itemInput(x2)\n",
    "        mlp_x1 = self.mlp_user_embedding_layer(x1)\n",
    "        mlp_x2 = self.mlp_item_embedding_layer(x2)\n",
    "        \n",
    "        gmf_x1 = self.gmf_user_embedding_layer(x1)\n",
    "        gmf_x2 = self.gmf_item_embedding_layer(x2)\n",
    "        \n",
    "        mlp_x3 = torch.cat((mlp_x1, mlp_x2), dim=1)\n",
    "        mlp_x  = torch.flatten(mlp_x3, start_dim=1)        \n",
    "        for i in range(1, self.n_layer):\n",
    "            mlp_x = self.fcs[i-1](mlp_x)\n",
    "            if self.do_bn: \n",
    "                mlp_x = self.bns[i-1](mlp_x)   # batch normalization\n",
    "            mlp_x = self.activation(mlp_x)\n",
    "        \n",
    "        gmf_x3 = torch.mul(gmf_x1, gmf_x2)\n",
    "        gmf_x  = torch.flatten(gmf_x3, start_dim=1)\n",
    "\n",
    "        x = torch.cat((mlp_x, gmf_x), dim=1)\n",
    "        out = torch.sigmoid(self.predict(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    #HR击中率，如果topk中有正例ID即认为正确\n",
    "    if gtItem in ranklist:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    #NDCG归一化折损累计增益\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return np.log(2) / np.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieEval_1(model, loss_func, test, train_matrix, n_user, n_item, topK = 100):   \n",
    "    item_list = np.array(range(n_item))\n",
    "    item_list = torch.from_numpy(item_list.reshape(-1, 1)).type(torch.LongTensor)\n",
    "    if torch.cuda.is_available():\n",
    "        item_list = item_list.cuda()\n",
    "    hit_list = list()\n",
    "    undcg_list = list()\n",
    "    rank_all_users = list()\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for line in test:\n",
    "            user = line[0]\n",
    "            pos_item = line[1]\n",
    "            user_list = np.array([user for i in range(n_item)])\n",
    "            user_list = torch.from_numpy(user_list.reshape(-1, 1)).type(torch.LongTensor)\n",
    "            if torch.cuda.is_available():\n",
    "                user_list = user_list.cuda()\n",
    "            prediction = model(user_list, item_list)\n",
    "            pred_vector = -1 * (prediction.cpu().data.numpy().reshape(-1))\n",
    "            ranklist = np.argsort(pred_vector)\n",
    "            real_r = list()\n",
    "            i = 0\n",
    "            while len(real_r) < topK:\n",
    "                if train_matrix[user][ranklist[i]] == 0:\n",
    "                    real_r.append(ranklist[i])\n",
    "                i += 1     \n",
    "            rank_all_users.append(real_r)\n",
    "            hit_list.append(getHitRatio(real_r, pos_item))\n",
    "            undcg_list.append(getNDCG(real_r, pos_item))\n",
    "    model.train()\n",
    "    hr = np.mean(hit_list)\n",
    "    ndcg = np.mean(undcg_list)\n",
    "    print('HR@', topK, ' = %.4f' %  hr)\n",
    "    print('NDCG@', topK, ' = %.4f' % ndcg)\n",
    "    return hr, ndcg, rank_all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLoader(train_user, train_item, train_label, batch_size):\n",
    "    torch_x1 = torch.from_numpy(train_user.reshape(-1, 1)).type(torch.LongTensor)\n",
    "    torch_x2 = torch.from_numpy(train_item.reshape(-1, 1)).type(torch.LongTensor)\n",
    "    torch_y  = torch.from_numpy(train_label.reshape(-1, 1)).type(torch.FloatTensor)\n",
    "\n",
    "    torch_dataset = data_utils.TensorDataset(torch_x1, torch_x2, torch_y)\n",
    "    loader = data_utils.DataLoader(dataset = torch_dataset, batch_size = batch_size, shuffle = True, num_workers = 0)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(n_factors, layers, lr, n_user, n_item):\n",
    "    ncf = NCF(gmf_n_factors = n_factors, layers = layers, n_user = n_user, n_item = n_item, \n",
    "              activation = torch.relu, batch_normalization = False, n_output = 1)\n",
    "    loss_func = torch.nn.BCELoss()\n",
    "    if(torch.cuda.is_available()):\n",
    "        ncf = ncf.cuda()\n",
    "        loss_func = loss_func.cuda()\n",
    "    optimizer = torch.optim.Adam(ncf.parameters(), lr = lr)\n",
    "    print(ncf)\n",
    "    return ncf, loss_func, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "------第1个epoch------\n",
      "train_loss = 0.2277\n",
      "------第2个epoch------\n",
      "train_loss = 0.1837\n",
      "------第3个epoch------\n",
      "train_loss = 0.1687\n",
      "------第4个epoch------\n",
      "train_loss = 0.1602\n",
      "------第5个epoch------\n",
      "train_loss = 0.1542\n",
      "------第6个epoch------\n",
      "train_loss = 0.1495\n",
      "HR@ 100  = 0.3709\n",
      "NDCG@ 100  = 0.0949\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "train_user, train_item, train_label, train_matrix = generate_train_from_local(path=\"../ml-1m/ml.train.txt\",n_user=N_USER, n_item=N_ITEM)\n",
    "test = generate_test_from_local(path=\"../ml-1m/ml.test.txt\", n_user=N_USER, n_item=N_ITEM)\n",
    "\n",
    "def train(train_user, train_item, train_label, test, train_matrix, epoch, batch_size, n_factors, layers, lr, topK, n_user, n_item):    \n",
    "    loader = createLoader(train_user, train_item, train_label, batch_size)\n",
    "    model, loss_func, optimizer = createModel(n_factors, layers, lr, n_user, n_item)\n",
    "    train_loss_list = list()\n",
    "    hr_list = [0.0]\n",
    "    ndcg_list = [0.0]\n",
    "    for e in range(epoch):\n",
    "        train_loss = list()\n",
    "        for step, (batch_x1, batch_x2, batch_y) in enumerate(loader):\n",
    "            if torch.cuda.is_available():\n",
    "                batch_x1, batch_x2, batch_y = batch_x1.cuda(), batch_x2.cuda(), batch_y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch_x1, batch_x2)\n",
    "            loss = loss_func(prediction, batch_y) \n",
    "            loss.backward()        \n",
    "            train_loss.append(loss.cpu().item())\n",
    "            optimizer.step()\n",
    "        print('------第'+str(e+1)+'个epoch------')\n",
    "        mean_train_loss = np.mean(train_loss)\n",
    "        print('train_loss', '= %.4f' % mean_train_loss)\n",
    "        train_loss_list.append(mean_train_loss)  \n",
    "    '''\n",
    "        if (e+1)%5==0:\n",
    "            hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user=n_user, n_item=n_item, topK=topK)\n",
    "            hr_list.append(hr)\n",
    "            ndcg_list.append(ndcg)\n",
    "    np.savetxt(\"./evalres/ncf/train_loss_list_\"+str(epoch)+\"epoch.txt\", train_loss_list)    \n",
    "    np.savetxt(\"./evalres/ncf/hr_list_\"+str(epoch)+\"epoch.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/ncf/ndcg_list_\"+str(epoch)+\"epoch.txt\", ndcg_list) \n",
    "    '''\n",
    "    movieEval_1(model, loss_func, test, train_matrix, n_user=n_user, n_item=n_item, topK=topK)\n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return model\n",
    "\n",
    "# Hyper parameters\n",
    "ACTIVATION = torch.relu\n",
    "TOPK = 100\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 200\n",
    "LAYERS = [128, 64, 32, 16, 8]    # MLP  0层为输入层  0层/2为嵌入层  \n",
    "GMF_N_FACTORS  = 64          # GMF隐层size  \n",
    "#train(train_user, train_item, train_label, test, train_matrix, epoch=EPOCH, batch_size=BATCH_SIZE, n_factors=GMF_N_FACTORS, layers=LAYERS, lr=LEARNING_RATE, topK=TOPK, n_user = N_USER, n_item = N_ITEM)\n",
    "model = train(train_user, train_item, train_label, test, train_matrix, epoch=6, batch_size=BATCH_SIZE, n_factors=GMF_N_FACTORS, layers=LAYERS, lr=LEARNING_RATE, topK=TOPK, n_user = N_USER, n_item = N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./evalres/model/NCF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@ 100  = 0.3543\n",
      "NDCG@ 100  = 0.0876\n",
      "HR@ 100  = 0.3892\n",
      "NDCG@ 100  = 0.0930\n",
      "HR@ 100  = 0.3858\n",
      "NDCG@ 100  = 0.0954\n",
      "HR@ 100  = 0.3760\n",
      "NDCG@ 100  = 0.0964\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "train_user, train_item, train_label, train_matrix = generate_train_from_local(path=\"../ml-1m/ml.train.txt\",n_user=N_USER, n_item=N_ITEM)\n",
    "test = generate_test_from_local(path=\"../ml-1m/ml.test.txt\", n_user=N_USER, n_item=N_ITEM)\n",
    "\n",
    "def train_eval_d(train_user, train_item, train_label, test, train_matrix, epoch, batch_size, n_factors, layers, lr, topK, n_user, n_item):    \n",
    "    loader = createLoader(train_user, train_item, train_label, batch_size)\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    for i, d in enumerate(n_factors):\n",
    "        model, loss_func, optimizer = createModel(d, layers[i], lr, n_user, n_item)\n",
    "        model.train()\n",
    "        for e in range(epoch):\n",
    "            train_loss = list()\n",
    "            for step, (batch_x1, batch_x2, batch_y) in enumerate(loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_x1, batch_x2, batch_y = batch_x1.cuda(), batch_x2.cuda(), batch_y.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                prediction = model(batch_x1, batch_x2)\n",
    "                loss = loss_func(prediction, batch_y) \n",
    "                loss.backward() \n",
    "                optimizer.step()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user=n_user, n_item=n_item, topK=topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "    np.savetxt(\"./evalres/ncf/hr_list_d.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/ncf/ndcg_list_d.txt\", ndcg_list) \n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "# Hyper parameters\n",
    "ACTIVATION = torch.relu\n",
    "TOPK = 100\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 6\n",
    "LAYERS = [[16,8,4,2],[32,16,8,4],[64,32,16,8],[128,64,32,16,8]]    # MLP  0层为输入层  0层/2为嵌入层  \n",
    "GMF_N_FACTORS  = [8,16,32,64]          # GMF隐层size  \n",
    "train_eval_d(train_user, train_item, train_label, test, train_matrix, epoch=EPOCH, batch_size=BATCH_SIZE, n_factors=GMF_N_FACTORS, layers=LAYERS, lr=LEARNING_RATE, topK=TOPK, n_user = N_USER, n_item = N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 50  = 0.2402\n",
      "NDCG@ 50  = 0.0728\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.3525\n",
      "NDCG@ 100  = 0.0886\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 200  = 0.5414\n",
      "NDCG@ 200  = 0.1198\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "train_user, train_item, train_label, train_matrix = generate_train_from_local(path=\"../ml-1m/ml.train.txt\",n_user=N_USER, n_item=N_ITEM)\n",
    "test = generate_test_from_local(path=\"../ml-1m/ml.test.txt\", n_user=N_USER, n_item=N_ITEM)\n",
    "\n",
    "def train_eval_topK(train_user, train_item, train_label, test, train_matrix, epoch, batch_size, n_factors, layers, lr, topK, n_user, n_item):    \n",
    "    loader = createLoader(train_user, train_item, train_label, batch_size)\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()    \n",
    "    model, loss_func, optimizer = createModel(n_factors, layers, lr, n_user, n_item)\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        for step, (batch_x1, batch_x2, batch_y) in enumerate(loader):\n",
    "            if torch.cuda.is_available():\n",
    "                batch_x1, batch_x2, batch_y = batch_x1.cuda(), batch_x2.cuda(), batch_y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch_x1, batch_x2)\n",
    "            loss = loss_func(prediction, batch_y) \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "    for k in topK:\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user=n_user, n_item=n_item, topK=k)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "    np.savetxt(\"./evalres/ncf/hr_list_topk.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/ncf/ndcg_list_topk.txt\", ndcg_list) \n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "# Hyper parameters\n",
    "ACTIVATION = torch.relu\n",
    "TOPK = [50,100,200]\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 6\n",
    "LAYERS = [128, 64, 32, 16, 8]    # MLP  0层为输入层  0层/2为嵌入层  \n",
    "GMF_N_FACTORS  = 64          # GMF隐层size  \n",
    "train_eval_topK(train_user, train_item, train_label, test, train_matrix, epoch=EPOCH, batch_size=BATCH_SIZE, n_factors=GMF_N_FACTORS, layers=LAYERS, lr=LEARNING_RATE, topK=TOPK, n_user = N_USER, n_item = N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.2896\n",
      "NDCG@ 100  = 0.0714\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.3349\n",
      "NDCG@ 100  = 0.0825\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.3611\n",
      "NDCG@ 100  = 0.0897\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.3636\n",
      "NDCG@ 100  = 0.0920\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.3583\n",
      "NDCG@ 100  = 0.0897\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.3848\n",
      "NDCG@ 100  = 0.0954\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.4041\n",
      "NDCG@ 100  = 0.1037\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.4025\n",
      "NDCG@ 100  = 0.1031\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.4071\n",
      "NDCG@ 100  = 0.1016\n",
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.3639\n",
      "NDCG@ 100  = 0.0930\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "def train_eval_negNum(n_neg, epoch, batch_size, n_factors, layers, lr, topK, n_user, n_item):\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    test = generate_test_from_local(\"../ml-1m/ml.test.txt\", n_user, n_item)\n",
    "    for n in n_neg:\n",
    "        train_user,train_item,train_label,train_matrix=generate_train_from_local(\"../ml-1m/ml.train.txt\", n_user, n_item, n_neg=n)\n",
    "        loader = createLoader(train_user, train_item, train_label, batch_size)\n",
    "        model, loss_func, optimizer = createModel(n_factors, layers, lr, n_user, n_item)\n",
    "        model.train()\n",
    "        for e in range(epoch):\n",
    "            for step, (batch_x1, batch_x2, batch_y) in enumerate(loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    batch_x1, batch_x2, batch_y = batch_x1.cuda(), batch_x2.cuda(), batch_y.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                prediction = model(batch_x1, batch_x2)\n",
    "                loss = loss_func(prediction, batch_y) \n",
    "                loss.backward() \n",
    "                optimizer.step()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user=n_user, n_item=n_item, topK=topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg) \n",
    "        torch.cuda.empty_cache()\n",
    "    np.savetxt(\"./evalres/ncf/hr_list_neg.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/ncf/ndcg_list_neg.txt\", ndcg_list)\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "ACTIVATION = torch.relu\n",
    "TOPK = 100\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 6\n",
    "LAYERS = [128, 64, 32, 16, 8]    # MLP  0层为输入层  0层/2为嵌入层  \n",
    "GMF_N_FACTORS  = 64          # GMF隐层size  \n",
    "N_NEG = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "train_eval_negNum(n_neg=N_NEG, epoch=EPOCH, batch_size=BATCH_SIZE, n_factors=GMF_N_FACTORS, layers=LAYERS, lr=LEARNING_RATE, topK=TOPK, n_user = N_USER, n_item = N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF(\n",
      "  (mlp_user_embedding_layer): Embedding(6040, 64)\n",
      "  (mlp_item_embedding_layer): Embedding(3952, 64)\n",
      "  (gmf_user_embedding_layer): Embedding(6040, 64)\n",
      "  (gmf_item_embedding_layer): Embedding(3952, 64)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc3): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (fc4): Linear(in_features=16, out_features=8, bias=True)\n",
      "  (predict): Linear(in_features=72, out_features=1, bias=True)\n",
      ")\n",
      "time cost: 201.6445610523224\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def train_eval_time(n_neg, epoch, batch_size, n_factors, layers, lr, topK, n_user, n_item):\n",
    "    test = generate_test_from_local(\"../ml-1m/ml.test.txt\", n_user, n_item)\n",
    "    train_user,train_item,train_label,train_matrix=generate_train_from_local(\"../ml-1m/ml.train.txt\", n_user, n_item, n_neg)\n",
    "    loader = createLoader(train_user, train_item, train_label, batch_size)\n",
    "    model, loss_func, optimizer = createModel(n_factors, layers, lr, n_user, n_item)\n",
    "    model.train()\n",
    "    time_start, time_end = 0, 0\n",
    "    for e in range(1):\n",
    "        time_start=time.time()\n",
    "        for step, (batch_x1, batch_x2, batch_y) in enumerate(loader):\n",
    "            if torch.cuda.is_available():\n",
    "                batch_x1, batch_x2, batch_y = batch_x1.cuda(), batch_x2.cuda(), batch_y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch_x1, batch_x2)\n",
    "            loss = loss_func(prediction, batch_y) \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "        time_end=time.time()\n",
    "    print('time cost:', time_end-time_start)\n",
    "    torch.cuda.empty_cache()\n",
    "    np.savetxt(\"./evalres/ncf/single_time.txt\", [time_end-time_start]) \n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "ACTIVATION = torch.relu\n",
    "TOPK = 100\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 6\n",
    "LAYERS = [128, 64, 32, 16, 8]    # MLP  0层为输入层  0层/2为嵌入层  \n",
    "GMF_N_FACTORS  = 64          # GMF隐层size  \n",
    "N_NEG = 4\n",
    "train_eval_time(n_neg=N_NEG, epoch=EPOCH, batch_size=BATCH_SIZE, n_factors=GMF_N_FACTORS, layers=LAYERS, lr=LEARNING_RATE, topK=TOPK, n_user = N_USER, n_item = N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
