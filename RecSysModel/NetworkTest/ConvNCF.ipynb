{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = np.loadtxt('../Yelp/yelp.rating', usecols=[0,1,3], dtype=int)\n",
    "dataset = np.loadtxt('../ml-1m/ratings.dat', delimiter='::', usecols=[0,1,3], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_SIZE = [64,32,16,8,4,2,1]\n",
    "N_FM = 32\n",
    "N_USERS = np.max(dataset[:,0])\n",
    "N_ITEMS = np.max(dataset[:,1])\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 512\n",
    "EPOCH = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_items = np.zeros((N_USERS+1,N_ITEMS+1), dtype=np.int8)\n",
    "for line in dataset:\n",
    "    users_items[line[0],line[1]] = 1\n",
    "users_items_train = users_items.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pos_test = list() # 作为测试集的交互正例\n",
    "for i in range(N_USERS+1):\n",
    "    if i==0: \n",
    "        continue\n",
    "    uitems = dataset[dataset[:,0]==i]\n",
    "    onepos = uitems[uitems[:,-1]==np.max(uitems),:2][0]   # 每个用户取时间戳最大的交互\n",
    "    u_pos_test.append(onepos)\n",
    "    users_items_train[onepos[0], onepos[1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_batch(interact_matrix, test, shuffle = True, drop_last = False, batch_size = BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    构造训练用的三元组\n",
    "    对于随机抽出的用户u，i可以从user_ratings随机抽出，而j也是从总的电影集中随机抽出，当然j必须保证(u,j)不在user_ratings中\n",
    "    \"\"\"\n",
    "    n_users = interact_matrix.shape[0]\n",
    "    n_items = interact_matrix.shape[1]\n",
    "    one_epoch = list()   # 当前epoch\n",
    "    index = np.array(range(n_users))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index)  # 用户索引shuffle\n",
    "    for start_index in range(0, n_users, batch_size):\n",
    "        if (start_index+batch_size) > n_users and drop_last:  # 不保留最后一个不满batch_size长度的batch\n",
    "            continue\n",
    "        end_index = n_users if (start_index+batch_size) > n_users else (start_index+batch_size)\n",
    "        users = index[start_index: end_index]   # 当前batch的用户编号\n",
    "        temp = interact_matrix[users]           # 当前batch用户的所有正负交互\n",
    "        one_batch = list()  # 当前batch\n",
    "        for u, line in enumerate(temp):\n",
    "            if users[u] == 0:\n",
    "                continue\n",
    "            P = np.nonzero(line)[0]                # 该用户users[u]的所有正例\n",
    "            N = list(set(range(n_items))^set(P))   # 该用户users[u]的所有负例\n",
    "            i = np.random.choice(P, 1)[0]\n",
    "            j = np.random.choice(N, 1)[0]\n",
    "            '''\n",
    "            while j == test[users[u]][1]:    # 若j存在于测试集正例中\n",
    "                j = np.random.choice(N, 1)[0]\n",
    "            '''\n",
    "            one_batch.append([users[u],i,j])\n",
    "        one_epoch.append(np.array(one_batch))\n",
    "    return np.array(one_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_batch(users_items, u_pos_test, negatives_sample_size = 999):\n",
    "    utest = list()\n",
    "    itest = list()\n",
    "    for ui in u_pos_test:\n",
    "        u = ui[0]\n",
    "        i = ui[1]    \n",
    "        P = np.nonzero(users_items[u])[0]\n",
    "        N = list(set(range(N_ITEMS+1))^set(P))\n",
    "        negatives_sample = np.random.choice(N, negatives_sample_size)  # 负采样 -- 不放回\n",
    "        negatives = [i]  # 正例\n",
    "        for n in negatives_sample:\n",
    "            negatives.append(n)  # 添加负例\n",
    "        utest.append([u for j in range(negatives_sample_size+1)])\n",
    "        itest.append(negatives)\n",
    "    return np.array(utest), np.array(itest)\n",
    "utest, itest = generate_test_batch(users_items, u_pos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNCF(nn.Module):\n",
    "    def __init__(self, fm_sizes, n_users, n_items, n_fm=N_FM, myStride=2, n_output=1):\n",
    "        ''' e.g.--> fm_sizes = [64,32,16,8,4,2,1] '''\n",
    "        super(ConvNCF, self).__init__()\n",
    "        self.convs = list()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.user_embedding_layer = nn.Embedding(n_users+1, fm_sizes[0])\n",
    "        self._set_normalInit(self.user_embedding_layer, hasBias = False) \n",
    "        #self._set_xavierInit(self.user_embedding_layer, hasBias = False)\n",
    "        #self._set_heInit(self.user_embedding_layer, hasBias = False) \n",
    "        self.item_embedding_layer = nn.Embedding(n_items+1, fm_sizes[0])\n",
    "        self._set_normalInit(self.item_embedding_layer, hasBias = False) \n",
    "        #self._set_xavierInit(self.item_embedding_layer, hasBias = False)\n",
    "        #self._set_heInit(self.item_embedding_layer, hasBias = False) \n",
    "        for i in range(1, len(fm_sizes)):\n",
    "            inChannel = 1 if i == 1 else n_fm\n",
    "            #conv = nn.Conv2d(in_channels=inChannel, out_channels=32, kernel_size=fm_sizes[i]+myStride, stride=myStride)\n",
    "            conv = nn.Conv2d(in_channels=inChannel, out_channels=n_fm, kernel_size=4, stride=myStride, padding=1)\n",
    "            #self._set_normalInit(conv)\n",
    "            #self._set_xavierInit(conv)\n",
    "            self._set_heInit(conv)\n",
    "            setattr(self, 'conv%i' % i, conv)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.predict = nn.Linear(n_fm, n_output)         # output layer\n",
    "        self._set_xavierInit(self.predict)            # parameters initialization\n",
    "        return\n",
    "    \n",
    "    def _set_xavierInit(self, layer, hasBias = True):\n",
    "        init.xavier_uniform_(layer.weight)\n",
    "        if hasBias:\n",
    "            init.constant_(layer.bias, 0.01)\n",
    "        return\n",
    "    \n",
    "    def _set_heInit(self, layer, hasBias = True):\n",
    "        init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.constant_(layer.bias, 0.01)\n",
    "        return\n",
    "    \n",
    "    def _set_normalInit(self, layer, parameter = [0.0, 0.1], hasBias = True):\n",
    "        init.normal_(layer.weight, mean = parameter[0], std = parameter[1])\n",
    "        if hasBias:\n",
    "            init.constant_(layer.bias, 0.01)\n",
    "        return\n",
    "    \n",
    "    def _set_uniformInit(self, layer, parameter = 1, hasBias = True):\n",
    "        init.uniform_(layer.weight, a = 0, b = parameter)\n",
    "        if hasBias:\n",
    "            init.uniform_(layer.bias, a = 0, b = parameter)\n",
    "        return\n",
    "    \n",
    "    def forward(self, user, item_pos, item_neg, train = True):\n",
    "        user = self.user_embedding_layer(user)\n",
    "        item_pos = self.item_embedding_layer(item_pos)\n",
    "        if train:\n",
    "            item_neg = self.item_embedding_layer(item_neg)\n",
    "        x1, x2 = None, None\n",
    "        temp1, temp2 = list(), list() \n",
    "        out1, out2 = None, None\n",
    "        for i in range(user.size()[0]):\n",
    "            temp1.append(torch.mm(user[i].T, item_pos[i]))\n",
    "            if train:\n",
    "                temp2.append(torch.mm(user[i].T, item_neg[i]))\n",
    "        x1 = torch.stack(temp1)\n",
    "        x1 = x1.view(x1.size()[0], -1, x1.size()[1], x1.size()[2])\n",
    "        if train:\n",
    "            x2 = torch.stack(temp2)\n",
    "            x2 = x2.view(x2.size()[0], -1, x2.size()[1], x2.size()[2])\n",
    "        ''' ## conv2d -input  (batch_size, channel, weight, height) '''\n",
    "        for conv in self.convs:\n",
    "            x1 = torch.relu(conv(x1))\n",
    "            if train:\n",
    "                x2 = torch.relu(conv(x2))\n",
    "        ''' ## conv2d -output (batch_size, out_channel, out_weight, out_height) '''\n",
    "        x1 = torch.flatten(x1, start_dim = 1)\n",
    "        x1 = self.dropout(x1)\n",
    "        if train:\n",
    "            x2 = torch.flatten(x2, start_dim = 1)\n",
    "            x2 = self.dropout(x2)\n",
    "        #out1 = torch.sigmoid(self.dropout(self.predict(x1)))\n",
    "        out1 = torch.sigmoid(self.predict(x1))\n",
    "        if train:\n",
    "            #out2 = torch.sigmoid(self.dropout(self.predict(x2)))\n",
    "            out2 = torch.sigmoid(self.predict(x2))\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(y1, y2):\n",
    "    return torch.sum(torch.log(1+torch.exp(-(y1 - y2))))\n",
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, y1, y2):\n",
    "        return torch.sum(torch.log(1+torch.exp(-(y1 - y2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6041, 64)\n",
      "  (item_embedding_layer): Embedding(3953, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvNCF(fm_sizes=FM_SIZE, n_fm=N_FM, n_users=N_USERS, n_items=N_ITEMS)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay=0.001)\n",
    "loss_func = My_loss()\n",
    "if(torch.cuda.is_available()):\n",
    "    model = model.cuda()\n",
    "    loss_func = loss_func.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    #HR击中率，如果topk中有正例ID即认为正确\n",
    "    if gtItem in ranklist:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    #NDCG归一化折损累计增益\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return np.log(2) / np.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def getH(ranklist1, ranklist2):\n",
    "    L = len(ranklist1)\n",
    "    common = len(list(set(ranklist1).intersection(set(ranklist2))))\n",
    "    return 1-common/L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieEval_1(model, loss_func, utest, itest, topK = 100):\n",
    "    if len(utest)==len(itest):\n",
    "        n_users = len(utest)\n",
    "    else:\n",
    "        print('the length of test sets are not equal.')\n",
    "        return\n",
    "    hit = 0\n",
    "    undcg = 0\n",
    "    rank_all_users = list()\n",
    "    #test_loss = list()\n",
    "    hr = 0\n",
    "    ndcg = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_users):\n",
    "            map_item_score = dict()\n",
    "            x1test = Variable(torch.from_numpy(utest[i].reshape(-1, 1)).type(torch.LongTensor))\n",
    "            x2test = Variable(torch.from_numpy(itest[i].reshape(-1, 1)).type(torch.LongTensor))\n",
    "            #y  = torch.from_numpy(ytest[i].reshape(-1, 1)).type(torch.FloatTensor)\n",
    "            #x1test, x2test, y = x1test.cuda(), x2test.cuda(), y.cuda()\n",
    "            x1test, x2test = x1test.cuda(), x2test.cuda()\n",
    "            prediction, _ = model(x1test, x2test, None, train = False)\n",
    "            #print(prediction)\n",
    "            #loss = loss_func(prediction, y)\n",
    "            #test_loss.append(loss.cpu().item())\n",
    "            pred_vector = prediction.cpu().data.numpy().T[0]\n",
    "            positive_item = itest[i][0]  # 取正例\n",
    "            for j in range(len(itest[i])):\n",
    "                map_item_score[itest[i][j]] = pred_vector[j]\n",
    "            ranklist = heapq.nlargest(topK, map_item_score, key=map_item_score.get)\n",
    "            rank_all_users.append(ranklist)\n",
    "            hit += getHitRatio(ranklist, positive_item)\n",
    "            undcg += getNDCG(ranklist, positive_item)\n",
    "        #mean_test_loss = np.mean(test_loss)\n",
    "        hr = hit / n_users\n",
    "        ndcg = undcg / n_users\n",
    "    #print('test_loss:', mean_test_loss)\n",
    "    print('HR@', topK, ' = %.4f' % hr)\n",
    "    print('NDCG@', topK, ' = %.4f' % ndcg)\n",
    "    return hr, ndcg, rank_all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------第1个epoch------\n",
      "train_loss: 348.95637766520184\n",
      "------第2个epoch------\n",
      "train_loss: 348.9585952758789\n",
      "------第3个epoch------\n",
      "train_loss: 348.8658803304036\n",
      "HR@ 100  = 0.1386\n",
      "NDCG@ 100  = 0.0294\n",
      "------第4个epoch------\n",
      "train_loss: 348.79567464192706\n",
      "------第5个epoch------\n",
      "train_loss: 347.4679590861003\n",
      "------第6个epoch------\n",
      "train_loss: 320.5345993041992\n",
      "HR@ 100  = 0.4571\n",
      "NDCG@ 100  = 0.1073\n",
      "------第7个epoch------\n",
      "train_loss: 289.3668390909831\n",
      "------第8个epoch------\n",
      "train_loss: 275.2731056213379\n",
      "------第9个epoch------\n",
      "train_loss: 266.8892822265625\n",
      "HR@ 100  = 0.4927\n",
      "NDCG@ 100  = 0.1344\n",
      "------第10个epoch------\n",
      "train_loss: 258.20362599690753\n",
      "------第11个epoch------\n",
      "train_loss: 255.66196950276694\n",
      "------第12个epoch------\n",
      "train_loss: 254.80242029825845\n",
      "HR@ 100  = 0.4959\n",
      "NDCG@ 100  = 0.1613\n",
      "------第13个epoch------\n",
      "train_loss: 257.0183970133464\n",
      "------第14个epoch------\n",
      "train_loss: 256.4720827738444\n",
      "------第15个epoch------\n",
      "train_loss: 256.8952166239421\n",
      "HR@ 100  = 0.4909\n",
      "NDCG@ 100  = 0.1737\n",
      "------第16个epoch------\n",
      "train_loss: 254.88804626464844\n",
      "------第17个epoch------\n",
      "train_loss: 252.53397369384766\n",
      "------第18个epoch------\n",
      "train_loss: 251.0420939127604\n",
      "HR@ 100  = 0.4997\n",
      "NDCG@ 100  = 0.2459\n",
      "------第19个epoch------\n",
      "train_loss: 250.94266255696616\n",
      "------第20个epoch------\n",
      "train_loss: 254.4355150858561\n",
      "------第21个epoch------\n",
      "train_loss: 252.6280085245768\n",
      "HR@ 100  = 0.5230\n",
      "NDCG@ 100  = 0.3877\n",
      "------第22个epoch------\n",
      "train_loss: 253.37935002644858\n",
      "------第23个epoch------\n",
      "train_loss: 250.3937085469564\n",
      "------第24个epoch------\n",
      "train_loss: 251.95151392618814\n",
      "HR@ 100  = 0.5018\n",
      "NDCG@ 100  = 0.2500\n",
      "------第25个epoch------\n",
      "train_loss: 250.41587702433267\n",
      "------第26个epoch------\n",
      "train_loss: 253.36306762695312\n",
      "------第27个epoch------\n",
      "train_loss: 252.73274358113608\n",
      "HR@ 100  = 0.5179\n",
      "NDCG@ 100  = 0.3457\n",
      "------第28个epoch------\n",
      "train_loss: 249.25223922729492\n",
      "------第29个epoch------\n",
      "train_loss: 248.06661987304688\n",
      "------第30个epoch------\n",
      "train_loss: 248.83727010091147\n",
      "HR@ 100  = 0.5202\n",
      "NDCG@ 100  = 0.3649\n",
      "------第31个epoch------\n",
      "train_loss: 246.54349009195963\n",
      "------第32个epoch------\n",
      "train_loss: 251.2095947265625\n",
      "------第33个epoch------\n",
      "train_loss: 248.45088322957358\n",
      "HR@ 100  = 0.5301\n",
      "NDCG@ 100  = 0.4018\n",
      "------第34个epoch------\n",
      "train_loss: 248.04569244384766\n",
      "------第35个epoch------\n",
      "train_loss: 247.66894149780273\n",
      "------第36个epoch------\n",
      "train_loss: 248.22625986735025\n",
      "HR@ 100  = 0.5543\n",
      "NDCG@ 100  = 0.4693\n",
      "------第37个epoch------\n",
      "train_loss: 251.28961817423502\n",
      "------第38个epoch------\n",
      "train_loss: 248.68201446533203\n",
      "------第39个epoch------\n",
      "train_loss: 250.80800755818686\n",
      "HR@ 100  = 0.5477\n",
      "NDCG@ 100  = 0.4439\n",
      "------第40个epoch------\n",
      "train_loss: 248.2516771952311\n",
      "------第41个epoch------\n",
      "train_loss: 245.39533360799155\n",
      "------第42个epoch------\n",
      "train_loss: 250.70968373616537\n",
      "HR@ 100  = 0.5533\n",
      "NDCG@ 100  = 0.4557\n",
      "------第43个epoch------\n",
      "train_loss: 248.58684285481772\n",
      "------第44个epoch------\n",
      "train_loss: 247.789306640625\n",
      "------第45个epoch------\n",
      "train_loss: 246.67007319132486\n",
      "HR@ 100  = 0.5515\n",
      "NDCG@ 100  = 0.4474\n",
      "------第46个epoch------\n",
      "train_loss: 245.65579477945963\n",
      "------第47个epoch------\n",
      "train_loss: 243.64996337890625\n",
      "------第48个epoch------\n",
      "train_loss: 245.65297317504883\n",
      "HR@ 100  = 0.5217\n",
      "NDCG@ 100  = 0.3253\n",
      "------第49个epoch------\n",
      "train_loss: 248.32602055867514\n",
      "------第50个epoch------\n",
      "train_loss: 245.58151499430338\n",
      "------第51个epoch------\n",
      "train_loss: 243.1890614827474\n",
      "HR@ 100  = 0.5685\n",
      "NDCG@ 100  = 0.4923\n",
      "------第52个epoch------\n",
      "train_loss: 248.2350616455078\n",
      "------第53个epoch------\n",
      "train_loss: 244.6050542195638\n",
      "------第54个epoch------\n",
      "train_loss: 244.69208399454752\n",
      "HR@ 100  = 0.6089\n",
      "NDCG@ 100  = 0.5723\n",
      "------第55个epoch------\n",
      "train_loss: 248.56068801879883\n",
      "------第56个epoch------\n",
      "train_loss: 247.8259162902832\n",
      "------第57个epoch------\n",
      "train_loss: 246.15975443522134\n",
      "HR@ 100  = 0.5300\n",
      "NDCG@ 100  = 0.3606\n",
      "------第58个epoch------\n",
      "train_loss: 246.91816202799478\n",
      "------第59个epoch------\n",
      "train_loss: 245.3245938618978\n",
      "------第60个epoch------\n",
      "train_loss: 243.91973876953125\n",
      "HR@ 100  = 0.5687\n",
      "NDCG@ 100  = 0.4964\n"
     ]
    }
   ],
   "source": [
    "train_loss_list = list()\n",
    "test_loss_list  = list()\n",
    "hr_list = list()\n",
    "ndcg_list = list()\n",
    "for e in range(EPOCH):\n",
    "    data = generate_train_batch(users_items_train, u_pos_test)\n",
    "    train_loss = list()\n",
    "    for train_batch in data:\n",
    "        u = Variable(torch.from_numpy(train_batch[:,0].reshape(-1, 1)).type(torch.LongTensor))\n",
    "        i = Variable(torch.from_numpy(train_batch[:,1].reshape(-1, 1)).type(torch.LongTensor))\n",
    "        j = Variable(torch.from_numpy(train_batch[:,2].reshape(-1, 1)).type(torch.LongTensor))\n",
    "        if (torch.cuda.is_available()):\n",
    "            u, i, j = u.cuda(), i.cuda(), j.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        yui, yuj = model(u, i, j)\n",
    "        loss = loss_func(yui, yuj) \n",
    "        loss.backward()  \n",
    "        train_loss.append(loss.cpu().item())\n",
    "        optimizer.step()\n",
    "    print('------第'+str(e+1)+'个epoch------')\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    print('train_loss:', mean_train_loss)\n",
    "    train_loss_list.append(mean_train_loss)\n",
    "    if (e+1)%3 == 0:\n",
    "        model.eval()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model, loss_func, utest, itest)\n",
    "        model.train()\n",
    "        #test_loss_list.append(test_loss)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
