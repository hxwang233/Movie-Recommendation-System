{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "SEED = 2019\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('../ml-1m/ratings.dat', delimiter='::', usecols=[0,1,3], dtype=int)\n",
    "#dataset = np.loadtxt('../Yelp/yelp.rating', usecols=[0,1,3], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_from_local(path, n_user, n_item):\n",
    "    data = np.loadtxt(fname=path, delimiter=\"\\t\", skiprows=1, dtype=int)\n",
    "    train_matrix = np.zeros((n_user, n_item), dtype = np.int8)\n",
    "    for line in data:\n",
    "        train_matrix[line[0],line[1]] = 1\n",
    "    user_pos = dict()\n",
    "    max_item_id = train_matrix.shape[1]\n",
    "    max_item_num = 0\n",
    "    for u, i in enumerate(train_matrix):\n",
    "        pos_item = list(np.nonzero(i)[0])\n",
    "        pos_item_num = len(pos_item)\n",
    "        if  pos_item_num > max_item_num:\n",
    "            max_item_num = pos_item_num\n",
    "        user_pos[u] = pos_item\n",
    "    train_user = list()\n",
    "    train_item = list()\n",
    "    for k in user_pos.keys():\n",
    "        while len(user_pos[k]) < max_item_num:\n",
    "            user_pos[k].append(max_item_id)\n",
    "        train_user.append(k)\n",
    "        train_item.append(user_pos[k])\n",
    "    return np.array(train_user), np.array(train_item), train_matrix\n",
    "\n",
    "def generate_test_from_local(path):\n",
    "    data = np.loadtxt(fname=path, delimiter=\"\\t\", skiprows=1, dtype=int)\n",
    "    return data\n",
    "\n",
    "train_user, train_item, train_matrix = generate_train_from_local(path=\"../ml-1m/ml.train.txt\",n_user=N_USER,n_item=N_ITEM)\n",
    "test = generate_test_from_local(path=\"../ml-1m/ml.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "EMB_SIZE = 64\n",
    "NEG_WEIGHT = 0.1\n",
    "DROP_RATIO = 0.3\n",
    "LEARNING_RATE = 0.05\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENMF(nn.Module):\n",
    "    def __init__(self, emb_size, n_user, n_item, neg_weight, drop_out, count, c0=512, x=0.6):\n",
    "        super().__init__()\n",
    "        self.c0 = c0\n",
    "        self.x  = x\n",
    "        self.count = count\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.neg_weight = neg_weight\n",
    "        self.emb_size   = emb_size\n",
    "        self.user_embs = nn.Embedding(n_user, emb_size)\n",
    "        self.item_embs = nn.Embedding(n_item+1, emb_size)\n",
    "        self.h = nn.Parameter(torch.randn(emb_size, 1))\n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "        self.freq = self.calcu_freq()\n",
    "        self._reset_para()\n",
    "        return\n",
    "    \n",
    "    def _reset_para(self):\n",
    "        nn.init.xavier_normal_(self.user_embs.weight)\n",
    "        nn.init.xavier_normal_(self.item_embs.weight)\n",
    "        nn.init.constant_(self.h, 0.01)\n",
    "        return\n",
    "    \n",
    "    def calcu_freq(self):\n",
    "        freq_items = sorted(self.count.keys())\n",
    "        freq_count = [self.count[k] for k in freq_items]\n",
    "        freq = np.zeros(self.item_embs.weight.shape[0])\n",
    "        freq[freq_items] = freq_count       \n",
    "        #freq = freq/np.sum(freq)\n",
    "        freq = np.power(freq, self.x)\n",
    "        freq = self.c0 * freq/np.sum(freq)\n",
    "        freq = torch.from_numpy(freq).type(torch.float).cuda()\n",
    "        return freq\n",
    "    \n",
    "    def forward(self, uids, pos_iids):\n",
    "        '''\n",
    "        uids: B\n",
    "        u_iids: B * L\n",
    "        '''\n",
    "        u_emb = self.dropout(self.user_embs(uids))\n",
    "        pos_embs = self.item_embs(pos_iids)\n",
    "\n",
    "        # torch.einsum(\"ab,abc->abc\")\n",
    "        # B * L * D\n",
    "        mask = (~(pos_iids.eq(self.n_item))).float()\n",
    "        pos_embs = pos_embs * mask.unsqueeze(2)\n",
    "\n",
    "        # torch.einsum(\"ac,abc->abc\")\n",
    "        # B * L * D\n",
    "        pq = u_emb.unsqueeze(1) * pos_embs\n",
    "        # torch.einsum(\"ajk,kl->ajl\")\n",
    "        # B * L\n",
    "        hpq = pq.matmul(self.h).squeeze(2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # loss\n",
    "        pos_data_loss = torch.sum((1 - self.neg_weight) * hpq.square() - 2.0 * hpq)\n",
    "\n",
    "        # torch.einsum(\"ab,ac->abc\")\n",
    "        part_1 = self.item_embs.weight.unsqueeze(2).bmm(self.item_embs.weight.unsqueeze(1))\n",
    "        part_2 = u_emb.unsqueeze(2).bmm(u_emb.unsqueeze(1))\n",
    "\n",
    "        # D * D\n",
    "        part_1 = part_1.sum(0)\n",
    "        part_2 = part_2.sum(0)\n",
    "        part_3 = self.h.mm(self.h.t())\n",
    "        all_data_loss = torch.sum(part_1 * part_2 * part_3)\n",
    "\n",
    "        loss = self.neg_weight * all_data_loss + pos_data_loss\n",
    "        return loss\n",
    "    \n",
    "    def rank(self, uid):\n",
    "        '''\n",
    "        uid: Batch_size\n",
    "        '''\n",
    "        uid_embs = self.user_embs(uid)\n",
    "        user_all_items = uid_embs.unsqueeze(1) * self.item_embs.weight\n",
    "        items_score = user_all_items.matmul(self.h).squeeze(2)\n",
    "        return items_score\n",
    "    \n",
    "'''    def rank(self, user):\n",
    "        res = self.user_embs(user).unsqueeze(0)\n",
    "        res = res * self.item_embs.weight\n",
    "        res = res.matmul(self.h).squeeze(1)\n",
    "        return res'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    #HR击中率，如果topk中有正例ID即认为正确\n",
    "    if gtItem in ranklist:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    #NDCG归一化折损累计增益\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return np.log(2) / np.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def getH(ranklist1, ranklist2):\n",
    "    L = len(ranklist1)\n",
    "    common = len(list(set(ranklist1).intersection(set(ranklist2))))\n",
    "    return 1-common/L\n",
    "\n",
    "def movieEval_1(model, test_loader, train_matrix, topK = 100):\n",
    "    n_users = train_matrix.shape[0]\n",
    "    hit_list = list()\n",
    "    undcg_list = list()\n",
    "    rank_all_users = list()\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for step, (batch_x, batch_y) in enumerate(test_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                batch_x = batch_x.cuda()  \n",
    "            prediction = model.rank(batch_x)\n",
    "            pred_vector = -1 * (prediction.cpu().data.numpy())\n",
    "            ranklist = np.argsort(pred_vector)\n",
    "            for j, r in enumerate(ranklist):\n",
    "                real_r = list()\n",
    "                u = batch_x[j].cpu().data.numpy()\n",
    "                i = 0\n",
    "                while len(real_r) < topK:\n",
    "                    if r[i]==train_matrix.shape[1]:\n",
    "                        continue\n",
    "                    if train_matrix[u][r[i]] == 0:\n",
    "                        real_r.append(r[i])\n",
    "                    i += 1     \n",
    "                rank_all_users.append(real_r)\n",
    "                pos_item = batch_y[j].cpu().data.numpy()\n",
    "                hit_list.append(getHitRatio(real_r, pos_item))\n",
    "                undcg_list.append(getNDCG(real_r, pos_item))\n",
    "    model.train()\n",
    "    hr = np.mean(hit_list)\n",
    "    ndcg = np.mean(undcg_list)\n",
    "    print('HR@', topK, ' = %.4f' %  hr)\n",
    "    print('NDCG@', topK, ' = %.4f' % ndcg)\n",
    "    return hr, ndcg, rank_all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq_model ENMF(\n",
      "  (user_embs): Embedding(6041, 64)\n",
      "  (item_embs): Embedding(3954, 64)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "tail_model ENMF(\n",
      "  (user_embs): Embedding(6041, 64)\n",
      "  (item_embs): Embedding(3954, 64)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def createLoader(train_user, train_item, test, batch_size):\n",
    "    torch_x1 = torch.from_numpy(train_user).type(torch.LongTensor)\n",
    "    torch_x2 = torch.from_numpy(train_item).type(torch.LongTensor)\n",
    "    torch_test = torch.from_numpy(test).type(torch.LongTensor)\n",
    "    torch_dataset = data_utils.TensorDataset(torch_x1, torch_x2)\n",
    "    train_loader = data_utils.DataLoader(dataset = torch_dataset, batch_size = batch_size, shuffle = True, num_workers = 0)\n",
    "    torch_testset = data_utils.TensorDataset(torch_test[:,0],torch_test[:,1])\n",
    "    test_loader = data_utils.DataLoader(dataset = torch_testset, batch_size = batch_size, num_workers = 0)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(emb_size, lr, n_user, n_item, neg_weight, drop_out, hhDict):\n",
    "    model = ENMF(emb_size=emb_size, n_user=n_user, n_item=n_item, neg_weight=neg_weight, drop_out = drop_out, count=hhDict)\n",
    "    if(torch.cuda.is_available()):\n",
    "        model = model.cuda()\n",
    "    optimizer = torch.optim.Adagrad(model.parameters(), lr = lr)\n",
    "    print(model)\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------第1个epoch------\n",
      "train_loss: 0.07700866061107565\n",
      "HR@ 100  = 0.0387\n",
      "NDCG@ 100  = 0.0096\n",
      "------第2个epoch------\n",
      "train_loss: -26.551939477523167\n",
      "HR@ 100  = 0.0720\n",
      "NDCG@ 100  = 0.0177\n",
      "------第3个epoch------\n",
      "train_loss: -467.6944483121236\n",
      "HR@ 100  = 0.0998\n",
      "NDCG@ 100  = 0.0241\n",
      "------第4个epoch------\n",
      "train_loss: -2683.3884785970054\n",
      "HR@ 100  = 0.1131\n",
      "NDCG@ 100  = 0.0271\n",
      "------第5个epoch------\n",
      "train_loss: -7152.594375610352\n",
      "HR@ 100  = 0.1219\n",
      "NDCG@ 100  = 0.0293\n",
      "------第6个epoch------\n",
      "train_loss: -11161.950764973959\n",
      "HR@ 100  = 0.1263\n",
      "NDCG@ 100  = 0.0294\n",
      "------第7个epoch------\n",
      "train_loss: -13850.70258585612\n",
      "HR@ 100  = 0.1250\n",
      "NDCG@ 100  = 0.0292\n",
      "------第8个epoch------\n",
      "train_loss: -14911.8918355306\n",
      "HR@ 100  = 0.1250\n",
      "NDCG@ 100  = 0.0297\n",
      "------第9个epoch------\n",
      "train_loss: -15480.157491048178\n",
      "HR@ 100  = 0.1361\n",
      "NDCG@ 100  = 0.0326\n",
      "------第10个epoch------\n",
      "train_loss: -15909.849416097006\n",
      "HR@ 100  = 0.1401\n",
      "NDCG@ 100  = 0.0338\n",
      "------第11个epoch------\n",
      "train_loss: -16193.700480143229\n",
      "HR@ 100  = 0.1454\n",
      "NDCG@ 100  = 0.0352\n",
      "------第12个epoch------\n",
      "train_loss: -16500.435546875\n",
      "HR@ 100  = 0.1487\n",
      "NDCG@ 100  = 0.0362\n",
      "------第13个epoch------\n",
      "train_loss: -16683.103037516277\n",
      "HR@ 100  = 0.1545\n",
      "NDCG@ 100  = 0.0371\n",
      "------第14个epoch------\n",
      "train_loss: -16927.789967854816\n",
      "HR@ 100  = 0.1540\n",
      "NDCG@ 100  = 0.0369\n",
      "------第15个epoch------\n",
      "train_loss: -17118.330576578777\n",
      "HR@ 100  = 0.1604\n",
      "NDCG@ 100  = 0.0380\n",
      "------第16个epoch------\n",
      "train_loss: -17303.25210571289\n",
      "HR@ 100  = 0.1634\n",
      "NDCG@ 100  = 0.0389\n",
      "------第17个epoch------\n",
      "train_loss: -17465.335408528645\n",
      "HR@ 100  = 0.1687\n",
      "NDCG@ 100  = 0.0401\n",
      "------第18个epoch------\n",
      "train_loss: -17601.842732747395\n",
      "HR@ 100  = 0.1719\n",
      "NDCG@ 100  = 0.0401\n",
      "------第19个epoch------\n",
      "train_loss: -17698.920928955078\n",
      "HR@ 100  = 0.1702\n",
      "NDCG@ 100  = 0.0403\n",
      "------第20个epoch------\n",
      "train_loss: -17860.85605875651\n",
      "HR@ 100  = 0.1705\n",
      "NDCG@ 100  = 0.0404\n",
      "------第21个epoch------\n",
      "train_loss: -17939.867126464844\n",
      "HR@ 100  = 0.1733\n",
      "NDCG@ 100  = 0.0409\n",
      "------第22个epoch------\n",
      "train_loss: -18030.215087890625\n",
      "HR@ 100  = 0.1765\n",
      "NDCG@ 100  = 0.0414\n",
      "------第23个epoch------\n",
      "train_loss: -18174.306213378906\n",
      "HR@ 100  = 0.1773\n",
      "NDCG@ 100  = 0.0410\n",
      "------第24个epoch------\n",
      "train_loss: -18254.024556477863\n",
      "HR@ 100  = 0.1828\n",
      "NDCG@ 100  = 0.0419\n",
      "------第25个epoch------\n",
      "train_loss: -18386.884440104168\n",
      "HR@ 100  = 0.1834\n",
      "NDCG@ 100  = 0.0422\n",
      "------第26个epoch------\n",
      "train_loss: -18458.394409179688\n",
      "HR@ 100  = 0.1841\n",
      "NDCG@ 100  = 0.0423\n",
      "------第27个epoch------\n",
      "train_loss: -18509.059611002605\n",
      "HR@ 100  = 0.1871\n",
      "NDCG@ 100  = 0.0426\n",
      "------第28个epoch------\n",
      "train_loss: -18606.625528971355\n",
      "HR@ 100  = 0.1879\n",
      "NDCG@ 100  = 0.0425\n",
      "------第29个epoch------\n",
      "train_loss: -18738.28065999349\n",
      "HR@ 100  = 0.1917\n",
      "NDCG@ 100  = 0.0434\n",
      "------第30个epoch------\n",
      "train_loss: -18832.01649983724\n",
      "HR@ 100  = 0.1904\n",
      "NDCG@ 100  = 0.0431\n",
      "------第31个epoch------\n",
      "train_loss: -18821.67832438151\n",
      "HR@ 100  = 0.1907\n",
      "NDCG@ 100  = 0.0434\n",
      "------第32个epoch------\n",
      "train_loss: -18949.90301513672\n",
      "HR@ 100  = 0.1911\n",
      "NDCG@ 100  = 0.0435\n",
      "------第33个epoch------\n",
      "train_loss: -18975.77490234375\n",
      "HR@ 100  = 0.1921\n",
      "NDCG@ 100  = 0.0438\n",
      "------第34个epoch------\n",
      "train_loss: -19034.275756835938\n",
      "HR@ 100  = 0.1929\n",
      "NDCG@ 100  = 0.0441\n",
      "------第35个epoch------\n",
      "train_loss: -19109.466247558594\n",
      "HR@ 100  = 0.1924\n",
      "NDCG@ 100  = 0.0437\n",
      "------第36个epoch------\n",
      "train_loss: -19107.060943603516\n",
      "HR@ 100  = 0.1947\n",
      "NDCG@ 100  = 0.0441\n",
      "------第37个epoch------\n",
      "train_loss: -19189.898864746094\n",
      "HR@ 100  = 0.1983\n",
      "NDCG@ 100  = 0.0448\n",
      "------第38个epoch------\n",
      "train_loss: -19253.568603515625\n",
      "HR@ 100  = 0.1990\n",
      "NDCG@ 100  = 0.0445\n",
      "------第39个epoch------\n",
      "train_loss: -19253.020914713543\n",
      "HR@ 100  = 0.1952\n",
      "NDCG@ 100  = 0.0437\n",
      "------第40个epoch------\n",
      "train_loss: -19319.785909016926\n",
      "HR@ 100  = 0.1990\n",
      "NDCG@ 100  = 0.0444\n",
      "------第41个epoch------\n",
      "train_loss: -19356.58009847005\n",
      "HR@ 100  = 0.1962\n",
      "NDCG@ 100  = 0.0440\n",
      "------第42个epoch------\n",
      "train_loss: -19402.692443847656\n",
      "HR@ 100  = 0.1960\n",
      "NDCG@ 100  = 0.0440\n",
      "------第43个epoch------\n",
      "train_loss: -19444.769755045574\n",
      "HR@ 100  = 0.1972\n",
      "NDCG@ 100  = 0.0440\n",
      "------第44个epoch------\n",
      "train_loss: -19513.19600423177\n",
      "HR@ 100  = 0.2015\n",
      "NDCG@ 100  = 0.0448\n",
      "------第45个epoch------\n",
      "train_loss: -19499.141174316406\n",
      "HR@ 100  = 0.2002\n",
      "NDCG@ 100  = 0.0449\n",
      "------第46个epoch------\n",
      "train_loss: -19572.26045735677\n",
      "HR@ 100  = 0.1977\n",
      "NDCG@ 100  = 0.0446\n",
      "------第47个epoch------\n",
      "train_loss: -19583.190775553387\n",
      "HR@ 100  = 0.2018\n",
      "NDCG@ 100  = 0.0449\n",
      "------第48个epoch------\n",
      "train_loss: -19590.74871826172\n",
      "HR@ 100  = 0.2002\n",
      "NDCG@ 100  = 0.0445\n",
      "------第49个epoch------\n",
      "train_loss: -19663.429545084637\n",
      "HR@ 100  = 0.1995\n",
      "NDCG@ 100  = 0.0444\n",
      "------第50个epoch------\n",
      "train_loss: -19633.89434814453\n"
     ]
    }
   ],
   "source": [
    "train_user, train_item, train_label, train_matrix = generate_train_from_local(path=\"../ml-1m/ml.train.txt\",n_user=N_USER, n_item=N_ITEM)\n",
    "test = generate_test_from_local(path=\"../ml-1m/ml.test.txt\", n_user=N_USER, n_item=N_ITEM)\n",
    "\n",
    "def train(train_user, train_item, train_label, test, train_matrix, epoch, batch_size, n_factors, layers, lr, topK, n_user, n_item):    \n",
    "    loader = createLoader(train_user, train_item, train_label, batch_size)\n",
    "    model, loss_func, optimizer = createModel(n_factors, layers, lr, n_user, n_item)\n",
    "    train_loss_list = list()\n",
    "    hr_list = [0.0]\n",
    "    ndcg_list = [0.0]\n",
    "    for e in range(epoch):\n",
    "        train_loss = list()\n",
    "        for step, (batch_x1, batch_x2, batch_y) in enumerate(loader):\n",
    "            if torch.cuda.is_available():\n",
    "                batch_x1, batch_x2, batch_y = batch_x1.cuda(), batch_x2.cuda(), batch_y.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(batch_x1, batch_x2)\n",
    "            loss = loss_func(prediction, batch_y) \n",
    "            loss.backward()        \n",
    "            train_loss.append(loss.cpu().item())\n",
    "            optimizer.step()\n",
    "        print('------第'+str(e+1)+'个epoch------')\n",
    "        mean_train_loss = np.mean(train_loss)\n",
    "        print('train_loss', '= %.4f' % mean_train_loss)\n",
    "        train_loss_list.append(mean_train_loss)  \n",
    "    '''\n",
    "        if (e+1)%5==0:\n",
    "            hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user=n_user, n_item=n_item, topK=topK)\n",
    "            hr_list.append(hr)\n",
    "            ndcg_list.append(ndcg)\n",
    "    np.savetxt(\"./evalres/ncf/train_loss_list_\"+str(epoch)+\"epoch.txt\", train_loss_list)    \n",
    "    np.savetxt(\"./evalres/ncf/hr_list_\"+str(epoch)+\"epoch.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/ncf/ndcg_list_\"+str(epoch)+\"epoch.txt\", ndcg_list) \n",
    "    '''\n",
    "    movieEval_1(model, loss_func, test, train_matrix, n_user=n_user, n_item=n_item, topK=topK)\n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return model\n",
    "\n",
    "# Hyper parameters\n",
    "ACTIVATION = torch.relu\n",
    "TOPK = 100\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 200\n",
    "LAYERS = [128, 64, 32, 16, 8]    # MLP  0层为输入层  0层/2为嵌入层  \n",
    "GMF_N_FACTORS  = 64          # GMF隐层size  \n",
    "#train(train_user, train_item, train_label, test, train_matrix, epoch=EPOCH, batch_size=BATCH_SIZE, n_factors=GMF_N_FACTORS, layers=LAYERS, lr=LEARNING_RATE, topK=TOPK, n_user = N_USER, n_item = N_ITEM)\n",
    "model = train(train_user, train_item, train_label, test, train_matrix, epoch=6, batch_size=BATCH_SIZE, n_factors=GMF_N_FACTORS, layers=LAYERS, lr=LEARNING_RATE, topK=TOPK, n_user = N_USER, n_item = N_ITEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
