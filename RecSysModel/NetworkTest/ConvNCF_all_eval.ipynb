{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "SEED = 2019\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('../ml-1m/ratings.dat', delimiter='::', usecols=[0,1,3], dtype=int)\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_from_local(path, n_user, n_item):\n",
    "    data = np.loadtxt(fname=path, delimiter=\"\\t\", skiprows=1, dtype=int)\n",
    "    train_matrix = np.zeros((n_user, n_item), dtype = np.int8)\n",
    "    for line in data:\n",
    "        train_matrix[line[0],line[1]] = 1\n",
    "    return train_matrix\n",
    "\n",
    "def generate_test_from_local(path):\n",
    "    data = np.loadtxt(fname=path, delimiter=\"\\t\", skiprows=1, dtype=int)\n",
    "    return data\n",
    "\n",
    "train_matrix = generate_train_from_local(path=\"../ml-1m/ml.train.txt\",n_user=N_USER, n_item=N_ITEM)\n",
    "test = generate_test_from_local(path=\"../ml-1m/ml.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_batch(interact_matrix, batch_size, neg_num=1, shuffle = True, drop_last = False):\n",
    "    \"\"\"\n",
    "    构造训练用的三元组\n",
    "    对于随机抽出的用户u，i可以从user_ratings随机抽出，而j也是从总的电影集中随机抽出，当然j必须保证(u,j)不在user_ratings中\n",
    "    \"\"\"\n",
    "    n_users = interact_matrix.shape[0]\n",
    "    n_items = interact_matrix.shape[1]\n",
    "    one_epoch = list()   # 当前epoch\n",
    "    index = np.array(range(n_users))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index)  # 用户索引shuffle\n",
    "    for start_index in range(0, n_users, batch_size):\n",
    "        if (start_index+batch_size) > n_users and drop_last:  # 不保留最后一个不满batch_size长度的batch\n",
    "            continue\n",
    "        end_index = n_users if (start_index+batch_size) > n_users else (start_index+batch_size)\n",
    "        users = index[start_index: end_index]   # 当前batch的用户编号\n",
    "        temp = interact_matrix[users]           # 当前batch用户的所有正负交互\n",
    "        one_batch = list()  # 当前batch\n",
    "        for u, line in enumerate(temp):\n",
    "            P = np.nonzero(line)[0]      # 该用户users[u]的所有正例\n",
    "            N = np.nonzero(line-1)[0]    # 该用户users[u]的所有负例\n",
    "            i = np.random.choice(P, 1)[0]\n",
    "            neg_item = np.random.choice(N, neg_num, replace=False)\n",
    "            for j in neg_item:\n",
    "                one_batch.append([users[u],i,j])\n",
    "        one_epoch.append(np.array(one_batch))\n",
    "    return np.array(one_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNCF(nn.Module):\n",
    "    def __init__(self, fm_sizes, n_user, n_item, n_fm, drop_out, myStride=2, n_output=1):\n",
    "        ''' e.g.--> fm_sizes = [64,32,16,8,4,2,1] '''\n",
    "        super(ConvNCF, self).__init__()\n",
    "        self.convs = list()\n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "        self.user_embedding_layer = nn.Embedding(n_user, fm_sizes[0])\n",
    "        self._set_normalInit(self.user_embedding_layer, hasBias = False) \n",
    "        #self._set_xavierInit(self.user_embedding_layer, hasBias = False)\n",
    "        #self._set_heInit(self.user_embedding_layer, hasBias = False) \n",
    "        self.item_embedding_layer = nn.Embedding(n_item, fm_sizes[0])\n",
    "        self._set_normalInit(self.item_embedding_layer, hasBias = False) \n",
    "        #self._set_xavierInit(self.item_embedding_layer, hasBias = False)\n",
    "        #self._set_heInit(self.item_embedding_layer, hasBias = False) \n",
    "        for i in range(1, len(fm_sizes)):\n",
    "            inChannel = 1 if i == 1 else n_fm\n",
    "            #conv = nn.Conv2d(in_channels=inChannel, out_channels=32, kernel_size=fm_sizes[i]+myStride, stride=myStride)\n",
    "            conv = nn.Conv2d(in_channels=inChannel, out_channels=n_fm, kernel_size=4, stride=myStride, padding=1)\n",
    "            #self._set_normalInit(conv)\n",
    "            #self._set_xavierInit(conv)\n",
    "            self._set_heInit(conv)\n",
    "            setattr(self, 'conv%i' % i, conv)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.predict = nn.Linear(n_fm, n_output)         # output layer\n",
    "        self._set_xavierInit(self.predict)            # parameters initialization\n",
    "        return\n",
    "    \n",
    "    def _set_xavierInit(self, layer, hasBias = True):\n",
    "        init.xavier_uniform_(layer.weight)\n",
    "        if hasBias:\n",
    "            init.constant_(layer.bias, 0.01)\n",
    "        return\n",
    "    \n",
    "    def _set_heInit(self, layer, hasBias = True):\n",
    "        init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.constant_(layer.bias, 0.01)\n",
    "        return\n",
    "    \n",
    "    def _set_normalInit(self, layer, parameter = [0.0, 0.1], hasBias = True):\n",
    "        init.normal_(layer.weight, mean = parameter[0], std = parameter[1])\n",
    "        if hasBias:\n",
    "            init.constant_(layer.bias, 0.01)\n",
    "        return\n",
    "    \n",
    "    def _set_uniformInit(self, layer, parameter = 1, hasBias = True):\n",
    "        init.uniform_(layer.weight, a = 0, b = parameter)\n",
    "        if hasBias:\n",
    "            init.uniform_(layer.bias, a = 0, b = parameter)\n",
    "        return\n",
    "    \n",
    "    def forward(self, user, item_pos, item_neg, train = True):\n",
    "        user = self.user_embedding_layer(user)\n",
    "        item_pos = self.item_embedding_layer(item_pos)\n",
    "        if train:\n",
    "            item_neg = self.item_embedding_layer(item_neg)\n",
    "        x1, x2 = None, None\n",
    "        temp1, temp2 = list(), list() \n",
    "        out1, out2 = None, None\n",
    "        for i in range(user.size()[0]):\n",
    "            temp1.append(torch.mm(user[i].T, item_pos[i]))\n",
    "            if train:\n",
    "                temp2.append(torch.mm(user[i].T, item_neg[i]))\n",
    "        x1 = torch.stack(temp1)\n",
    "        x1 = x1.view(x1.size()[0], -1, x1.size()[1], x1.size()[2])\n",
    "        if train:\n",
    "            x2 = torch.stack(temp2)\n",
    "            x2 = x2.view(x2.size()[0], -1, x2.size()[1], x2.size()[2])\n",
    "        ''' ## conv2d -input  (batch_size, channel, weight, height) '''\n",
    "        for conv in self.convs:\n",
    "            x1 = torch.relu(conv(x1))\n",
    "            if train:\n",
    "                x2 = torch.relu(conv(x2))\n",
    "        ''' ## conv2d -output (batch_size, out_channel, out_weight, out_height) '''\n",
    "        x1 = torch.flatten(x1, start_dim = 1)\n",
    "        x1 = self.dropout(x1)\n",
    "        if train:\n",
    "            x2 = torch.flatten(x2, start_dim = 1)\n",
    "            x2 = self.dropout(x2)\n",
    "        #out1 = torch.sigmoid(self.dropout(self.predict(x1)))\n",
    "        out1 = torch.sigmoid(self.predict(x1))\n",
    "        if train:\n",
    "            #out2 = torch.sigmoid(self.dropout(self.predict(x2)))\n",
    "            out2 = torch.sigmoid(self.predict(x2))\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, y1, y2):\n",
    "        return torch.sum(torch.log(1+torch.exp(-(y1 - y2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    #HR击中率，如果topk中有正例ID即认为正确\n",
    "    if gtItem in ranklist:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    #NDCG归一化折损累计增益\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return np.log(2) / np.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieEval_1(model, loss_func, test, train_matrix, n_user, n_item, topK = 100):   \n",
    "    item_list = np.array(range(n_item))\n",
    "    item_list = torch.from_numpy(item_list.reshape(-1, 1)).type(torch.LongTensor)\n",
    "    if torch.cuda.is_available():\n",
    "        item_list = item_list.cuda()\n",
    "    hit_list = list()\n",
    "    undcg_list = list()\n",
    "    rank_all_users = list()\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for line in test:\n",
    "            user = line[0]\n",
    "            pos_item = line[1]\n",
    "            user_list = np.array([user for i in range(n_item)])\n",
    "            user_list = torch.from_numpy(user_list.reshape(-1, 1)).type(torch.LongTensor)\n",
    "            if torch.cuda.is_available():\n",
    "                user_list = user_list.cuda()\n",
    "            prediction, _ = model(user_list, item_list, None, train = False)\n",
    "            pred_vector = -1 * (prediction.cpu().data.numpy().reshape(-1))\n",
    "            ranklist = np.argsort(pred_vector)\n",
    "            real_r = list()\n",
    "            i = 0\n",
    "            while len(real_r) < topK:\n",
    "                if train_matrix[user][ranklist[i]] == 0:\n",
    "                    real_r.append(ranklist[i])\n",
    "                i += 1     \n",
    "            rank_all_users.append(real_r)\n",
    "            hit_list.append(getHitRatio(real_r, pos_item))\n",
    "            undcg_list.append(getNDCG(real_r, pos_item))\n",
    "    model.train()\n",
    "    hr = np.mean(hit_list)\n",
    "    ndcg = np.mean(undcg_list)\n",
    "    print('HR@', topK, ' = %.4f' %  hr)\n",
    "    print('NDCG@', topK, ' = %.4f' % ndcg)\n",
    "    return hr, ndcg, rank_all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(fm_sizes, n_fm, lr, drop_out, n_user, n_item):\n",
    "    model = ConvNCF(fm_sizes=fm_sizes, n_fm=n_fm, drop_out=drop_out, n_user=n_user, n_item=n_item)\n",
    "    loss_func = My_loss()\n",
    "    if(torch.cuda.is_available()):\n",
    "        model = model.cuda()\n",
    "        loss_func = loss_func.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay=0.001)\n",
    "    print(model)\n",
    "    return model, loss_func, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "------第1个epoch------\n",
      "train_loss: 348.89106973012287\n",
      "------第2个epoch------\n",
      "train_loss: 340.2500733534495\n",
      "------第3个epoch------\n",
      "train_loss: 290.7422326405843\n",
      "------第4个epoch------\n",
      "train_loss: 276.81707469622296\n",
      "------第5个epoch------\n",
      "train_loss: 268.5294523239136\n",
      "------第6个epoch------\n",
      "train_loss: 263.9873646100362\n",
      "------第7个epoch------\n",
      "train_loss: 262.37355462710065\n",
      "------第8个epoch------\n",
      "train_loss: 258.6440494855245\n",
      "------第9个epoch------\n",
      "train_loss: 256.5001958211263\n",
      "------第10个epoch------\n",
      "train_loss: 257.0308195749919\n",
      "------第11个epoch------\n",
      "train_loss: 251.29650036493936\n",
      "------第12个epoch------\n",
      "train_loss: 251.88747398058572\n",
      "------第13个epoch------\n",
      "train_loss: 251.58112915356955\n",
      "------第14个epoch------\n",
      "train_loss: 249.74063523610434\n",
      "------第15个epoch------\n",
      "train_loss: 249.49407831827799\n",
      "------第16个epoch------\n",
      "train_loss: 250.36655632654825\n",
      "------第17个epoch------\n",
      "train_loss: 247.76767428716025\n",
      "------第18个epoch------\n",
      "train_loss: 252.22931067148843\n",
      "------第19个epoch------\n",
      "train_loss: 250.61897293726602\n",
      "------第20个epoch------\n",
      "train_loss: 249.39382219314575\n",
      "------第21个epoch------\n",
      "train_loss: 248.95710929234824\n",
      "------第22个epoch------\n",
      "train_loss: 248.10512272516885\n",
      "------第23个epoch------\n",
      "train_loss: 248.89306497573853\n",
      "------第24个epoch------\n",
      "train_loss: 247.49261236190796\n",
      "------第25个epoch------\n",
      "train_loss: 247.12747160593668\n",
      "------第26个epoch------\n",
      "train_loss: 247.73344723383585\n",
      "------第27个epoch------\n",
      "train_loss: 246.4023036956787\n",
      "------第28个epoch------\n",
      "train_loss: 245.40889692306519\n",
      "------第29个epoch------\n",
      "train_loss: 244.66503516832987\n",
      "------第30个epoch------\n",
      "train_loss: 244.68990604082742\n",
      "------第31个epoch------\n",
      "train_loss: 244.28316060702005\n",
      "------第32个epoch------\n",
      "train_loss: 244.70458817481995\n",
      "------第33个epoch------\n",
      "train_loss: 246.53936457633972\n",
      "------第34个epoch------\n",
      "train_loss: 247.35456371307373\n",
      "------第35个epoch------\n",
      "train_loss: 244.3161064783732\n",
      "------第36个epoch------\n",
      "train_loss: 247.21754455566406\n",
      "------第37个epoch------\n",
      "train_loss: 246.18091090520224\n",
      "------第38个epoch------\n",
      "train_loss: 244.6450481414795\n",
      "------第39个epoch------\n",
      "train_loss: 246.02927446365356\n",
      "------第40个epoch------\n",
      "train_loss: 245.69127941131592\n",
      "------第41个epoch------\n",
      "train_loss: 244.99415143330893\n",
      "------第42个epoch------\n",
      "train_loss: 244.45712447166443\n",
      "------第43个epoch------\n",
      "train_loss: 245.59866587320963\n",
      "------第44个epoch------\n",
      "train_loss: 245.9622008005778\n",
      "------第45个epoch------\n",
      "train_loss: 243.8567194143931\n",
      "------第46个epoch------\n",
      "train_loss: 241.40259623527527\n",
      "------第47个epoch------\n",
      "train_loss: 245.6533991495768\n",
      "------第48个epoch------\n",
      "train_loss: 247.83904043833414\n",
      "------第49个epoch------\n",
      "train_loss: 245.55482165018717\n",
      "------第50个epoch------\n",
      "train_loss: 244.9495267868042\n",
      "------第51个epoch------\n",
      "train_loss: 245.80218124389648\n",
      "------第52个epoch------\n",
      "train_loss: 246.0713472366333\n",
      "------第53个epoch------\n",
      "train_loss: 244.94754711786905\n",
      "------第54个epoch------\n",
      "train_loss: 243.95030037562051\n",
      "------第55个epoch------\n",
      "train_loss: 246.18104887008667\n",
      "------第56个epoch------\n",
      "train_loss: 243.1522435347239\n",
      "------第57个epoch------\n",
      "train_loss: 243.37190175056458\n",
      "------第58个epoch------\n",
      "train_loss: 244.716322183609\n",
      "------第59个epoch------\n",
      "train_loss: 244.15599505106607\n",
      "------第60个epoch------\n",
      "train_loss: 245.91457653045654\n",
      "------第61个epoch------\n",
      "train_loss: 245.00685103734335\n",
      "------第62个epoch------\n",
      "train_loss: 243.2945741812388\n",
      "------第63个epoch------\n",
      "train_loss: 243.8298525015513\n",
      "------第64个epoch------\n",
      "train_loss: 244.34459336598715\n",
      "------第65个epoch------\n",
      "train_loss: 242.99759984016418\n",
      "------第66个epoch------\n",
      "train_loss: 245.410862604777\n",
      "------第67个epoch------\n",
      "train_loss: 243.5647655328115\n",
      "------第68个epoch------\n",
      "train_loss: 242.78160285949707\n",
      "------第69个epoch------\n",
      "train_loss: 246.59494614601135\n",
      "------第70个epoch------\n",
      "train_loss: 243.83833821614584\n",
      "------第71个epoch------\n",
      "train_loss: 245.65963768959045\n",
      "------第72个epoch------\n",
      "train_loss: 244.02587143580118\n",
      "------第73个epoch------\n",
      "train_loss: 242.31037171681723\n",
      "------第74个epoch------\n",
      "train_loss: 244.10156925519308\n",
      "------第75个epoch------\n",
      "train_loss: 242.22959979375204\n",
      "------第76个epoch------\n",
      "train_loss: 242.98863895734152\n",
      "------第77个epoch------\n",
      "train_loss: 241.61389581362405\n",
      "------第78个epoch------\n",
      "train_loss: 245.81864380836487\n",
      "------第79个epoch------\n",
      "train_loss: 243.83528979619345\n",
      "------第80个epoch------\n",
      "train_loss: 243.89944124221802\n",
      "HR@ 100  = 0.2070\n",
      "NDCG@ 100  = 0.0408\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "def train(train_matrix, test, neg_num, epoch, batch_size, fm_sizes, n_fm, lr, drop_out, n_user, n_item, topK):\n",
    "    model, loss_func, optimizer = createModel(fm_sizes, n_fm, lr, drop_out, n_user, n_item)\n",
    "    train_loss_list = list()\n",
    "    hr_list = [.0]\n",
    "    ndcg_list = [.0]\n",
    "    for e in range(epoch):\n",
    "        data = generate_train_batch(train_matrix, batch_size, neg_num)\n",
    "        train_loss = list()\n",
    "        for train_batch in data:\n",
    "            u = torch.from_numpy(train_batch[:,0].reshape(-1, 1)).type(torch.LongTensor)\n",
    "            i = torch.from_numpy(train_batch[:,1].reshape(-1, 1)).type(torch.LongTensor)\n",
    "            j = torch.from_numpy(train_batch[:,2].reshape(-1, 1)).type(torch.LongTensor)\n",
    "            if (torch.cuda.is_available()):\n",
    "                u, i, j = u.cuda(), i.cuda(), j.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            yui, yuj = model(u, i, j)\n",
    "            loss = loss_func(yui, yuj) \n",
    "            loss.backward()  \n",
    "            train_loss.append(loss.cpu().item())\n",
    "            optimizer.step()\n",
    "        print('------第'+str(e+1)+'个epoch------')\n",
    "        mean_train_loss = np.mean(train_loss)\n",
    "        print('train_loss:', mean_train_loss)\n",
    "        train_loss_list.append(mean_train_loss) \n",
    "    '''\n",
    "        if (e+1)%5==0:\n",
    "            hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user, n_item, topK)\n",
    "            hr_list.append(hr)\n",
    "            ndcg_list.append(ndcg)\n",
    "    np.savetxt(\"./evalres/convncf/train_loss_list_\"+str(epoch)+\"epoch.txt\", train_loss_list)    \n",
    "    np.savetxt(\"./evalres/convncf/hr_list_\"+str(epoch)+\"epoch.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/convncf/ndcg_list_\"+str(epoch)+\"epoch.txt\", ndcg_list) \n",
    "    '''\n",
    "    movieEval_1(model, loss_func, test, train_matrix, n_user, n_item, topK)\n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return model\n",
    "\n",
    "FM_SIZE = [64,32,16,8,4,2,1]\n",
    "N_FM = 32\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "DROP_OUT = 0.5\n",
    "NEG_NUM = 4\n",
    "EPOCH = 200\n",
    "TOPK = 100\n",
    "#train(train_matrix, test, neg_num=NEG_NUM, epoch=EPOCH, batch_size=BATCH_SIZE, fm_sizes=FM_SIZE, n_fm=N_FM, lr=LEARNING_RATE, drop_out=DROP_OUT, n_user=N_USER, n_item=N_ITEM, topK=TOPK)\n",
    "model = train(train_matrix, test, neg_num=NEG_NUM, epoch=80, batch_size=BATCH_SIZE, fm_sizes=FM_SIZE, n_fm=N_FM, lr=LEARNING_RATE, drop_out=DROP_OUT, n_user=N_USER, n_item=N_ITEM, topK=TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./evalres/model/ConvNCF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 8)\n",
      "  (item_embedding_layer): Embedding(3952, 8)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.2243\n",
      "NDCG@ 100  = 0.0468\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 16)\n",
      "  (item_embedding_layer): Embedding(3952, 16)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.2162\n",
      "NDCG@ 100  = 0.0481\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 32)\n",
      "  (item_embedding_layer): Embedding(3952, 32)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.1300\n",
      "NDCG@ 100  = 0.0280\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.2429\n",
      "NDCG@ 100  = 0.0543\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "def train_eval_d(train_matrix, test, neg_num, epoch, batch_size, fm_sizes, n_fm, lr, drop_out, n_user, n_item, topK):\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    for d in fm_sizes:\n",
    "        model, loss_func, optimizer = createModel(d, n_fm, lr, drop_out, n_user, n_item)\n",
    "        model.train()\n",
    "        for e in range(epoch):\n",
    "            data = generate_train_batch(train_matrix, batch_size, neg_num)\n",
    "            train_loss = list()\n",
    "            for train_batch in data:\n",
    "                u = torch.from_numpy(train_batch[:,0].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                i = torch.from_numpy(train_batch[:,1].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                j = torch.from_numpy(train_batch[:,2].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                if (torch.cuda.is_available()):\n",
    "                    u, i, j = u.cuda(), i.cuda(), j.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                yui, yuj = model(u, i, j)\n",
    "                loss = loss_func(yui, yuj) \n",
    "                loss.backward() \n",
    "                optimizer.step()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user, n_item, topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)   \n",
    "    np.savetxt(\"./evalres/convncf/hr_list_d.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/convncf/ndcg_list_d.txt\", ndcg_list) \n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "FM_SIZE = [[8,4,2,1],[16,8,4,2,1],[32,16,8,4,2,1],[64,32,16,8,4,2,1]]\n",
    "N_FM = 32\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "DROP_OUT = 0.5\n",
    "NEG_NUM = 4\n",
    "EPOCH = 60\n",
    "TOPK = 100\n",
    "train_eval_d(train_matrix, test, neg_num=NEG_NUM, epoch=EPOCH, batch_size=BATCH_SIZE, fm_sizes=FM_SIZE, n_fm=N_FM, \n",
    "             lr=LEARNING_RATE, drop_out=DROP_OUT, n_user=N_USER, n_item=N_ITEM, topK=TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 50  = 0.0646\n",
      "NDCG@ 50  = 0.0149\n",
      "HR@ 100  = 0.1440\n",
      "NDCG@ 100  = 0.0278\n",
      "HR@ 200  = 0.2373\n",
      "NDCG@ 200  = 0.0407\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "def train_eval_topK(train_matrix, test, neg_num, epoch, batch_size, fm_sizes, n_fm, lr, drop_out, n_user, n_item, topK):\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    model, loss_func, optimizer = createModel(fm_sizes, n_fm, lr, drop_out, n_user, n_item)\n",
    "    model.train()\n",
    "    for e in range(epoch):\n",
    "        data = generate_train_batch(train_matrix, batch_size, neg_num)\n",
    "        train_loss = list()\n",
    "        for train_batch in data:\n",
    "            u = torch.from_numpy(train_batch[:,0].reshape(-1, 1)).type(torch.LongTensor)\n",
    "            i = torch.from_numpy(train_batch[:,1].reshape(-1, 1)).type(torch.LongTensor)\n",
    "            j = torch.from_numpy(train_batch[:,2].reshape(-1, 1)).type(torch.LongTensor)\n",
    "            if (torch.cuda.is_available()):\n",
    "                u, i, j = u.cuda(), i.cuda(), j.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            yui, yuj = model(u, i, j)\n",
    "            loss = loss_func(yui, yuj) \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "    for k in topK:\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user, n_item, k)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)   \n",
    "    np.savetxt(\"./evalres/convncf/hr_list_topk.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/convncf/ndcg_list_topk.txt\", ndcg_list) \n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "FM_SIZE = [64,32,16,8,4,2,1]\n",
    "N_FM = 32\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "DROP_OUT = 0.5\n",
    "NEG_NUM = 4\n",
    "EPOCH = 60\n",
    "TOPK = [50,100,200]\n",
    "train_eval_topK(train_matrix, test, neg_num=NEG_NUM, epoch=EPOCH, batch_size=BATCH_SIZE, fm_sizes=FM_SIZE, n_fm=N_FM, \n",
    "             lr=LEARNING_RATE, drop_out=DROP_OUT, n_user=N_USER, n_item=N_ITEM, topK=TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.0000\n",
      "NDCG@ 100  = 0.0000\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.2164\n",
      "NDCG@ 100  = 0.0499\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.0954\n",
      "NDCG@ 100  = 0.0188\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.2084\n",
      "NDCG@ 100  = 0.0448\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.1772\n",
      "NDCG@ 100  = 0.0359\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.1333\n",
      "NDCG@ 100  = 0.0276\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.2281\n",
      "NDCG@ 100  = 0.0481\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.1904\n",
      "NDCG@ 100  = 0.0399\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.1647\n",
      "NDCG@ 100  = 0.0353\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.0937\n",
      "NDCG@ 100  = 0.0189\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "def train_eval_negNum(train_matrix, test, neg_num, epoch, batch_size, fm_sizes, n_fm, lr, drop_out, n_user, n_item, topK):\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    for n in neg_num:\n",
    "        model, loss_func, optimizer = createModel(fm_sizes, n_fm, lr, drop_out, n_user, n_item)\n",
    "        model.train()\n",
    "        for e in range(epoch):\n",
    "            data = generate_train_batch(train_matrix, batch_size, n)\n",
    "            train_loss = list()\n",
    "            for train_batch in data:\n",
    "                u = torch.from_numpy(train_batch[:,0].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                i = torch.from_numpy(train_batch[:,1].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                j = torch.from_numpy(train_batch[:,2].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                if (torch.cuda.is_available()):\n",
    "                    u, i, j = u.cuda(), i.cuda(), j.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                yui, yuj = model(u, i, j)\n",
    "                loss = loss_func(yui, yuj) \n",
    "                loss.backward() \n",
    "                optimizer.step()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user, n_item, topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)   \n",
    "    np.savetxt(\"./evalres/convncf/hr_list_neg.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/convncf/ndcg_list_neg.txt\", ndcg_list) \n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "FM_SIZE = [64,32,16,8,4,2,1]\n",
    "N_FM = 32\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "DROP_OUT = 0.5\n",
    "NEG_NUM = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "EPOCH = 60\n",
    "TOPK = 100\n",
    "train_eval_negNum(train_matrix, test, neg_num=NEG_NUM, epoch=EPOCH, batch_size=BATCH_SIZE, fm_sizes=FM_SIZE, n_fm=N_FM, \n",
    "             lr=LEARNING_RATE, drop_out=DROP_OUT, n_user=N_USER, n_item=N_ITEM, topK=TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.1364\n",
      "NDCG@ 100  = 0.0253\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "def train_eval_negNum(train_matrix, test, neg_num, epoch, batch_size, fm_sizes, n_fm, lr, drop_out, n_user, n_item, topK):\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    for n in neg_num:\n",
    "        model, loss_func, optimizer = createModel(fm_sizes, n_fm, lr, drop_out, n_user, n_item)\n",
    "        model.train()\n",
    "        for e in range(epoch):\n",
    "            data = generate_train_batch(train_matrix, batch_size, n)\n",
    "            train_loss = list()\n",
    "            for train_batch in data:\n",
    "                u = torch.from_numpy(train_batch[:,0].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                i = torch.from_numpy(train_batch[:,1].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                j = torch.from_numpy(train_batch[:,2].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                if (torch.cuda.is_available()):\n",
    "                    u, i, j = u.cuda(), i.cuda(), j.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                yui, yuj = model(u, i, j)\n",
    "                loss = loss_func(yui, yuj) \n",
    "                loss.backward() \n",
    "                optimizer.step()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user, n_item, topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)   \n",
    "    np.savetxt(\"./evalres/convncf/hr_list_neg_1.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/convncf/ndcg_list_neg_1.txt\", ndcg_list) \n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "FM_SIZE = [64,32,16,8,4,2,1]\n",
    "N_FM = 32\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "DROP_OUT = 0.5\n",
    "NEG_NUM = [1]\n",
    "EPOCH = 60\n",
    "TOPK = 100\n",
    "train_eval_negNum(train_matrix, test, neg_num=NEG_NUM, epoch=EPOCH, batch_size=BATCH_SIZE, fm_sizes=FM_SIZE, n_fm=N_FM, \n",
    "             lr=LEARNING_RATE, drop_out=DROP_OUT, n_user=N_USER, n_item=N_ITEM, topK=TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.1808\n",
      "NDCG@ 100  = 0.0435\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.1755\n",
      "NDCG@ 100  = 0.0380\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.1051\n",
      "NDCG@ 100  = 0.0217\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.7, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.2493\n",
      "NDCG@ 100  = 0.0530\n",
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.9, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.0000\n",
      "NDCG@ 100  = 0.0000\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "def train_eval_drop_out(train_matrix, test, neg_num, epoch, batch_size, fm_sizes, n_fm, lr, drop_out, n_user, n_item, topK):\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    for dout in drop_out:\n",
    "        model, loss_func, optimizer = createModel(fm_sizes, n_fm, lr, dout, n_user, n_item)\n",
    "        model.train()\n",
    "        for e in range(epoch):\n",
    "            data = generate_train_batch(train_matrix, batch_size, neg_num)\n",
    "            train_loss = list()\n",
    "            for train_batch in data:\n",
    "                u = torch.from_numpy(train_batch[:,0].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                i = torch.from_numpy(train_batch[:,1].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                j = torch.from_numpy(train_batch[:,2].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                if (torch.cuda.is_available()):\n",
    "                    u, i, j = u.cuda(), i.cuda(), j.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                yui, yuj = model(u, i, j)\n",
    "                loss = loss_func(yui, yuj) \n",
    "                loss.backward() \n",
    "                optimizer.step()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user, n_item, topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)   \n",
    "    np.savetxt(\"./evalres/convncf/hr_list_drop_out.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/convncf/ndcg_list_drop_out.txt\", ndcg_list) \n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "FM_SIZE = [64,32,16,8,4,2,1]\n",
    "N_FM = 32\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "DROP_OUT = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "NEG_NUM = 4\n",
    "EPOCH = 60\n",
    "TOPK = 100\n",
    "train_eval_drop_out(train_matrix, test, neg_num=NEG_NUM, epoch=EPOCH, batch_size=BATCH_SIZE, fm_sizes=FM_SIZE, n_fm=N_FM, \n",
    "             lr=LEARNING_RATE, drop_out=DROP_OUT, n_user=N_USER, n_item=N_ITEM, topK=TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.9, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "HR@ 100  = 0.2065\n",
      "NDCG@ 100  = 0.0459\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "def train_eval_drop_out(train_matrix, test, neg_num, epoch, batch_size, fm_sizes, n_fm, lr, drop_out, n_user, n_item, topK):\n",
    "    hr_list = list()\n",
    "    ndcg_list = list()\n",
    "    for dout in drop_out:\n",
    "        model, loss_func, optimizer = createModel(fm_sizes, n_fm, lr, dout, n_user, n_item)\n",
    "        model.train()\n",
    "        for e in range(epoch):\n",
    "            data = generate_train_batch(train_matrix, batch_size, neg_num)\n",
    "            train_loss = list()\n",
    "            for train_batch in data:\n",
    "                u = torch.from_numpy(train_batch[:,0].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                i = torch.from_numpy(train_batch[:,1].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                j = torch.from_numpy(train_batch[:,2].reshape(-1, 1)).type(torch.LongTensor)\n",
    "                if (torch.cuda.is_available()):\n",
    "                    u, i, j = u.cuda(), i.cuda(), j.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                yui, yuj = model(u, i, j)\n",
    "                loss = loss_func(yui, yuj) \n",
    "                loss.backward() \n",
    "                optimizer.step()\n",
    "        hr, ndcg, rank_all_users = movieEval_1(model, loss_func, test, train_matrix, n_user, n_item, topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)   \n",
    "    np.savetxt(\"./evalres/convncf/hr_list_drop_out_0.9.txt\", hr_list)\n",
    "    np.savetxt(\"./evalres/convncf/ndcg_list_drop_out_0.9.txt\", ndcg_list) \n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "FM_SIZE = [64,32,16,8,4,2,1]\n",
    "N_FM = 32\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "DROP_OUT = [0.9]\n",
    "NEG_NUM = 4\n",
    "EPOCH = 60\n",
    "TOPK = 100\n",
    "train_eval_drop_out(train_matrix, test, neg_num=NEG_NUM, epoch=EPOCH, batch_size=BATCH_SIZE, fm_sizes=FM_SIZE, n_fm=N_FM, \n",
    "             lr=LEARNING_RATE, drop_out=DROP_OUT, n_user=N_USER, n_item=N_ITEM, topK=TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNCF(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (user_embedding_layer): Embedding(6040, 64)\n",
      "  (item_embedding_layer): Embedding(3952, 64)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv4): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv5): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (conv6): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (predict): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "time cost: 7.253255367279053\n",
      "------Finished------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def train_eval_time(train_matrix, test, neg_num, epoch, batch_size, fm_sizes, n_fm, lr, drop_out, n_user, n_item, topK):\n",
    "    model, loss_func, optimizer = createModel(fm_sizes, n_fm, lr, drop_out, n_user, n_item)\n",
    "    model.train()\n",
    "    time_start, time_end = 0, 0\n",
    "    for e in range(1):\n",
    "        data = generate_train_batch(train_matrix, batch_size, neg_num)\n",
    "        time_start=time.time()\n",
    "        for train_batch in data:\n",
    "            u = torch.from_numpy(train_batch[:,0].reshape(-1, 1)).type(torch.LongTensor)\n",
    "            i = torch.from_numpy(train_batch[:,1].reshape(-1, 1)).type(torch.LongTensor)\n",
    "            j = torch.from_numpy(train_batch[:,2].reshape(-1, 1)).type(torch.LongTensor)\n",
    "            if (torch.cuda.is_available()):\n",
    "                u, i, j = u.cuda(), i.cuda(), j.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            yui, yuj = model(u, i, j)\n",
    "            loss = loss_func(yui, yuj) \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "        time_end=time.time()\n",
    "    print('time cost:', time_end-time_start)\n",
    "    np.savetxt(\"./evalres/convncf/single_time.txt\", [time_end-time_start]) \n",
    "    torch.cuda.empty_cache()\n",
    "    print('------Finished------')\n",
    "    return\n",
    "\n",
    "FM_SIZE = [64,32,16,8,4,2,1]\n",
    "N_FM = 32\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "DROP_OUT = 0.5\n",
    "NEG_NUM = 4\n",
    "EPOCH = 60\n",
    "TOPK = 100\n",
    "train_eval_time(train_matrix, test, neg_num=NEG_NUM, epoch=EPOCH, batch_size=BATCH_SIZE, fm_sizes=FM_SIZE, n_fm=N_FM, \n",
    "             lr=LEARNING_RATE, drop_out=DROP_OUT, n_user=N_USER, n_item=N_ITEM, topK=TOPK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
