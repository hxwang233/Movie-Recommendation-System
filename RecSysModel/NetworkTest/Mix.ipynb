{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "SEED = 2019\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('../ml-1m/ratings.dat', delimiter='::', usecols=[0,1,3], dtype=int)\n",
    "N_USER = np.max(dataset[:,0])\n",
    "N_ITEM = np.max(dataset[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_from_local(path, n_user, n_item):\n",
    "    data = np.loadtxt(fname=path, delimiter=\"\\t\", skiprows=1, dtype=int)\n",
    "    train_matrix = np.zeros((n_user, n_item), dtype = np.int8)\n",
    "    for line in data:\n",
    "        train_matrix[line[0],line[1]] = 1\n",
    "    user_pos = dict()\n",
    "    max_item_id = train_matrix.shape[1]\n",
    "    max_item_num = 0\n",
    "    for u, i in enumerate(train_matrix):\n",
    "        pos_item = list(np.nonzero(i)[0])\n",
    "        pos_item_num = len(pos_item)\n",
    "        if  pos_item_num > max_item_num:\n",
    "            max_item_num = pos_item_num\n",
    "        user_pos[u] = pos_item\n",
    "    train_user = list()\n",
    "    train_item = list()\n",
    "    for k in user_pos.keys():\n",
    "        while len(user_pos[k]) < max_item_num:\n",
    "            user_pos[k].append(max_item_id)\n",
    "        train_user.append(k)\n",
    "        train_item.append(user_pos[k])\n",
    "    return np.array(train_user), np.array(train_item), train_matrix\n",
    "\n",
    "def generate_test_from_local(path):\n",
    "    data = np.loadtxt(fname=path, delimiter=\"\\t\", skiprows=1, dtype=int)\n",
    "    return data\n",
    "\n",
    "train_user, train_item, train_matrix = generate_train_from_local(path=\"../ml-1m/ml.train.txt\",n_user=N_USER,n_item=N_ITEM)\n",
    "test = generate_test_from_local(path=\"../ml-1m/ml.test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    def rank(self, user):\\n        res = self.user_embs(user).unsqueeze(0)\\n        res = res * self.item_embs.weight\\n        res = res.matmul(self.h).squeeze(1)\\n        return res'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ENMF(nn.Module):\n",
    "    def __init__(self, emb_size, n_user, n_item, neg_weight, drop_out, count, c0=512, x=0.6):\n",
    "        super().__init__()\n",
    "        self.c0 = c0\n",
    "        self.x  = x\n",
    "        self.count = count\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.neg_weight = neg_weight\n",
    "        self.emb_size   = emb_size\n",
    "        self.user_embs = nn.Embedding(n_user, emb_size)\n",
    "        self.item_embs = nn.Embedding(n_item+1, emb_size)\n",
    "        self.h = nn.Parameter(torch.randn(emb_size, 1))\n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "        self.freq = self.calcu_freq()\n",
    "        self._reset_para()\n",
    "        return\n",
    "    \n",
    "    def _reset_para(self):\n",
    "        nn.init.xavier_normal_(self.user_embs.weight)\n",
    "        nn.init.xavier_normal_(self.item_embs.weight)\n",
    "        nn.init.constant_(self.h, 0.01)\n",
    "        return\n",
    "    \n",
    "    def calcu_freq(self):\n",
    "        freq_items = sorted(self.count.keys())\n",
    "        freq_count = [self.count[k] for k in freq_items]\n",
    "        freq = np.zeros(self.item_embs.weight.shape[0])\n",
    "        freq[freq_items] = freq_count       \n",
    "        #freq = freq/np.sum(freq)\n",
    "        freq = np.power(freq, self.x)\n",
    "        freq = self.c0 * freq/np.sum(freq)\n",
    "        freq = torch.from_numpy(freq).type(torch.float).cuda()\n",
    "        return freq\n",
    "    \n",
    "    def forward(self, uids, pos_iids):\n",
    "        '''\n",
    "        uids: B\n",
    "        u_iids: B * L\n",
    "        '''\n",
    "        u_emb = self.dropout(self.user_embs(uids))\n",
    "        pos_embs = self.item_embs(pos_iids)\n",
    "\n",
    "        # torch.einsum(\"ab,abc->abc\")\n",
    "        # B * L * D\n",
    "        mask = (~(pos_iids.eq(self.n_item))).float()\n",
    "        pos_embs = pos_embs * mask.unsqueeze(2)\n",
    "\n",
    "        # torch.einsum(\"ac,abc->abc\")\n",
    "        # B * L * D\n",
    "        pq = u_emb.unsqueeze(1) * pos_embs\n",
    "        # torch.einsum(\"ajk,kl->ajl\")\n",
    "        # B * L\n",
    "        hpq = pq.matmul(self.h).squeeze(2)\n",
    "\n",
    "        # loss\n",
    "        pos_data_loss = torch.sum((1 - self.neg_weight) * hpq.square() - 2.0 * hpq)\n",
    "\n",
    "        # torch.einsum(\"ab,ac->abc\")\n",
    "        part_1 = self.item_embs.weight.unsqueeze(2).bmm(self.item_embs.weight.unsqueeze(1))\n",
    "        part_2 = u_emb.unsqueeze(2).bmm(u_emb.unsqueeze(1))\n",
    "\n",
    "        # D * D\n",
    "        part_1 = part_1.sum(0)\n",
    "        part_2 = part_2.sum(0)\n",
    "        part_3 = self.h.mm(self.h.t())\n",
    "        all_data_loss = torch.sum(part_1 * part_2 * part_3)\n",
    "\n",
    "        loss = self.neg_weight * all_data_loss + pos_data_loss\n",
    "        return loss\n",
    "    \n",
    "    def rank(self, uid):\n",
    "        '''\n",
    "        uid: Batch_size\n",
    "        '''\n",
    "        uid_embs = self.user_embs(uid)\n",
    "        user_all_items = uid_embs.unsqueeze(1) * self.item_embs.weight\n",
    "        items_score = user_all_items.matmul(self.h).squeeze(2)\n",
    "        return items_score\n",
    "    \n",
    "'''    def rank(self, user):\n",
    "        res = self.user_embs(user).unsqueeze(0)\n",
    "        res = res * self.item_embs.weight\n",
    "        res = res.matmul(self.h).squeeze(1)\n",
    "        return res'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, gmf_n_factors, layers,  n_user, n_item, activation = torch.relu, batch_normalization = False, n_output = 1):\n",
    "        super(NCF, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.do_bn = batch_normalization\n",
    "        self.fcs = []\n",
    "        self.bns = []\n",
    "        self.n_layer  = len(layers)\n",
    "        parameter_LeCun = np.sqrt(gmf_n_factors + layers[-1])\n",
    "\n",
    "        #self.bn_userInput = nn.BatchNorm1d(1)   # for input data\n",
    "        #self.bn_itemInput = nn.BatchNorm1d(1)   # for input data\n",
    "        \n",
    "        self.mlp_user_embedding_layer = nn.Embedding(n_user, int(layers[0]/2))\n",
    "        self._set_normalInit(self.mlp_user_embedding_layer, hasBias = False) \n",
    "        self.mlp_item_embedding_layer = nn.Embedding(n_item, int(layers[0]/2))\n",
    "        self._set_normalInit(self.mlp_item_embedding_layer, hasBias = False) \n",
    "        \n",
    "        self.gmf_user_embedding_layer = nn.Embedding(n_user, gmf_n_factors)\n",
    "        self._set_normalInit(self.gmf_user_embedding_layer, hasBias = False) \n",
    "        self.gmf_item_embedding_layer = nn.Embedding(n_item, gmf_n_factors)\n",
    "        self._set_normalInit(self.gmf_item_embedding_layer, hasBias = False) \n",
    "        \n",
    "        for i in range(1, self.n_layer):               # build hidden layers and BN layers\n",
    "            fc = nn.Linear(layers[i-1], layers[i])\n",
    "            self._set_normalInit(fc)                  # parameters initialization\n",
    "            setattr(self, 'fc%i' % i, fc)       # IMPORTANT set layer to the Module\n",
    "            self.fcs.append(fc)\n",
    "            if self.do_bn:\n",
    "                bn = nn.BatchNorm1d(layers[i])\n",
    "                setattr(self, 'bn%i' % i, bn)   # IMPORTANT set layer to the Module\n",
    "                self.bns.append(bn)\n",
    "\n",
    "        self.predict = nn.Linear(gmf_n_factors + layers[-1], n_output)         # output layer\n",
    "        self._set_uniformInit(self.predict, parameter = parameter_LeCun)            # parameters initialization\n",
    "        return\n",
    "\n",
    "    def _set_normalInit(self, layer, parameter = [0.0, 0.01], hasBias=True):\n",
    "        init.normal_(layer.weight, mean = parameter[0], std = parameter[1])\n",
    "        if hasBias:\n",
    "            init.normal_(layer.bias, mean = parameter[0], std = parameter[1])\n",
    "        return\n",
    "    \n",
    "    def _set_uniformInit(self, layer, parameter = 5, hasBias = True):\n",
    "        init.uniform_(layer.weight, a = - parameter, b = parameter)\n",
    "        if hasBias:\n",
    "            init.uniform_(layer.bias, a = - parameter, b = parameter)\n",
    "        return\n",
    "    \n",
    "    def _set_heNormalInit(self, layer, hasBias=True):\n",
    "        init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.kaiming_normal_(layer.bias, nonlinearity='relu')\n",
    "        return\n",
    "    \n",
    "    def _set_heUniformInit(self, layer, hasBias=True):\n",
    "        init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.kaiming_uniform_(layer.bias, nonlinearity='relu')\n",
    "        return\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        #if self.do_bn: \n",
    "            #x1 = self.bn_userInput(x1)     # input batch normalization\n",
    "            #x2 = self.bn_itemInput(x2)\n",
    "        mlp_x1 = self.mlp_user_embedding_layer(x1)\n",
    "        mlp_x2 = self.mlp_item_embedding_layer(x2)\n",
    "        \n",
    "        gmf_x1 = self.gmf_user_embedding_layer(x1)\n",
    "        gmf_x2 = self.gmf_item_embedding_layer(x2)\n",
    "        \n",
    "        mlp_x3 = torch.cat((mlp_x1, mlp_x2), dim=1)\n",
    "        mlp_x  = torch.flatten(mlp_x3, start_dim=1)        \n",
    "        for i in range(1, self.n_layer):\n",
    "            mlp_x = self.fcs[i-1](mlp_x)\n",
    "            if self.do_bn: \n",
    "                mlp_x = self.bns[i-1](mlp_x)   # batch normalization\n",
    "            mlp_x = self.activation(mlp_x)\n",
    "        \n",
    "        gmf_x3 = torch.mul(gmf_x1, gmf_x2)\n",
    "        gmf_x  = torch.flatten(gmf_x3, start_dim=1)\n",
    "\n",
    "        x = torch.cat((mlp_x, gmf_x), dim=1)\n",
    "        out = torch.sigmoid(self.predict(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNCF(nn.Module):\n",
    "    def __init__(self, fm_sizes, n_user, n_item, n_fm, drop_out, myStride=2, n_output=1):\n",
    "        ''' e.g.--> fm_sizes = [64,32,16,8,4,2,1] '''\n",
    "        super(ConvNCF, self).__init__()\n",
    "        self.convs = list()\n",
    "        self.dropout = nn.Dropout(p=drop_out)\n",
    "        self.user_embedding_layer = nn.Embedding(n_user, fm_sizes[0])\n",
    "        self._set_normalInit(self.user_embedding_layer, hasBias = False) \n",
    "        #self._set_xavierInit(self.user_embedding_layer, hasBias = False)\n",
    "        #self._set_heInit(self.user_embedding_layer, hasBias = False) \n",
    "        self.item_embedding_layer = nn.Embedding(n_item, fm_sizes[0])\n",
    "        self._set_normalInit(self.item_embedding_layer, hasBias = False) \n",
    "        #self._set_xavierInit(self.item_embedding_layer, hasBias = False)\n",
    "        #self._set_heInit(self.item_embedding_layer, hasBias = False) \n",
    "        for i in range(1, len(fm_sizes)):\n",
    "            inChannel = 1 if i == 1 else n_fm\n",
    "            #conv = nn.Conv2d(in_channels=inChannel, out_channels=32, kernel_size=fm_sizes[i]+myStride, stride=myStride)\n",
    "            conv = nn.Conv2d(in_channels=inChannel, out_channels=n_fm, kernel_size=4, stride=myStride, padding=1)\n",
    "            #self._set_normalInit(conv)\n",
    "            #self._set_xavierInit(conv)\n",
    "            self._set_heInit(conv)\n",
    "            setattr(self, 'conv%i' % i, conv)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.predict = nn.Linear(n_fm, n_output)         # output layer\n",
    "        self._set_xavierInit(self.predict)            # parameters initialization\n",
    "        return\n",
    "    \n",
    "    def _set_xavierInit(self, layer, hasBias = True):\n",
    "        init.xavier_uniform_(layer.weight)\n",
    "        if hasBias:\n",
    "            init.constant_(layer.bias, 0.01)\n",
    "        return\n",
    "    \n",
    "    def _set_heInit(self, layer, hasBias = True):\n",
    "        init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.constant_(layer.bias, 0.01)\n",
    "        return\n",
    "    \n",
    "    def _set_normalInit(self, layer, parameter = [0.0, 0.1], hasBias = True):\n",
    "        init.normal_(layer.weight, mean = parameter[0], std = parameter[1])\n",
    "        if hasBias:\n",
    "            init.constant_(layer.bias, 0.01)\n",
    "        return\n",
    "    \n",
    "    def _set_uniformInit(self, layer, parameter = 1, hasBias = True):\n",
    "        init.uniform_(layer.weight, a = 0, b = parameter)\n",
    "        if hasBias:\n",
    "            init.uniform_(layer.bias, a = 0, b = parameter)\n",
    "        return\n",
    "    \n",
    "    def forward(self, user, item_pos, item_neg, train = True):\n",
    "        user = self.user_embedding_layer(user)\n",
    "        item_pos = self.item_embedding_layer(item_pos)\n",
    "        if train:\n",
    "            item_neg = self.item_embedding_layer(item_neg)\n",
    "        x1, x2 = None, None\n",
    "        temp1, temp2 = list(), list() \n",
    "        out1, out2 = None, None\n",
    "        for i in range(user.size()[0]):\n",
    "            temp1.append(torch.mm(user[i].T, item_pos[i]))\n",
    "            if train:\n",
    "                temp2.append(torch.mm(user[i].T, item_neg[i]))\n",
    "        x1 = torch.stack(temp1)\n",
    "        x1 = x1.view(x1.size()[0], -1, x1.size()[1], x1.size()[2])\n",
    "        if train:\n",
    "            x2 = torch.stack(temp2)\n",
    "            x2 = x2.view(x2.size()[0], -1, x2.size()[1], x2.size()[2])\n",
    "        ''' ## conv2d -input  (batch_size, channel, weight, height) '''\n",
    "        for conv in self.convs:\n",
    "            x1 = torch.relu(conv(x1))\n",
    "            if train:\n",
    "                x2 = torch.relu(conv(x2))\n",
    "        ''' ## conv2d -output (batch_size, out_channel, out_weight, out_height) '''\n",
    "        x1 = torch.flatten(x1, start_dim = 1)\n",
    "        x1 = self.dropout(x1)\n",
    "        if train:\n",
    "            x2 = torch.flatten(x2, start_dim = 1)\n",
    "            x2 = self.dropout(x2)\n",
    "        #out1 = torch.sigmoid(self.dropout(self.predict(x1)))\n",
    "        out1 = torch.sigmoid(self.predict(x1))\n",
    "        if train:\n",
    "            #out2 = torch.sigmoid(self.dropout(self.predict(x2)))\n",
    "            out2 = torch.sigmoid(self.predict(x2))\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    #HR击中率，如果topk中有正例ID即认为正确\n",
    "    if gtItem in ranklist:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    #NDCG归一化折损累计增益\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return np.log(2) / np.log(i+2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "enmf = torch.load(\"./evalres/model/ENMF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_test = torch.from_numpy(test).type(torch.LongTensor)\n",
    "torch_testset = data_utils.TensorDataset(torch_test[:,0],torch_test[:,1])\n",
    "test_loader = data_utils.DataLoader(dataset = torch_testset, batch_size = 128, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enmf_rough(model, train_matrix, test_loader, topK = 100):\n",
    "    n_users = train_matrix.shape[0]\n",
    "    rank_all_users = list()\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for step, (batch_x, batch_y) in enumerate(test_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                batch_x = batch_x.cuda()  \n",
    "            prediction = model.rank(batch_x)\n",
    "            pred_vector = -1 * (prediction.cpu().data.numpy())\n",
    "            ranklist = np.argsort(pred_vector)\n",
    "            for j, r in enumerate(ranklist):\n",
    "                real_r = list()\n",
    "                u = batch_x[j].cpu().data.numpy()\n",
    "                i = 0\n",
    "                while len(real_r) < topK:\n",
    "                    if r[i]==train_matrix.shape[1]:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    if train_matrix[u][r[i]] == 0:\n",
    "                        real_r.append(r[i])\n",
    "                    i += 1     \n",
    "                rank_all_users.append(real_r)\n",
    "    model.train()\n",
    "    return np.array(rank_all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_rank = enmf_rough(enmf, train_matrix, torch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 100)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rough_rank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convNCF_fine(model, train_matrix, test, rough_rank, topK = 100):\n",
    "    n_user = rough_rank.shape[0]\n",
    "    n_item = rough_rank.shape[1]\n",
    "    model.eval()\n",
    "    rank_all_users = list()\n",
    "    hit_list = list()\n",
    "    undcg_list = list()\n",
    "    with torch.no_grad():\n",
    "        for u, rank in enumerate(rough_rank):\n",
    "            item_list = torch.from_numpy(rank.reshape(-1, 1)).type(torch.LongTensor)\n",
    "            user_list = torch.from_numpy(np.array([u for i in range(n_item)]).reshape(-1, 1)).type(torch.LongTensor)\n",
    "            if torch.cuda.is_available():\n",
    "                user_list, item_list = user_list.cuda(), item_list.cuda()\n",
    "            prediction, _ = model(user_list, item_list, None, train = False)\n",
    "            pred_vector = -1 * (prediction.cpu().data.numpy().reshape(-1))\n",
    "            ranklist = rank[np.argsort(pred_vector)[:topK]]\n",
    "            pos_item = test[u][1]\n",
    "            rank_all_users.append(ranklist)\n",
    "            hit_list.append(getHitRatio(ranklist, pos_item))\n",
    "            undcg_list.append(getNDCG(ranklist, pos_item))\n",
    "    model.train()\n",
    "    hr = np.mean(hit_list)\n",
    "    ndcg = np.mean(undcg_list)\n",
    "    print('HR@', topK, ' = %.4f' %  hr)\n",
    "    print('NDCG@', topK, ' = %.4f' % ndcg)\n",
    "    return hr, ndcg, np.array(rank_all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@ 100  = 0.4525\n",
      "NDCG@ 100  = 0.0935\n"
     ]
    }
   ],
   "source": [
    "convNCF = torch.load(\"./evalres/model/ConvNCF.pkl\")\n",
    "hr, ndcg, rank_all_users = convNCF_fine(convNCF, train_matrix, test, rough_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncf_fine(model, train_matrix, test, rough_rank, topK = 50):\n",
    "    n_user = rough_rank.shape[0]\n",
    "    n_item = rough_rank.shape[1]\n",
    "    model.eval()\n",
    "    rank_all_users = list()\n",
    "    hit_list = list()\n",
    "    undcg_list = list()\n",
    "    with torch.no_grad():\n",
    "        for u, rank in enumerate(rough_rank):\n",
    "            item_list = torch.from_numpy(rank.reshape(-1, 1)).type(torch.LongTensor)\n",
    "            user_list = torch.from_numpy(np.array([u for i in range(n_item)]).reshape(-1, 1)).type(torch.LongTensor)\n",
    "            if torch.cuda.is_available():\n",
    "                user_list, item_list = user_list.cuda(), item_list.cuda()\n",
    "            prediction = model(user_list, item_list)\n",
    "            pred_vector = -1 * (prediction.cpu().data.numpy().reshape(-1))\n",
    "            ranklist = rank[np.argsort(pred_vector)[:topK]]\n",
    "            pos_item = test[u][1]\n",
    "            rank_all_users.append(ranklist)\n",
    "            hit_list.append(getHitRatio(ranklist, pos_item))\n",
    "            undcg_list.append(getNDCG(ranklist, pos_item))\n",
    "    model.train()\n",
    "    hr = np.mean(hit_list)\n",
    "    ndcg = np.mean(undcg_list)\n",
    "    print('HR@', topK, ' = %.4f' %  hr)\n",
    "    print('NDCG@', topK, ' = %.4f' % ndcg)\n",
    "    return hr, ndcg, np.array(rank_all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR@ 50  = 0.3012\n",
      "NDCG@ 50  = 0.0885\n"
     ]
    }
   ],
   "source": [
    "ncf = torch.load(\"./evalres/model/NCF.pkl\")\n",
    "hr, ndcg, rank_all_users = ncf_fine(ncf, train_matrix, test, rough_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-142-f5ae4001f4a4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-142-f5ae4001f4a4>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    HR@ 100  = 0.4525\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "HR@ 100  = 0.4525\n",
    "NDCG@ 100  = 0.1103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HR@ 50  = 0.2753\n",
    "NDCG@ 50  = 0.0817"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
