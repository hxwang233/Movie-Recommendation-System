{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "dataset = np.loadtxt(\"./ml-1m/ratings.dat\",delimiter='::',dtype=int)[:,[0,1,3]]\n",
    "N_USERS = np.max(dataset[:,0])\n",
    "N_ITEMS = np.max(dataset[:,1])\n",
    "n_negatives = 4  ## 1正例对应n个负例 ##\n",
    "users_items = np.zeros((N_USERS+1, N_ITEMS+1), dtype = np.int8)  # 混淆矩阵\n",
    "user_input, item_input, labels = [],[],[]  # x1 x2 -> y\n",
    "for u in range(dataset.shape[0]):   # 评分数据集隐式化\n",
    "    users_items[dataset[u][0], dataset[u][1]] = 1\n",
    "uipositives = list() # 作为测试集的交互正例\n",
    "for i in range(N_USERS+1):\n",
    "    if i==0: \n",
    "        continue\n",
    "    uitems = dataset[dataset[:,0]==i]\n",
    "    onepos = uitems[uitems[:,-1]==np.max(uitems),:2][0]\n",
    "    uipositives.append(onepos)\n",
    "    users_items[onepos[0], onepos[1]]=0\n",
    "for uno, uitems in enumerate(users_items):\n",
    "    if uno == 0:\n",
    "        continue\n",
    "    positives = np.nonzero(uitems)[0]\n",
    "    n_sample = len(positives) * n_negatives\n",
    "    negative_items = list(set(range(N_ITEMS+1))^set(positives))\n",
    "    negatives = np.random.choice(negative_items, n_sample)  # 负采样 -- 不放回\n",
    "    for i in range(len(positives)): # 正实例\n",
    "        user_input.append(uno)\n",
    "        item_input.append(positives[i])\n",
    "        labels.append(1)\n",
    "    for j in range(n_sample): # 负实例\n",
    "        user_input.append(uno)\n",
    "        item_input.append(negatives[j])\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "utest = list()\n",
    "itest = list()\n",
    "for ui in uipositives:\n",
    "    u = ui[0]\n",
    "    i = ui[1]\n",
    "    positives = np.nonzero(users_items[u])[0]\n",
    "    negative_items = list(set(range(1,N_ITEMS+1))^set(positives))\n",
    "    negatives_sample = np.random.choice(negative_items, 99)  # 负采样 -- 不放回\n",
    "    negatives = [i]  # 正例\n",
    "    for n in negatives_sample:\n",
    "        negatives.append(n)  # 添加负例\n",
    "    utest.append([u for j in range(100)])\n",
    "    itest.append(negatives)\n",
    "ytest = np.zeros((N_USERS,100))\n",
    "ytest[:, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 12\n",
    "USER_VECTOR_SIZE = 1    # len(one-hot of user vecter) \n",
    "ITEM_VECTOR_SIZE = 1    # len(one-hot of item vecter) \n",
    "N_FACTORS  = 8          # 隐层size  \n",
    "ACTIVATION = torch.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_x1 = torch.from_numpy(np.array(user_input, ndmin=2).T).type(torch.LongTensor)\n",
    "torch_x2 = torch.from_numpy(np.array(item_input, ndmin=2).T).type(torch.LongTensor)\n",
    "torch_y  = torch.from_numpy(np.array(labels, ndmin=2).T).type(torch.FloatTensor)\n",
    "torch_dataset = data_utils.TensorDataset(torch_x1, torch_x2, torch_y)\n",
    "loader = data_utils.DataLoader(dataset = torch_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, user_vector_size, item_vector_size, n_factors,  \n",
    "                 n_users, n_items, activation = torch.relu, batch_normalization = False, n_output = 1):\n",
    "        super(GMF, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.do_bn = batch_normalization\n",
    "        parameter_LeCun = np.sqrt(n_factors)\n",
    "        \n",
    "        #self.bn_userInput = nn.BatchNorm1d(1)   # for input data\n",
    "        #self.bn_itemInput = nn.BatchNorm1d(1)   # for input data\n",
    "        \n",
    "        self.gmf_user_embedding_layer = nn.Embedding(n_users+1, n_factors)\n",
    "        self._set_normalInit(self.gmf_user_embedding_layer, hasBias = False) \n",
    "        self.gmf_item_embedding_layer = nn.Embedding(n_items+1, n_factors)\n",
    "        self._set_normalInit(self.gmf_item_embedding_layer, hasBias = False) \n",
    "        \n",
    "        #self.bn_user_elayer = nn.BatchNorm1d(mlp_embedding_size) \n",
    "        #self.bn_item_elayer = nn.BatchNorm1d(mlp_embedding_size)     \n",
    "\n",
    "        self.predict = nn.Linear(n_factors, n_output)         # output layer\n",
    "        self._set_uniformInit(self.predict, parameter = parameter_LeCun)            # parameters initialization\n",
    "        return\n",
    "\n",
    "    def _set_normalInit(self, layer, parameter = [0.0, 0.01], hasBias=True):\n",
    "        init.normal_(layer.weight, mean = parameter[0], std = parameter[1])\n",
    "        if hasBias:\n",
    "            init.normal_(layer.bias, mean = parameter[0], std = parameter[1])\n",
    "        return\n",
    "    \n",
    "    def _set_uniformInit(self, layer, parameter = 5, hasBias = True):\n",
    "        init.uniform_(layer.weight, a = - parameter, b = parameter)\n",
    "        if hasBias:\n",
    "            init.uniform_(layer.bias, a = - parameter, b = parameter)\n",
    "        return\n",
    "    \n",
    "    def _set_heNormalInit(self, layer, hasBias=True):\n",
    "        init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.kaiming_normal_(layer.bias, nonlinearity='relu')\n",
    "        return\n",
    "    \n",
    "    def _set_heUniformInit(self, layer, hasBias=True):\n",
    "        init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.kaiming_uniform_(layer.bias, nonlinearity='relu')\n",
    "        return\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        #if self.do_bn: \n",
    "            #x1 = self.bn_userInput(x1)     # input batch normalization\n",
    "            #x2 = self.bn_itemInput(x2)\n",
    "        x1 = self.gmf_user_embedding_layer(x1)\n",
    "        x2 = self.gmf_item_embedding_layer(x2)\n",
    "        x3 = torch.mul(x1, x2)\n",
    "        #print(x3.data.numpy().shape)\n",
    "        x  = torch.flatten(x3, start_dim=1)\n",
    "        #print(x.data.numpy().shape)\n",
    "        out = torch.sigmoid(self.predict(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf = GMF(user_vector_size = USER_VECTOR_SIZE, item_vector_size = ITEM_VECTOR_SIZE,\n",
    "          n_factors = N_FACTORS, n_users = N_USERS, n_items = N_ITEMS, activation = ACTIVATION, batch_normalization = False, n_output = 1)\n",
    "optimizer = torch.optim.Adam(gmf.parameters(), lr = LEARNING_RATE)\n",
    "loss_func = torch.nn.BCELoss()\n",
    "print(gmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieEval(model, loss_func, utest, itest, ytest, topK = 10):\n",
    "    if len(utest)==len(itest)==len(ytest):\n",
    "        n_users = len(utest)\n",
    "    else:\n",
    "        print('the length of test sets are not equal.')\n",
    "        return\n",
    "    hit = 0\n",
    "    undcg = 0\n",
    "    test_loss = list()\n",
    "    for i in range(n_users):\n",
    "        map_item_score = dict()\n",
    "        x1test = Variable(torch.from_numpy(np.array(utest[i], ndmin=2).T).type(torch.LongTensor))\n",
    "        x2test = Variable(torch.from_numpy(np.array(itest[i], ndmin=2).T).type(torch.LongTensor))\n",
    "        y  = Variable(torch.from_numpy(np.array(ytest[i], ndmin=2).T).type(torch.FloatTensor))\n",
    "        prediction = model(x1test, x2test)\n",
    "        loss = loss_func(prediction, y)\n",
    "        test_loss.append(loss.item())\n",
    "        pred_vector = prediction.data.numpy().T[0]\n",
    "        positive_item = itest[i][0]  # 取正例\n",
    "        for j in range(len(itest[i])):\n",
    "            map_item_score[itest[i][j]] = pred_vector[j]\n",
    "        ranklist = heapq.nlargest(topK, map_item_score, key=map_item_score.get)\n",
    "        hit += getHitRatio(ranklist, positive_item)\n",
    "        undcg += getNDCG(ranklist, positive_item)\n",
    "    mean_test_loss = np.mean(test_loss)\n",
    "    hr = hit / n_users\n",
    "    ndcg = undcg / n_users\n",
    "    print('test_loss:', mean_test_loss)\n",
    "    print('HR@', topK, ' = %.4f' % hr)\n",
    "    print('NDCG@', topK, ' = %.4f' % ndcg)\n",
    "    return mean_test_loss, hr, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = list()\n",
    "test_loss_list  = list()\n",
    "hr_list = list()\n",
    "ndcg_list = list()\n",
    "for e in range(EPOCH):\n",
    "    train_loss = list()\n",
    "    for step, (batch_x1, batch_x2, batch_y) in enumerate(loader):\n",
    "        x1, x2, y = Variable(batch_x1), Variable(batch_x2), Variable(batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        prediction = gmf(x1, x2)\n",
    "        loss = loss_func(prediction, y)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('------第'+str(e+1)+'个epoch------')\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    print('train_loss:', mean_train_loss)\n",
    "    train_loss_list.append(mean_train_loss)    \n",
    "    test_loss, hr, ndcg = movieEval(gmf, loss_func, utest, itest, ytest)\n",
    "    test_loss_list.append(test_loss)\n",
    "    hr_list.append(hr)\n",
    "    ndcg_list.append(ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
