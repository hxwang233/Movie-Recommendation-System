{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Avail\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"./ml-1m/ratings.dat\",delimiter='::',dtype=int)[:,[0,1,3]]\n",
    "N_USERS = np.max(dataset[:,0])\n",
    "N_ITEMS = np.max(dataset[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def veiwData(dataset):\n",
    "    n_users  = np.max(dataset[:,0])\n",
    "    n_items  = np.max(dataset[:,1])\n",
    "    avgS     = round(len(dataset) / n_items, 0)\n",
    "    itemFreq = [0 for x in range(n_items)]\n",
    "    for record in dataset:\n",
    "        itemFreq[record[1]-1] += 1\n",
    "    realHH = set()\n",
    "    for i,n in enumerate(itemFreq):\n",
    "        if n >= avgS:\n",
    "            realHH.add(i+1)\n",
    "    itemFreq.sort(reverse=True)\n",
    "    plt.plot(range(len(itemFreq)), itemFreq)\n",
    "    print(\"number of items: \", n_items)\n",
    "    print(\"number of flows: \", len(dataset))\n",
    "    print(\"avg of S(x): \", avgS)\n",
    "    print(\"parameter phi: \", round(1 / n_items, 5))\n",
    "    print(\"parameter epsilon should less than or equal phi\")\n",
    "    print(\"sketch belongs to half of the stream\")\n",
    "    return realHH\n",
    "\n",
    "realHH = veiwData(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_table(\"./ml-1m/users.dat\", sep = '::', header = None, engine = 'python').iloc[:,0].values\n",
    "movies = pd.read_table(\"./ml-1m/movies.dat\", sep = '::', header = None, engine = 'python').iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "n_negatives = 4  ## 1正例对应n个负例 ##\n",
    "users_items = np.zeros((N_USERS+1, N_ITEMS+1), dtype = np.int8)  # 混淆矩阵\n",
    "user_input, item_input, labels = [],[],[]  # x1 x2 -> y\n",
    "for u in range(dataset.shape[0]):   # 评分数据集隐式化\n",
    "    users_items[dataset[u][0], dataset[u][1]] = 1\n",
    "uipositives = list() # 作为测试集的交互正例\n",
    "for i in range(N_USERS+1):\n",
    "    if i==0: \n",
    "        continue\n",
    "    uitems = dataset[dataset[:,0]==i]\n",
    "    onepos = uitems[uitems[:,-1]==np.max(uitems),:2][0]\n",
    "    uipositives.append(onepos)\n",
    "    users_items[onepos[0], onepos[1]]=0\n",
    "for uno, uitems in enumerate(users_items):\n",
    "    if uno == 0:\n",
    "        continue\n",
    "    positives = np.nonzero(uitems)[0]\n",
    "    n_sample = len(positives) * n_negatives\n",
    "    negative_items = list(set(range(N_ITEMS+1))^set(positives))\n",
    "    negatives = np.random.choice(negative_items, n_sample)  # 负采样 -- 不放回\n",
    "    for i in range(len(positives)): # 正实例\n",
    "        user_input.append(uno)\n",
    "        item_input.append(positives[i])\n",
    "        labels.append(1)\n",
    "    for j in range(n_sample): # 负实例\n",
    "        user_input.append(uno)\n",
    "        item_input.append(negatives[j])\n",
    "        labels.append(0)\n",
    "user_input = np.array(user_input)\n",
    "item_input = np.array(item_input)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "utest = list()\n",
    "itest = list()\n",
    "for ui in uipositives:\n",
    "    u = ui[0]\n",
    "    i = ui[1]\n",
    "    positives = np.nonzero(users_items[u])[0]\n",
    "    negative_items = list(set(range(1,N_ITEMS+1))^set(positives))\n",
    "    negatives_sample = np.random.choice(negative_items, 99)  # 负采样 -- 不放回\n",
    "    negatives = [i]  # 正例\n",
    "    for n in negatives_sample:\n",
    "        negatives.append(n)  # 添加负例\n",
    "    utest.append([u for j in range(100)])\n",
    "    itest.append(negatives)\n",
    "ytest = np.zeros((N_USERS,100))\n",
    "ytest[:, 0] = 1\n",
    "utest = np.array(utest)\n",
    "itest = np.array(itest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCH = 5\n",
    "USER_VECTOR_SIZE = 1        # len(one-hot of user vecter) \n",
    "ITEM_VECTOR_SIZE = 1        # len(one-hot of item vecter) \n",
    "LAYERS = [64, 32, 16, 8]    # MLP  0层为输入层  0层/2为嵌入层  \n",
    "GMF_N_FACTORS  = 8          # GMF隐层size  \n",
    "ACTIVATION = torch.relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_x1 = torch.from_numpy(user_input.reshape(-1, 1)).type(torch.LongTensor)\n",
    "torch_x2 = torch.from_numpy(item_input.reshape(-1, 1)).type(torch.LongTensor)\n",
    "torch_y  = torch.from_numpy(labels.reshape(-1, 1)).type(torch.FloatTensor)\n",
    "\n",
    "torch_dataset = data_utils.TensorDataset(torch_x1, torch_x2, torch_y)\n",
    "loader = data_utils.DataLoader(dataset = torch_dataset, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, user_vector_size, item_vector_size, gmf_n_factors, layers,  \n",
    "                 n_users, n_items, activation = torch.relu, batch_normalization = False, n_output = 1):\n",
    "        super(NCF, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.do_bn = batch_normalization\n",
    "        self.fcs = []\n",
    "        self.bns = []\n",
    "        self.n_layer  = len(layers)\n",
    "        parameter_LeCun = np.sqrt(gmf_n_factors + layers[-1])\n",
    "\n",
    "        #self.bn_userInput = nn.BatchNorm1d(1)   # for input data\n",
    "        #self.bn_itemInput = nn.BatchNorm1d(1)   # for input data\n",
    "        \n",
    "        self.mlp_user_embedding_layer = nn.Embedding(n_users+1, int(layers[0]/2))\n",
    "        self._set_normalInit(self.mlp_user_embedding_layer, hasBias = False) \n",
    "        self.mlp_item_embedding_layer = nn.Embedding(n_items+1, int(layers[0]/2))\n",
    "        self._set_normalInit(self.mlp_item_embedding_layer, hasBias = False) \n",
    "        \n",
    "        self.gmf_user_embedding_layer = nn.Embedding(n_users+1, gmf_n_factors)\n",
    "        self._set_normalInit(self.gmf_user_embedding_layer, hasBias = False) \n",
    "        self.gmf_item_embedding_layer = nn.Embedding(n_items+1, gmf_n_factors)\n",
    "        self._set_normalInit(self.gmf_item_embedding_layer, hasBias = False) \n",
    "        \n",
    "        for i in range(1, self.n_layer):               # build hidden layers and BN layers\n",
    "            fc = nn.Linear(layers[i-1], layers[i])\n",
    "            self._set_normalInit(fc)                  # parameters initialization\n",
    "            setattr(self, 'fc%i' % i, fc)       # IMPORTANT set layer to the Module\n",
    "            self.fcs.append(fc)\n",
    "            if self.do_bn:\n",
    "                bn = nn.BatchNorm1d(layers[i])\n",
    "                setattr(self, 'bn%i' % i, bn)   # IMPORTANT set layer to the Module\n",
    "                self.bns.append(bn)\n",
    "\n",
    "        self.predict = nn.Linear(gmf_n_factors + layers[-1], n_output)         # output layer\n",
    "        self._set_uniformInit(self.predict, parameter = parameter_LeCun)            # parameters initialization\n",
    "        return\n",
    "\n",
    "    def _set_normalInit(self, layer, parameter = [0.0, 0.01], hasBias=True):\n",
    "        init.normal_(layer.weight, mean = parameter[0], std = parameter[1])\n",
    "        if hasBias:\n",
    "            init.normal_(layer.bias, mean = parameter[0], std = parameter[1])\n",
    "        return\n",
    "    \n",
    "    def _set_uniformInit(self, layer, parameter = 5, hasBias = True):\n",
    "        init.uniform_(layer.weight, a = - parameter, b = parameter)\n",
    "        if hasBias:\n",
    "            init.uniform_(layer.bias, a = - parameter, b = parameter)\n",
    "        return\n",
    "    \n",
    "    def _set_heNormalInit(self, layer, hasBias=True):\n",
    "        init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.kaiming_normal_(layer.bias, nonlinearity='relu')\n",
    "        return\n",
    "    \n",
    "    def _set_heUniformInit(self, layer, hasBias=True):\n",
    "        init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "        if hasBias:\n",
    "            init.kaiming_uniform_(layer.bias, nonlinearity='relu')\n",
    "        return\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        #if self.do_bn: \n",
    "            #x1 = self.bn_userInput(x1)     # input batch normalization\n",
    "            #x2 = self.bn_itemInput(x2)\n",
    "        mlp_x1 = self.mlp_user_embedding_layer(x1)\n",
    "        mlp_x2 = self.mlp_item_embedding_layer(x2)\n",
    "        \n",
    "        gmf_x1 = self.gmf_user_embedding_layer(x1)\n",
    "        gmf_x2 = self.gmf_item_embedding_layer(x2)\n",
    "        \n",
    "        mlp_x3 = torch.cat((mlp_x1, mlp_x2), dim=1)\n",
    "        mlp_x  = torch.flatten(mlp_x3, start_dim=1)        \n",
    "        for i in range(1, self.n_layer):\n",
    "            mlp_x = self.fcs[i-1](mlp_x)\n",
    "            if self.do_bn: \n",
    "                mlp_x = self.bns[i-1](mlp_x)   # batch normalization\n",
    "            mlp_x = self.activation(mlp_x)\n",
    "        \n",
    "        gmf_x3 = torch.mul(gmf_x1, gmf_x2)\n",
    "        gmf_x  = torch.flatten(gmf_x3, start_dim=1)\n",
    "\n",
    "        x = torch.cat((mlp_x, gmf_x), dim=1)\n",
    "        out = torch.sigmoid(self.predict(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf = NCF(user_vector_size = USER_VECTOR_SIZE, item_vector_size = ITEM_VECTOR_SIZE, gmf_n_factors = GMF_N_FACTORS, \n",
    "          layers = LAYERS, n_users = N_USERS, n_items = N_ITEMS, activation = ACTIVATION, batch_normalization = False, n_output = 1)\n",
    "optimizer = torch.optim.Adam(ncf.parameters(), lr = LEARNING_RATE)\n",
    "loss_func = torch.nn.BCELoss()\n",
    "if(torch.cuda.is_available()):\n",
    "    ncf = ncf.cuda()\n",
    "    loss_func = loss_func.cuda()\n",
    "print(ncf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHitRatio(ranklist, gtItem):\n",
    "    #HR击中率，如果topk中有正例ID即认为正确\n",
    "    if gtItem in ranklist:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def getNDCG(ranklist, gtItem):\n",
    "    #NDCG归一化折损累计增益\n",
    "    for i in range(len(ranklist)):\n",
    "        item = ranklist[i]\n",
    "        if item == gtItem:\n",
    "            return np.log(2) / np.log(i+2)\n",
    "    return 0\n",
    "\n",
    "def getH(ranklist1, ranklist2):\n",
    "    L = len(ranklist1)\n",
    "    common = len(list(set(ranklist1).intersection(set(ranklist2))))\n",
    "    return 1-common/L\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieEval_1(model, loss_func, utest, itest, ytest, topK = 20):\n",
    "    if len(utest)==len(itest)==len(ytest):\n",
    "        n_users = len(utest)\n",
    "    else:\n",
    "        print('the length of test sets are not equal.')\n",
    "        return\n",
    "    hit = 0\n",
    "    undcg = 0\n",
    "    rank_all_users = list()\n",
    "    test_loss = list()\n",
    "    for i in range(n_users):\n",
    "        map_item_score = dict()\n",
    "        x1test = Variable(torch.from_numpy(utest[i].reshape(-1, 1)).type(torch.LongTensor))\n",
    "        x2test = Variable(torch.from_numpy(itest[i].reshape(-1, 1)).type(torch.LongTensor))\n",
    "        y  = Variable(torch.from_numpy(ytest[i].reshape(-1, 1)).type(torch.FloatTensor))\n",
    "        x1test, x2test, y = x1test.cuda(), x2test.cuda(), y.cuda()\n",
    "        prediction = model(x1test, x2test)\n",
    "        loss = loss_func(prediction, y)\n",
    "        test_loss.append(loss.cpu().item())\n",
    "        pred_vector = prediction.cpu().data.numpy().T[0]\n",
    "        positive_item = itest[i][0]  # 取正例\n",
    "        for j in range(len(itest[i])):\n",
    "            map_item_score[itest[i][j]] = pred_vector[j]\n",
    "        ranklist = heapq.nlargest(topK, map_item_score, key=map_item_score.get)\n",
    "        rank_all_users.append(ranklist)\n",
    "        hit += getHitRatio(ranklist, positive_item)\n",
    "        undcg += getNDCG(ranklist, positive_item)\n",
    "    mean_test_loss = np.mean(test_loss)\n",
    "    hr = hit / n_users\n",
    "    ndcg = undcg / n_users\n",
    "    print('test_loss:', mean_test_loss)\n",
    "    print('HR@', topK, ' = %.4f' % hr)\n",
    "    print('NDCG@', topK, ' = %.4f' % ndcg)\n",
    "    return mean_test_loss, hr, ndcg, rank_all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movieEval_2(rank_all_users, movies, topK = 20):\n",
    "    n_users = len(rank_all_users)\n",
    "    n_movies = len(movies)\n",
    "    \n",
    "    # 评估个性化\n",
    "    h_list = list()\n",
    "    for i in range(n_users - 1):\n",
    "        for j in range(i + 1, n_users):\n",
    "            h_list.append(getH(rank_all_users[i], rank_all_users[j]))\n",
    "    personalization = np.mean(h_list)\n",
    "    \n",
    "    # 评估新颖性\n",
    "    I_all_user = list()\n",
    "    for ranklist in rank_all_users:\n",
    "        I_user = list()\n",
    "        for i in ranklist:\n",
    "            k = 0\n",
    "            for temp in rank_all_users:\n",
    "                if i in temp:\n",
    "                    k += 1\n",
    "            I_user.append(np.log2(n_users / k))\n",
    "        I_all_user.append(np.mean(I_user))\n",
    "    surprisal = np.mean(I_all_user)\n",
    "    \n",
    "    #评估覆盖率(熵度量)\n",
    "    entropy = 0\n",
    "    count = 0.0\n",
    "    p = dict()\n",
    "    for i in movies:\n",
    "        p[i] = 0\n",
    "        for ranklist in rank_all_users: \n",
    "            if i in ranklist:\n",
    "                p[i] += 1\n",
    "                count += 1\n",
    "    for v in p.values():\n",
    "        if v != 0:\n",
    "            temp = v/count\n",
    "            entropy -= temp * np.log2(temp) \n",
    "\n",
    "    #评估覆盖率\n",
    "    r = set()\n",
    "    for ranklist in rank_all_users:\n",
    "        for i in ranklist:\n",
    "            r.add(i)\n",
    "    coverage = len(r) / len(movies)\n",
    "    \n",
    "    print('Personalization@', topK, ' = %.4f' % personalization)\n",
    "    print('Surprisal@', topK, ' = %.4f' % surprisal)\n",
    "    print('Entropy@', topK, ' = %.4f' % entropy)\n",
    "    print('Coverage@', topK, ' = %.4f' % coverage)\n",
    "    return personalization, surprisal, entropy, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = list()\n",
    "test_loss_list  = list()\n",
    "hr_list = list()\n",
    "ndcg_list = list()\n",
    "p_list = list()\n",
    "s_list = list()\n",
    "e_list = list()\n",
    "c_list = list()\n",
    "for e in range(EPOCH):\n",
    "    train_loss = list()\n",
    "    for step, (batch_x1, batch_x2, batch_y) in enumerate(loader):\n",
    "        x1, x2, y = Variable(batch_x1), Variable(batch_x2), Variable(batch_y)\n",
    "        if (torch.cuda.is_available()):\n",
    "            x1, x2, y = x1.cuda(), x2.cuda(), y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        prediction = ncf(x1, x2)\n",
    "        loss = loss_func(prediction, y) \n",
    "        loss.backward()        \n",
    "        train_loss.append(loss.cpu().item())\n",
    "        optimizer.step()\n",
    "    print('------第'+str(e+1)+'个epoch------')\n",
    "    mean_train_loss = np.mean(train_loss)\n",
    "    print('train_loss:', mean_train_loss)\n",
    "    train_loss_list.append(mean_train_loss)    \n",
    "    test_loss, hr, ndcg, rank_all_users = movieEval_1(ncf, loss_func, utest, itest, ytest)\n",
    "    personalization, surprisal, entropy, coverage = movieEval_2(rank_all_users, movies)\n",
    "    test_loss_list.append(test_loss)\n",
    "    hr_list.append(hr)\n",
    "    ndcg_list.append(ndcg)\n",
    "    p_list.append(personalization)\n",
    "    s_list.append(surprisal)\n",
    "    e_list.append(entropy)\n",
    "    c_list.append(coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(ncf,\"./model/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = torch.load(\"./model/model.pkl\")\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initSketch(sketch_deep, sketch_width):\n",
    "    sketch = [[(0,0,0) for x in range(sketch_width)] for y in range(sketch_deep)]\n",
    "    return sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = 0.00025\n",
    "S = len(dataset)\n",
    "delta   = 0.05\n",
    "epsilon = 0.002\n",
    "r = round(np.log2(1 / delta)).astype(np.int)\n",
    "w = round(2 / epsilon)\n",
    "print(\"r =\", r)\n",
    "print(\"w =\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch = initSketch(r, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(sketch, item):\n",
    "    sketch_deep  = len(sketch)\n",
    "    sketch_width = len(sketch[0])\n",
    "    x  = item[0]\n",
    "    vx = item[1]\n",
    "    for i in range(sketch_deep):\n",
    "        np.random.seed(i + x)\n",
    "        j = np.random.choice(sketch_width)\n",
    "        V = sketch[i][j][0] + vx\n",
    "        K = sketch[i][j][1]\n",
    "        C = sketch[i][j][2]\n",
    "        if K == x:\n",
    "            C += vx\n",
    "        else:\n",
    "            C -= vx\n",
    "            if C < 0:\n",
    "                K = x\n",
    "                C = -C\n",
    "        sketch[i][j] = (V, K, C)\n",
    "    return sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processStream_HH(sketch, dataset):\n",
    "    for record in dataset:\n",
    "        item = (record[1], 1)\n",
    "        update(sketch,item)\n",
    "    return \n",
    "\n",
    "data = dataset[dataset[:,2].argsort()]\n",
    "processStream_HH(sketch, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryU(sketch, x):\n",
    "    sketch_deep  = len(sketch)\n",
    "    sketch_width = len(sketch[0])\n",
    "    res_list = list()\n",
    "    for i in range(sketch_deep):\n",
    "        np.random.seed(i + x)\n",
    "        j = np.random.choice(sketch_width)\n",
    "        V = sketch[i][j][0]\n",
    "        K = sketch[i][j][1]\n",
    "        C = sketch[i][j][2] \n",
    "        if K == x:\n",
    "            S = (V + C) / 2\n",
    "        else:\n",
    "            S = (V - C) / 2\n",
    "        res_list.append(S)\n",
    "    return min(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryL(sketch, x):\n",
    "    sketch_deep  = len(sketch)\n",
    "    sketch_width = len(sketch[0])\n",
    "    res_list = list()\n",
    "    for i in range(sketch_deep):\n",
    "        np.random.seed(i + x)\n",
    "        j = np.random.choice(sketch_width)\n",
    "        K = sketch[i][j][1]\n",
    "        C = sketch[i][j][2] \n",
    "        if K == x:\n",
    "            S = C\n",
    "        else:\n",
    "            S = 0\n",
    "        res_list.append(S)\n",
    "    return max(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitter(sketch, phi, S):\n",
    "    print(\"heavy hitter threshold: \", phi * S)\n",
    "    hh = set()\n",
    "    sketch_deep  = len(sketch)\n",
    "    sketch_width = len(sketch[0])\n",
    "    for i in range(sketch_deep):\n",
    "        for j in range(sketch_width):\n",
    "            if sketch[i][j][0] >= phi * S:\n",
    "                x = sketch[i][j][1]\n",
    "                if queryU(sketch, x) >= phi * S:\n",
    "                    hh.add(x)\n",
    "    return hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resHH = hitter(sketch, phi, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(res, real):\n",
    "    tp = fp = fn = 0\n",
    "    for i in res:\n",
    "        if i in real:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    for j in real:\n",
    "        if j not in res:\n",
    "            fn += 1\n",
    "    print(\"TP =\",tp,\"   FP =\", fp,\"   FN =\", fn)\n",
    "    recall = tp / (tp + fn)\n",
    "    print('reacall:', recall)\n",
    "    precision = tp / (tp + fp)\n",
    "    print('precision:',precision)\n",
    "    f1 = (2 * recall * precision) / (precision + recall)\n",
    "    print('F1-score:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(resHH, realHH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resHH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
