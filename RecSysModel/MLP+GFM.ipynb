{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络结构定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, user_vector_size, item_vector_size, mf_n_factors, mlp_n_factors, mlp_n_layers, mlp_n_last_layer, n_output = 1):\n",
    "        ''' user_vector_size - user输入向量长度   item_vector_size - item输入向量长度\n",
    "            mf_n_factors - MF模型嵌入层神经元数目   MLP_n_factors  -  MLP模型嵌入层神经元数目\n",
    "            mlp_n_layers - MLP模型层数   mlp_n_first_layer - MLP第一层神经元数目（后续依次减半）\n",
    "            n_output - 输出层神经元数目\n",
    "        '''\n",
    "        super(Net, self).__init__()\n",
    "        self.mlp_n_layers = mlp_n_layers\n",
    "        mlp_n_first_layer = int(mlp_n_last_layer * np.power(2, mlp_n_layers-1))\n",
    "        # GMF\n",
    "        self.mf_user_embedding_layer = nn.Linear(user_vector_size, mf_n_factors)\n",
    "        self.mf_item_embedding_layer = nn.Linear(item_vector_size, mf_n_factors)\n",
    "        \n",
    "        # MLP\n",
    "        self.mlp_user_embedding_layer = nn.Linear(user_vector_size, mlp_n_factors)\n",
    "        self.mlp_item_embedding_layer = nn.Linear(item_vector_size, mlp_n_factors)\n",
    "        self.hidden = list()\n",
    "        for i in range(1,mlp_n_layers+1):\n",
    "            if i==1:\n",
    "                self.hidden.append(nn.Linear(2 * mlp_n_factors, int(mlp_n_first_layer/i)))\n",
    "            else:\n",
    "                self.hidden.append(nn.Linear(int(mlp_n_first_layer/(i-1)), int(mlp_n_first_layer/i)))\n",
    "    \n",
    "        # NeuMF Layer\n",
    "        self.neuMF_layer = nn.Linear(int(mlp_n_first_layer/mlp_n_layers) + mf_n_factors, n_output)\n",
    "        return\n",
    "    \n",
    "    def forward(self, user_input, item_input): # 多输入\n",
    "        # print('in forward', user_input.dtype)\n",
    "        # MLP\n",
    "        out01 = self.mlp_user_embedding_layer(user_input)\n",
    "        out02 = torch.relu(out01)\n",
    "        out11 = self.mlp_item_embedding_layer(item_input)\n",
    "        out12 = torch.relu(out11)\n",
    "        input_of_NCF = torch.cat((out02, out12), dim=1) # concatenation \n",
    "        \n",
    "        # MLP隐层全连接\n",
    "        out2x = list()\n",
    "        for i in range(1,self.mlp_n_layers+1):\n",
    "            if i==1:\n",
    "                out2x.append(self.hidden[i-1](input_of_NCF))\n",
    "                out2x.append(torch.relu(out2x[-1]))\n",
    "            else:\n",
    "                out2x.append(self.hidden[i-1](out2x[-1]))\n",
    "                out2x.append(torch.relu(out2x[-1]))\n",
    "        '''\n",
    "        out21 = self.hidden1(input_of_NCF)\n",
    "        out22 = torch.relu(out21)    \n",
    "        out23 = self.hidden2(out22)\n",
    "        out24 = torch.relu(out23)     \n",
    "        out25 = self.hidden3(out24)\n",
    "        out26 = torch.relu(out25)\n",
    "        '''\n",
    "        \n",
    "        # GMF\n",
    "        out31 = self.mf_user_embedding_layer(user_input)\n",
    "        out32 = torch.relu(out31)\n",
    "        out41 = self.mf_item_embedding_layer(item_input)\n",
    "        out42 = torch.relu(out41)\n",
    "        input_of_GMF = torch.mul(out32, out42) # element-wise product\n",
    "        \n",
    "        #NeuMF Layer\n",
    "        input_of_neuMF = torch.cat((input_of_GMF, out2x[-1]), dim=1) # concatenation  \n",
    "        out51 = self.neuMF_layer(input_of_neuMF)\n",
    "        output_of_neuMF = torch.sigmoid(out51)\n",
    "        return output_of_neuMF \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "## 数据集处理\n",
    "### 依照留一法划分数据集。。。（待续）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_users = len(open('./ml-1m/users.dat', encoding = 'ISO-8859-1').readlines())\n",
    "n_users = 6040\n",
    "# n_items = len(open('./ml-1m/movies.dat', encoding = 'ISO-8859-1').readlines())\n",
    "n_items = 3952\n",
    "n_negatives = 4  ## 1正例对应n个负例 ##\n",
    "users_items = np.zeros((n_users+1, n_items+1), dtype = np.int8)  # 混淆矩阵\n",
    "dataset = np.loadtxt(\"./ml-1m/ratings.dat\",delimiter='::',dtype=int)[:,[0,1,3]]\n",
    "\n",
    "user_input, item_input, labels = [],[],[]  # x1 x2 -> y\n",
    "for u in range(dataset.shape[0]):   # 评分数据集隐式化\n",
    "    users_items[dataset[u][0], dataset[u][1]] = 1\n",
    "uipositives = list() # 作为测试集的交互正例\n",
    "for i in range(n_users+1):\n",
    "    if i==0: \n",
    "        continue\n",
    "    uitems = dataset[dataset[:,0]==i]\n",
    "    onepos = uitems[uitems[:,-1]==np.max(uitems),:2][0]\n",
    "    uipositives.append(onepos)\n",
    "    users_items[onepos[0], onepos[1]]=0\n",
    "for uno, uitems in enumerate(users_items):\n",
    "    if uno == 0:\n",
    "        continue\n",
    "    positives = np.nonzero(uitems)[0]\n",
    "    n_sample = len(positives) * n_negatives\n",
    "    negative_items = list(set(range(n_items+1))^set(positives))\n",
    "    negatives = np.random.choice(negative_items, n_sample)  # 负采样 -- 不放回\n",
    "    for i in range(len(positives)): # 正实例\n",
    "        user_input.append(uno)\n",
    "        item_input.append(positives[i])\n",
    "        labels.append(1)\n",
    "    for j in range(n_sample): # 负实例\n",
    "        user_input.append(uno)\n",
    "        item_input.append(negatives[j])\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utest = list()\n",
    "itest = list()\n",
    "for ui in uipositives:\n",
    "    u = ui[0]\n",
    "    i = ui[1]\n",
    "    positives = np.nonzero(users_items[u])[0]\n",
    "    negative_items = list(set(range(1,n_items+1))^set(positives))\n",
    "    negatives = list(np.random.choice(negative_items, 100))  # 负采样 -- 不放回\n",
    "    negatives.append(i)\n",
    "    utest.append([u for j in range(101)])\n",
    "    itest.append(negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.0005\n",
    "EPOCH = 20\n",
    "user_vector_size = 1    # len(one-hot of user vecter) \n",
    "item_vector_size = 1    # len(one-hot of item vecter) \n",
    "mf_n_factors = 8        # MF  嵌入层神经元数\n",
    "mlp_n_factors = 8       # MLP 嵌入层神经元数\n",
    "mlp_n_layers  = 3       # MLP 隐层数\n",
    "mlp_n_last_layer = 16  # MLP 第一层神经元数  后续依次减半"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Variable(torch.from_numpy(np.array(user_input, ndmin=2, dtype=np.float32).T))\n",
    "x2 = Variable(torch.from_numpy(np.array(item_input, ndmin=2, dtype=np.float32).T))\n",
    "y = Variable(torch.from_numpy(np.array(labels, ndmin=2, dtype=np.float32).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-batch训练\n",
    "torch_dataset = data_utils.TensorDataset(x1,x2,y)\n",
    "loader = data_utils.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m): # 初始化 - 高斯分布 均值0 标准差0.01\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight.data, mean=0, std=0.01)\n",
    "        nn.init.normal_(m.bias.data, mean=0, std=0.01)\n",
    "    return\n",
    "\n",
    "net = Net(user_vector_size = user_vector_size, item_vector_size = item_vector_size, mf_n_factors = mf_n_factors, \n",
    "          mlp_n_factors = mlp_n_factors, mlp_n_layers = mlp_n_layers, mlp_n_last_layer = mlp_n_last_layer, n_output = 1)\n",
    "net.apply(weights_init)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE) \n",
    "loss_func = torch.nn.BCELoss()  # 二分交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(EPOCH):\n",
    "    for step, (batch_x1, batch_x2, batch_y) in enumerate(loader):\n",
    "        prediction = net(batch_x1, batch_x2)\n",
    "        loss = loss_func(prediction, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Loss = %.4f' % loss.data)\n",
    "        \n",
    "        hit = 0\n",
    "        for i in range(n_users):\n",
    "            x1test = Variable(torch.from_numpy(np.array(utest[i], ndmin=2, dtype=np.float32).T))\n",
    "            x2test = Variable(torch.from_numpy(np.array(itest[i], ndmin=2, dtype=np.float32).T))\n",
    "            prediction = net(x1test, x2test)\n",
    "            #print(prediction.data)\n",
    "            itestResIndex = np.argsort(prediction.detach().numpy().T[0])\n",
    "            itestTopKIndex = itestResIndex[len(itestResIndex)-10:]\n",
    "            #print(itestTopKIndex)\n",
    "            if 100 in itestTopKIndex:\n",
    "                hit += 1\n",
    "        print('HR@10 = %.4f' % (hit/n_users))\n",
    "        \n",
    "    #if e % 2 == 0:\n",
    "    print('------第'+str(e)+'个epoch------')\n",
    "    print('epoch loss = %.4f' % loss.data)\n",
    "    hit = 0\n",
    "    for i in range(n_users):\n",
    "        x1test = Variable(torch.from_numpy(np.array(utest[i], ndmin=2, dtype=np.float32).T))\n",
    "        x2test = Variable(torch.from_numpy(np.array(itest[i], ndmin=2, dtype=np.float32).T))\n",
    "        prediction = net(x1test, x2test)\n",
    "        itestResIndex = np.argsort(prediction.detach().numpy().T[0])\n",
    "        itestTopKIndex = itestResIndex[len(itestResIndex)-10:]\n",
    "        print(itestTopKIndex)\n",
    "        if 100 in itestTopKIndex:\n",
    "            hit += 1\n",
    "    print('HR@10 = %.4f' % (hit/n_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估 留一法。。。（待续）\n",
    "### HR\n",
    "### NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python3.6(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
